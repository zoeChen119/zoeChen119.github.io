---
title: RAG
categories: [NLP,基础知识]
tags: [nlp,RAG,技术综述]     # TAG names should always be lowercase
---

# RAG调研

## 一、一些共识

> External knowledge is the key to resolving the problems of LLMs such as hallucination and outdated knowledge, which can make LLMs generate more accurate and reliable responses through retrieval-augmented generation (RAG). However, LLMs cannot always response as expected with RAG. For one thing, there are numerous irrelevant documents and false information on the Internet. Incorporating these external documents into LLMs could have a detrimental effect. For anthoer, LLMs suffer from the unreliable generation challenge. The generation of LLMs is often unpredictable, and we cannot guarantee that they will utilize the useful information entailed in the external documents. Additionally, LLMs can easily be misled by incorrect information in the document. To this end, we build RetrievalAugmented Generation Benchmark (RGB) to evaluate the retrieval-augmented generation of LLMs, and we concern about 4 specific abilities:[^Benchmarking Large Language Models in Retrieval-Augmented Generation]

外部知识是解决幻觉和过时知识等llm问题的关键，它可以通过检索增强生成(RAG)使llm产生更准确、更可靠的响应。然而，

⭐**LLM 并不总是像 RAG 预期的那样响应**。

⭐**将这些不相关的或者虚假的外部文档合并到 LLM 中可能会产生不利影响。**

⭐**llm的生成通常是不可预测的，我们不能保证它们将利用外部文档中包含的有用信息。**

⭐**LLM 很容易被文档中不正确的信息误导。**

> In real-world scenarios, it is not possible to obtain perfect documents with all the necessary external knowledge. Therefore, evaluating these four abilities of the model becomes essential in order to measure the RAG of LLMs.[^Benchmarking Large Language Models in Retrieval-Augmented Generation]

⭐**在现实世界的场景中，不可能获得具有所有必要的外部知识的完美文档。**因此需要能够评估模型的这四个能力。



## 二、论文调研

###  Benchmarking Large Language Models in Retrieval-Augmented Generation[^Benchmarking Large Language Models in Retrieval-Augmented Generation]

<p align="right">---中科院计算所，23年9月4日</p>

[^Benchmarking Large Language Models in Retrieval-Augmented Generation]:https://readpaper.com/pdf-annotate/note?pdfId=4797068573826613249&noteId=2070517605082304512

#### 摘要：

检索增强生成 (RAG) 是一种**减轻大型语言模型 (LLM) 幻觉**的有前途的方法。然而，现有的研究缺乏对检索增强生成对不同大型语言模型的影响的严格评估，这使得识别RAG对不同llm能力的潜在瓶颈具有挑战性。在本文中，我们**系统地调查了 Retrieval-Augmented Generation 对大型语言模型的影响**。我们**分析了不同大型语言模型在RAG所需的4个基本能力下的性能，包括噪声鲁棒性、负拒绝、信息集成和反事实鲁棒性**。为此，我们**建立了 Retrieval-Augmented Generation Benchmark (RGB)，这是一个用于英文和中文 RAG 评估的新语料库**。RGB 根据解决案例所需的上述基本能力，将基准内的实例划分为 4 个单独的测试平台。然后我们在 RGB 上评估 6 个具有代表性的 LLM，以诊断应用 RAG 时当前 LLM 的挑战。<u>评估表明，虽然 LLM 表现出一定程度的噪声鲁棒性，但它们在负拒绝、信息集成和处理虚假信息方面仍然会遇到重大困难</u>。上述评估结果表明，在有效地将RAG应用于LLM之前，仍有相当大的旅程。



#### Introduction：

🌱为什么要研究RAG模型？

> Recently, there have been impressive advancements in large language models (LLMs) like ChatGPT (OpenAI 2022), LLaMA-2 (Touvron et al. 2023), and ChatGLM (THUDM 2023a). Although these models have shown remarkable general abilities (Bang et al. 2023; Guo et al. 2023), they still suffer severely from challenges including factual hallucination (Cao et al. 2020; Raunak, Menezes, and JunczysDowmunt 2021; Ji et al. 2023), knowledge out-dating (He, Zhang, and Roth 2022), and the lack of domain-specific expertise (Li et al. 2023c; Shen et al. 2023).

ChatGPT、LLaMA-2、ChatGLM等大模型虽然有优秀的通用能力，但是存在一些问题，①factual hallucination事实幻觉；②knowledge out-dating知识过时；③domain-specific expertise特定领域的专业知识。

![RAG的研究动机-2023-12-04-1505](..\..\..\..\..\zoeChen119.github.io\assets\img\2023-12-04-RAG检索知识增强\RAG的研究动机-2023-12-04-1505.png)

> With the help of external knowledge, LLMs can generate more accurate and reliable responses. The most common method is to use a search engine as a retriever such as New Bing.

LLMs+RAG的一个典型应用就是New Bing。

> On the other hand, LLMs suffer from unreliable generation challenge. LLMs can be misled by incorrect information contained in the context (Bian et al. 2023) and also suffer from hallucination during the generation (Adlakha et al. 2023), resulting in generating content that goes beyond external information.

RAG也会给LLMs带来负面影响，比如①互联网中存在虚假信息，②LLM可能会被上下文中包含的错误信息误导，③LLM在生成过程中存在幻觉，会生成超出外部信息的内容。

> Unfortunately, currently there lacks of comprehensive understanding on how these factors can influence RAG, and how could each model survives from these drawbacks and improvement their performance via information retrieval.

**不幸的是，目前对这些因素如何影响RAG，以及每个模型如何从这些缺陷中幸存下来并通过信息检索提高其性能缺乏全面的了解。**因此，迫切需要对LLM进行全面评估，评估其有效利用检索到的信息的能力，以及抵御信息检索中存在的各种缺点的能力。



💡为了确保LLM的内部知识不会在评估结果中引入偏差，RGB选择聚合**最新的新闻信息**，并基于新闻信息构建查询。然后，基于这些查询，我们使用搜索API获取相关文档，并从内容中**选择最相关的片段作为外部检索文档**。最后，基于查询和文档集对的不同组成，我们**扩展语料库**，并将其**划分为4个测试平台**，根据RAG中常见的挑战来评估LLM的以下基本能力，如图1所示：

![img](..\..\..\..\..\zoeChen119.github.io\assets\img\2023-12-04-RAG检索知识增强\6ccbadec7b7dfc8af05d8871f80c3983_0_Figure_1_645056946.png)

1. 噪声鲁棒性Noise Robustness：“从嘈杂文档中提取有用信息”的能力。

   本文中，我们将【嘈杂的文档】定义为“与问题相关的文档，但不包含任何答案信息。”

   噪声鲁棒性平台：

   	所有相关外部文档=噪声文档+包含答案信息的文档
		
   	噪声文档=噪声比*所有相关外部文档

2. 负拒绝Negative Rejection：“拒绝回答无答案查询”的能力

   无答案查询：所需知识不存在于任何检索到的文档中。此情况，LLM应给出“信息不足”或其他拒绝信号。

   负拒绝平台：

   	外部文档=噪声文档

3. 信息整合Information Integration：“回答关联多个文档的复杂问题”的能力

   信息整合平台：

   	查询=只能用多个文档才能回答的实例
		
   	外部文档=多个包含答案信息的文档+噪声文档

4. 反事实鲁棒性Counterfactual Robustness：“通过指令提示LLMs'警告：检索到的信息存在潜在风险‘时，能够识别检索到的文档中的已知事实错误”的能力

   反事实稳健性平台：

   	LLM已知的知识，即可以直接回答的query。
		
   	外部文档=存在事实错误的文档

   请注意，我们只评估 LLM 通过指令对检索到的信息中潜在风险的警告的情况。



测评模型：

1. ChatGPT
2. ChatGLM-6B
3. ChatGLM2-6B
4. Vicuna-7b
5. Qwen-7B-Chat
6. BELLE-7B



结论：

> We found that even though RAG can improve the response accuracy of LLMs, they still suffer from the abovementioned challenges significantly. Specifically, we found that even though LLMs demonstrate some level of noise robustness, they tend to confuse similar information and frequently generate inaccurate answers when relevant information exists.

尽管RAG可以提高llm的响应精度，但它们仍然受到上述挑战的显著影响。具体来说，

1. 我们发现，尽管 LLM 展示了一定程度的噪声鲁棒性，但当相关信息存在时，它们往往会混淆相似的信息并经常生成不准确的答案。

例如，当面对有关 2022 年诺贝尔文学奖的问题时，如果有关外部文件文献中 2021 年诺贝尔奖的嘈杂文档，LLM 可能会混淆并提供不准确的答案。

2. 此外，当没有一个外部文档包含相关信息时，LLM 经常无法拒绝回答并生成不正确的答案。

3. 此外，LLM 缺乏从多个文档中总结的能力，因此如果需要多个文档来回答问题，LLM 通常无法提供准确的答案。
4. 最后，我们发现，即使llm包含所需的知识，并通过指令对检索到的信息中潜在风险的警告，它们仍然倾向于信任和优先考虑检索到的信息而不是他们自己的现有知识。

#### Retrival-Augmented Generation Benchmark

数据集构建过程：

![img](..\..\..\..\..\zoeChen119.github.io\assets\img\2023-12-04-RAG检索知识增强\6ccbadec7b7dfc8af05d8871f80c3983_2_Figure_2_-1246759510.png)

News Collection--真实的

Step 0：收集最新的新闻文章

Step 1：QA实例生成（QA instances generation）：使用prompt让ChatGPT为每篇文章生成**事件（Related event）、问题（Question）和答案（Key information）**。这一步能够顺便初步过滤出不包含任何事件的新闻文章。

Step 2：人工检查调整、过滤**难以通过搜索引擎检索的数据**。

Step 3：使用搜索引擎API检索（Retrieve using search engine）：对于每个Query使用谷歌 API获取10个相关网页，并提取相应的文本片段。

Step 4：阅读这些网页，并将其文本内容转换为最大长度为300个token的文本块。使用现有的密集检索模型选择与查询最有效匹配的前30个文本块。这些检索到的文本块，以及搜索API提供的片段，将用作我们的**外部文档**。

Step 5：扩展语料库，为4种能力构建Testbeds（试验台）。

	对于噪声鲁棒性，根据所需的噪声比率对不同数量的负面文档进行采样。
	
	对于负样本拒绝，所有的外部分档都是从负文档中采样的。
	
	对于信息整合，基于前面生成的问题构造复杂问题。这涉及到扩展或重写这些问题，使它们的答案包含多个方面。例如，“谁获得了2023年超级碗的MVP？”这个问题可以改写为“谁赢得了2022年和2023年的超级碗MVP。因此，回答这样的问题需要利用来自各种文件的信息。
	
	对于反事实稳健性，反事实稳健性数据完全基于模型的内部知识构建，也就是说让模型自动生成已知的问题和答案，例如，基于“谁获得了 2022 年诺贝尔生理学和医学奖？”的问题，该模型将生成已知问题“谁获得了 2021 年诺贝尔文学奖？”并回答“Abdulrazak Gurnah”。然后人工验证生成的答案，并检索相关文档，为了使文档包含事实错误，我们人工修改答案并替换文档中的相应部分。

> 密集检索模型:
>
> for English：https://huggingface.co/sentence-transformers/all-mpnet-basev2
>
> for Chinese：https://huggingface.co/moka-ai/m3e-base



RGB统计数据：

共600个基本问题，200个信息整合能力的附加问题，200个反事实稳健性能力的附加问题。

所有问题，一半中文，一半英文。



评价指标：

**该基准的核心是评估 LLM 是否可以利用提供的外部文档来获取知识并生成合理的答案。**

* Accuracy：for 噪声鲁棒性、信息整合

  	精确匹配-如果生成的文本包含与答案的精确匹配，则视为回答正确。

* Rejection rate：for 负样本拒绝

  	当只提供嘈杂的文档时，LLM应输出具体内容—— "I can not answer the question because of the insufficient information in documents."（“由于文档中的信息不足，我无法回答问题。”）如果模型生成此内容，则表示成功拒绝。

    	PS.使用说明提示模型。

* Error detection rate：for 反事实鲁棒性（衡量模型是否能检测出文档中的事实错误）

  	当提供的文档包含事实错误时，模型应该输出特定的内容-"There are factual errors in the provided documents."（“提供的文档中存在事实错误”。）如果模型生成该内容，则表明模型在文档中检测到错误信息。

    	PS.使用说明提示模型。

* Error correction rate：for 反事实鲁棒性（衡量模型是否能在识别到事实错误之后仍能提供正确的答案）

  	如果该模型生成正确的答案，则表明模型能够 修正 文档中的事实错误。

  $$ACC=\frac{\#tt}{\#nums}$$

  #tt：正确的response数量

  #nums：所有待评估的❓实例数量

  

考虑到模型可能无法完全遵守指令，对于拒绝率和错误检测率，我们还使用ChatGPT对答案进行额外评估。具体而言，❓我们通过使用指导（instructions）和演示（demonstrations）来评估模型的响应，以确定它们是否能够反映文件中没有的信息或识别任何事实错误。

#### Experiments

##### Settings

由于上下文限制，我们为每个问题提供**5个外部文档**。在我们关于噪声鲁棒性的实验中，我们评估了噪声比在0到0.8之间的场景。为了全面评估整体能力，我们**对每种语言都采用了统一的指导**（instructions），如图3所示。实验是使用NVIDIA GeForce RTX 3090进行的。

![img](..\..\..\..\..\zoeChen119.github.io\assets\img\2023-12-04-RAG检索知识增强\6ccbadec7b7dfc8af05d8871f80c3983_3_Figure_3_536383974.png)

include a system instruction followed by a user input instruction

##### Results on Noise Robustness

针对 噪声鲁棒性 实验：

![img](..\..\..\..\..\zoeChen119.github.io\assets\img\2023-12-04-RAG检索知识增强\6ccbadec7b7dfc8af05d8871f80c3983_4_Table_1_1366057918.png)

**结论**：

1. RAG能有效改善LLMs的响应。

   即使在存在噪声的情况下，llm也表现出了很强的性能，这表明RAG是llm产生准确可靠响应的一种有希望的方法。

2. 噪声率的不断提高对llm中的RAG提出了挑战。

   具体来说，当噪声比超过80%时，模型的精度明显下降。例如，ChatGPT的性能从96.33%下降到76.00%，而ChatGLM2-6B的性能从91.33%下降到57.33%。

深入分析这个实验中的模型答错的答案，发现错误通常源于**3个原因**：

![img](..\..\..\..\..\zoeChen119.github.io\assets\img\2023-12-04-RAG检索知识增强\6ccbadec7b7dfc8af05d8871f80c3983_4_Table_2_1780650320.png)

给出了噪声鲁棒性的误差案例，并且只给出了一个正面文档和一个负面文档。响应由ChatGLM2-6B生成。蓝色文本表示文档与问题或答案之间匹配的部分，而红色文本突出显示不匹配的部分。

1. 长距离信息

   当外部文档中**“与问题相关的信息”与“与答案相关的信息”相距甚远时**，模型难以识别正确答案。这种情况中，大部分是“与问题相关的信息”首先出现在文档的开头，在下文中使用代词指代它。这种情况出现时，**模型会去依赖其他文档的信息**，产生错误的印象（幻觉）。

   如上图中卡塔尔公开赛只在开头出现了一次，与答案文本Anett Kontaveit人名相距甚远。

2. 证据的不确定性

   在备受关注的事件发生前，人们倾向于**预测、猜测、预言**它。尽管这样的预测文档明确指出这是不确定或推测性的内容，但它们仍然会影响LLM的检索增强生成。

   如上图中，错误文档中的内容都是关于苹果新耳机的预测（Apple Reality Pro），并且存在有正确答案的文档（Vision Pro），模型仍然被**误导**了。

3. 概念混淆

   外部文档中的概念可能与问题中的**概念相似**，但又不同。这会让LLM**混淆**，并生成不正确的答案。

   如上图中，模型对问题的理解集中在”汽车收入“的概念上（特斯拉，收入->通常提到特斯拉都是谈论特斯拉汽车->特斯拉，汽车收入），但问题问的是特斯拉这个公司一季度全部“收入”。

通过上述分析，LLM需要进一步详细的增强，比如长文档建模、精确的概念理解

##### Results on Negative Rejection testbed

针对 负拒绝 实验：（只提供负样本，看模型的拒绝率）

采用**精确匹配**评估拒绝率（Rej），利用**ChatGPT**来确定LLM的response是否包含拒绝信息（Rej*）。

![img](..\..\..\..\..\zoeChen119.github.io\assets\img\2023-12-04-RAG检索知识增强\6ccbadec7b7dfc8af05d8871f80c3983_5_Table_3_1815174249.png)

**结论**：

1. 负拒绝对LLM中的RAG提出了挑战。

   中文/英文LLM的最高拒绝率最高43.33%/45%。

2. 通过比较Rej和Rej*，发现LLM不能严格遵循指令，且经常产生不可预测的response，因此LLM很难直接用来识别负样本并判定拒绝。



**2个原因**：

![img](..\..\..\..\..\zoeChen119.github.io\assets\img\2023-12-04-RAG检索知识增强\6ccbadec7b7dfc8af05d8871f80c3983_5_Table_4_-1001365273.png)



1. 证据不确定性

   虽然文件中只提到与 "亚当-麦凯 "的联系，并没有明确指出他是这部电影的导演，但模型仍然断定他担任了这一角色。

2. 概念混淆

   答案中提供的信息与问题中提到的“2022 年冬季奥运会”而不是“2022 年奥运会”。与直接回答相比，检索增强生成对负拒绝提出了更大的挑战，因为它提供了可能误导 LLM 并导致错误响应的相关文档。



##### Results on Information Integration testbed

针对 信息整合 实验：不同噪声率下，评估模型准确率。

![img](..\..\..\..\..\zoeChen119.github.io\assets\img\2023-12-04-RAG检索知识增强\6ccbadec7b7dfc8af05d8871f80c3983_6_Table_5_-1246759510.png)



**结论**：

1. 信息整合 对于LLMs中的RAG来说是一个大挑战（**LLM难以有效地整合信息，不太适合直接回答复杂的问题。**）

   即使噪声率=0，英文/中文中最高也只有60%/67%的准确率

   添加噪声文档后，英文/中文下降到43%/55%

2. 对于文档嘈杂的RAG来说，**复杂的问题**更具挑战性。

   对比 噪声鲁棒性 实验（简单问题，添加噪声文档），如下图。简单问题时，噪声比达到0.8时，性能才显著下降，而复杂问题时，噪声比=0.4时，性能猛降。

   ![img](..\..\..\..\..\zoeChen119.github.io\assets\img\2023-12-04-RAG检索知识增强\6ccbadec7b7dfc8af05d8871f80c3983_4_Table_1_1366057918.png)

   这表明复杂的问题更容易受到噪声的干扰。我们推测，这是因为解决复杂问题需要整合来自多个文档的信息，而这些信息可以被视为彼此的噪声，使模型更难从文档中提取相关信息。

**4个原因**：（这里分析的是噪声率-0的ChatGLM2-6B的错误数据）

![img](..\..\..\..\..\zoeChen119.github.io\assets\img\2023-12-04-RAG检索知识增强\6ccbadec7b7dfc8af05d8871f80c3983_6_Table_6_-1649557801.png)

1. 38%的错误实例属于 噪声稳健性 实验中发现的3个错误原因

2. 合并错误（占比28%）：

   模型试图合并两个子问题的答案，错误地使用一个问题的答案直接回答两个子问题，然后忽略另一个子问题相关的所有文档。

   例如：模型回答D 组是法国队和德国队的世界杯小组，而实际上德国队被分到了 E 组。

3. 忽视错误（28%）

   模型只回答一个问题，直接忽略另一个问题。原因是模型对问题缺乏全面的理解，没有认识到问题由多个子问题组成。

   例如：模型只提供了 2022 年超级碗 MVP 的答案，而没有考虑 2023 年。

4. 错位误差（6%）

   模型把子问题1的文档误识别为子问题2的问答，导致答案错位。

   例如：模型仅提到了 2023（95）学院奖的最佳图片，完全忽略了 2022 年奖项。此外，它错误地指出，“CODA”是 2023 的最佳图片，当它实际上被授予 2022 年的最佳图片时。

上述错误主要是由于对复杂问题的理解有限，这阻碍了有效利用来自不同子问题的信息的能力。**关键是提高模型的推理能力。**一种可能的解决方案是使用**思维链方法来分解复杂问题**（Zhou et al. 2023a; Xu et al. 2023b; Drozdov et al. 2023）。然而，这些方法**会减慢推理速度**，无法提供及时的响应。

##### Results on Counterfactual Robustness testbed

针对 反事实鲁棒性 实验：

（考察模型对于已知事实知识的问题，包含事实错误的噪声文档能否干扰他的思考。因此只考虑准确率超过 70% 的 LLM。）

4个指标：①不包含任何文档的准确率、②包含反事实文档的准确率、③错误识别率、④错误纠正率

![img](..\..\..\..\..\zoeChen119.github.io\assets\img\2023-12-04-RAG检索知识增强\6ccbadec7b7dfc8af05d8871f80c3983_6_Table_7_2078354437.png)

采用**精确匹配**评估错误识别率（ED），利用**ChatGPT**来识别LLM的response是否包含反事实错误信息（ED*）。

> 检索增强生成不是为自动解决给定上下文中的事实错误而设计的，因为这与模型缺乏知识并且依赖于检索到的文档以获取附加信息的基本假设相矛盾。
>
> 然而，由于互联网上假新闻泛滥，这个问题在实际应用中至关重要。现有的 LLM 不具备处理因错误信息造成的不准确回复的保障措施。事实上，它们在很大程度上依赖于检索到的信息。**即使 LLM 包含有关问题的内部知识，它们也经常相信检索到的虚假信息**。这对未来在 LLMs 中发展 RAG 提出了重大挑战。



<font color=#008B8B>

#### 🤹个人总结

![image-20231206113404826](..\..\..\..\..\zoeChen119.github.io\assets\img\2023-12-04-RAG检索知识增强\image-20231206113404826.png)



</font>

