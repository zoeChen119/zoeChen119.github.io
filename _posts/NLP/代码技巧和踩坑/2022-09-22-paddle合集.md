---
title: Paddle写代码踩坑记录
categories: [NLP,代码技巧和踩坑]
tags: [nlp,paddle,代码技巧和踩坑]     # TAG names should always be lowercase
---

# Paddle写代码踩坑记录

##### 1. 由于样本不平衡，指定交叉熵损失函数每个类别的权重
```python
# 由于样本不平衡，指定交叉熵损失函数每个类别的权重M2
# M1.[1/count(0),1/count(1)]
# M2.[max(count)/count(0),max(count)/count(1)]
weight_data = np.array([2.31,1.0]).astype("float32")
weight = paddle.to_tensor(weight_data)

criterion = paddle.nn.loss.CrossEntropyLoss(weight=weight)  # 交叉熵损失函数
```
![](/assets/img/paddle合集/2022-09-22-14-08-49.png)
![](/assets/img/paddle合集/2022-09-22-14-09-30.png)
![](/assets/img/paddle合集/2022-09-22-14-09-59.png)

官方文档：https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/nn/CrossEntropyLoss_cn.html#cn-api-nn-loss-crossentropyloss
参考资料：https://blog.csdn.net/qq_48345413/article/details/117925597
备注：需要注意的是weight_data的类型<font color=Sienna>应该是要和logits的类型一致,label的类型必须是int64</font>。

##### 2. DataFrame增加一列

（逐行增加一个属性,全部都是同一个值）
```python
from pandas import DataFrame
merge_dt_dict = {'date':date_list,
                'update':update_list,
                'serverip':serverip_list}

data_df = DataFrame(merge_dt_dict)
# add one column add_column and set values=1 
data_df['add_column']  = 1 

```

（逐行增加一个属性,是一个列表）
```python
from pandas import DataFrame
merge_dt_dict = {'date':date_list,
                'update':update_list,
                'serverip':serverip_list}

data_df = DataFrame(merge_dt_dict)
# add one column add_column and set values=1 
data_df['add_column']  = add_list

```

##### 3. 合并2个DataFrame
   
（结构完全一样的2个DataFrame）
按行合并
```python
pd.concat([df1,df2],axis=0)
```
![](/assets/img/2022-09-22-paddle合集/2022-09-23-14-07-06.png)

按列合并
```python
pd.concat([df1,df2],axis=1)
```
![](/assets/img/2022-09-22-paddle合集/2022-09-23-14-08-00.png)

##### 4. DataFrame打乱数据和随机采样
   
```python
df.sample(frac=1)
```
其中参数frac是要返回的比例，比如df中有10行数据，我只想返回其中的30%,那么frac=0.3。

如果需要打混后数据集的index（索引）还是按照正常的排序。我们只需要这样操作
```python
df.sample(frac=1).reset_index(drop=True)
```

使用Sklearn库
```python
from sklearn.utils import shuffle
df = shuffle(df)
```

##### 5. dataframe取每一行的最大值/最小值,类似argmax的意思
idxmax(axis=1) | idxmin(axis=1)
```
predict = pd.read_csv("test_results.tsv",sep="\t",header=0,index_col=None).idxmax(axis=1)
```

##### 6. 创建一个新的空的DataFrame
```
compire = pd.DataFrame(None,index=None,columns=["predict","answer"])
```

##### 7. 比较两列是否一致
:boom:要用lambda，有人测试了这个方法最快
```
t = compire.apply(lambda x:x["predict"]==x["answer"],axis=1)
compire["是否一致"] = t
```

##### 8. 按照某一列取满足条件的行
比如：按照“是否一致”这列，取其中值为True的行
```
right = compire[compire.是否一致==True]
```

##### 9. 针对某一列数据/一个series,逐个元素进行相同的处理
比如：删去这个series的每一行数据的前后括号
```
def qukuohao(str):
    return str[1:-1]

predict = predict.map(qukuohao)
```
![](/assets/img/2022-09-22-paddle合集/2022-10-10-17-38-08.png)


##### 10. 统计label列的类别分布情况
```
M1:data['positionName'].value_counts().sort_values(ascending=False)
M2:data['positionName'].value_counts()
```
![](/assets/img/2022-09-22-paddle合集/2022-11-14-11-01-16.png)

sort_values()函数的具体参数

用法：
DataFrame.sort_values(by=‘##’,axis=0,ascending=True, inplace=False, na_position=‘last’)

参数说明
by：表示根据什么字段或者索引进行排序，可以是一个或多个
axis：排序是在横轴还是纵轴，默认是纵轴axis=0
ascending：排序结果是升序还是降序，默认是升序
inplace：表示排序的结果是直接在原数据上的就地修改还是生成新的DatFrame
kind：表示使用排序的算法，快排quicksort,，归并mergesort， 堆排序heapsort，稳定排序stable ，默认是 ：快排quicksort
na_position：缺失值的位置处理，默认是最后，另一个选择是首位
ignore_index：新生成的数据帧的索引是否重排，默认False（采用原数据的索引）
key：排序之前使用的函数

![](/assets/img/2022-09-22-paddle合集/2022-11-14-11-06-15.png)



![](/assets/img/2022-09-22-paddle合集/2022-11-14-11-12-21.png)
上面的例子很好地显示了参数key的使用，解释下上面两行代码的运行结果，我们对col3字段排序：

默认情况下，字母是按照它们对应的ASCII码进行比较的(A-65,a-97)；所以升序的结果就是：BDFace
加上了key参数，我们写了一个匿名函数lambda，作用是将col3中的字符串全部变成小写字母，这样升序自然是aBcDeF，因为此时的BDF变成了bdf



https://blog.csdn.net/MsSpark/article/details/83154128
https://blog.csdn.net/qq_25443541/article/details/118697711