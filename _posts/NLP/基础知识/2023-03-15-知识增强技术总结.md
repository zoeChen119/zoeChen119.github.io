---
title: 知识增强技术总结
categories: [NLP,基础知识]
tags: [nlp,知识增强,技术综述]     # TAG names should always be lowercase
---



# 知识增强的价值关键
预训练语言模型目前的瓶颈：
1. 不可解释，黑盒模型
2. 下游任务需要大量标注数据
虽然少样本中人工给出思维链提示的成本很小，但这种注释成本相对于微调还是令人望而却步（也可以用synthetic data generation合成数据生成, or zero-shot generalization零样本泛化来处理这个问题）。
1. 推理能力差

ChatGPT和之前PLM的创新：
1. 小样本提示学习和指令学习
2. 思维链（Chain of Thought，COT）补充了逻辑推理过程（知识）给模型
3. 基于人类反馈的强化学习（Reinforcement Learning with Human Feedback，RLHF）
4. 训练数据中补充了过程性知识（代码）

ChatGPT在专业性强的问题上“一本正经的胡说八道”
1. 提高结果的可信度【知识】
2. 提高推理能力，改善模型的可解释性差的问题【知识】
![](/assets/img/2023-03-15-知识增强技术总结/2023-03-15-11-16-55.png)
# 一、技术研究：知识增强
## 【What】知识是什么？
1. 从业务的角度：事实知识（陈述性知识）、机理知识（过程性知识）、数据知识

2. 从研究的角度：“知识”有两种不同的分类方法[灰色色块为研究中常用的，可实现的数据类型]
  2.1. 按照不同来源分类：内部知识、外部知识
![](/assets/img/2023-03-15-知识增强技术总结/2023-03-15-10-39-43.png)

  2.2. 按照不同性质分类 [根据北大《人工智能原理》]
“可变性”：那么知识分为静态/动态；
“可理解性”：那么知识分为表层/深层；
“内容的性质”：那么知识分为陈述性/过程性；
...
![](/assets/img/2023-03-15-知识增强技术总结/2023-03-15-10-40-06.png)

## 【How】如何表征知识？
![](/assets/img/2023-03-15-知识增强技术总结/2023-03-15-10-40-14.png)
根据上面的分类，内部或外部知识通常用(1)实体（三元组中的实体）词典；(2)知识图谱；(3)纯文本直接作为补充知识；(4)与上下文有关系的图像。
### 确定性知识和不确定性知识
#### 确定性知识：
通常用(1)语义网络；(2)知识图谱；(3)框架语言frame；(4)一阶逻辑；(5)命题逻辑；(6)模态逻辑；(7)描述逻辑；(8)本体...，他们分别由各自的使用情况和局限性。
![](/assets/img/2023-03-15-知识增强技术总结/2023-03-15-10-40-23.png)
#### 不确定性知识：
不确定性知识指的是不精确（imprecise）、不完全（incomplete）、随机性（stochastic）的知识。
表示方法如下
![](/assets/img/2023-03-15-知识增强技术总结/2023-03-15-10-40-52.png)
### 过程性知识和陈述性知识
> 过程性知识描述“怎么做”的知识，描述解决问题的过程，通常也是从已有的知识中整理出来的规则，它具有动态的特征，不同情况不同任务下动态变化。
陈述性知识描述“是什么”的知识，往往是事实性知识，包括事物、事件、过程描述、属性、关系这些知识。


<table><tr><td bgcolor=PowderBlue>💡  ChatGPT引入了代码数据作为预训练数据</td></tr></table>

## ---KEPLM：引入Knowledge到PLM---
## 【How】（知识）增强/注入的方法
PLM的网络结构层有：Input、Embedding层、Encoder层
PLM的训练任务有：Masked Language 掩码任务和NSP下一句预测任务
<p align="right">💡这些地方都可以作为知识注入的切入口。</p>

### M1：修改Input
![](/assets/img/2023-03-15-知识增强技术总结/2023-03-15-10-46-04.png)
#### 思路1：
1. 在知识图谱中找$e1$对应的实体的三元组插入在input文本中
2. 把$e1$对应的实体描述插入在input文本中
![](/assets/img/2023-03-15-知识增强技术总结/2023-03-15-10-46-20.png)
例子：

<details> 
    <summary>2个例子</summary>
    <center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    width = "300" height = "200"
    src="/assets/img/2023-03-15-知识增强技术总结/2023-03-15-10-47-44.png" width = "60%" alt=""/>
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
      ERNIE 3.0 框架图
  	</div>
    </center>
    <center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    width = "300" height = "200"
    src="/assets/img/2023-03-15-知识增强技术总结/2023-03-15-10-52-37.png" width = "60%" alt=""/>
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
      一篇来自中文信息处理实验室的发布在AAAI的论文：Benchmarking Knowledge-Enhanced Commonsense Question Answering via Knowledge-to-Text Transformation.
  	</div>
    </center>
</details>


#### 思路2：
先把原始input文本组织成图结构，再和来自于知识图谱中的子图拼接（可以根据input中的实体词或者其他），构建成补充后的图结构，再重新展平成文本序列的形式作为PLM的输入。
![](/assets/img/2023-03-15-知识增强技术总结/2023-03-15-10-53-41.png)


<details> 
    <summary>2个例子</summary>
    <center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    width = "300" height = "200"
    src="/assets/img/2023-03-15-知识增强技术总结/2023-03-15-10-53-59.png" width = "60%" alt=""/>
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
      [ACL]CoLAKE 知识增强
  	</div>
    </center>
    <center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    width = "300" height = "200"
    src="/assets/img/2023-03-15-知识增强技术总结/2023-03-15-10-54-34.png" width = "60%" alt=""/>
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
      [AAAI]K-BERT 知识增强
  	</div>
    </center>
</details>



### M2：在Encoder层中增加知识融合模块
(1) on top of the entire PLM：我们可以在n个Encoder整体之后增加一个知识融合模块；
(2) between the Transformer layers of PLM：我们也可以单层的Encoder增加知识融合模块，这样n个Encoder就重复n次；
(3) inside the Transformer layers of PLM：Encoder包含着很多子层比如多头自注意力层，feed forward层等等，我们也可以在这其中插入知识融合模块。

![](/assets/img/2023-03-15-知识增强技术总结/2023-03-15-10-55-30.png)

<details> 
    <summary>三个例子</summary>
    在n个Encoder整体之后增加知识融合模块的例子：
    <center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    width = "300" height = "200"
    src="/assets/img/2023-03-15-知识增强技术总结/2023-03-15-10-55-44.png" width = "60%" alt=""/>
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
      清华ERNIE
  	</div>
    </center>
    在单层的Encoder增加知识融合模块的例子：
    <center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    width = "300" height = "200"
    src="/assets/img/2023-03-15-知识增强技术总结/2023-03-15-10-55-50.png" width = "60%" alt=""/>
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">  
  	</div>
    </center>
    在Transformer层内部增加知识融合模块的例子
    <center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    width = "300" height = "200"
    src="/assets/img/2023-03-15-知识增强技术总结/2023-03-15-10-56-12.png" width = "60%" alt=""/>
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
      [ACL]KALA
  	</div>
    </center>

</details>



### M3：增加或修改预训练任务
(1) 修改掩码任务
![](/assets/img/2023-03-15-知识增强技术总结/2023-03-15-10-58-39.png)

<details> 
    <summary>1个例子</summary>
    <center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    width = "300" height = "200"
    src="/assets/img/2023-03-15-知识增强技术总结/2023-03-15-10-59-16.png" width = "60%" alt=""/>
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
      [中科院]E-BERT
  	</div>
    </center>
</details>

(2) 增加知识相关的预训练任务
![](/assets/img/2023-03-15-知识增强技术总结/2023-03-15-11-00-00.png)

<details> 
    <summary>1个例子</summary>
    <center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" 
    width = "300" height = "200"
    src="/assets/img/2023-03-15-知识增强技术总结/2023-03-15-11-00-09.png" width = "60%" alt=""/>
    <br>
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">
      [ACL Trans]KEPLER
  	</div>
    </center>
</details>

## 【评估】如何评估KEPLM的好坏
过去，评价KEPLM的优劣通常通过它生成的Representation（表征）的**理解能力**好坏来定义。
![](/assets/img/2023-03-15-知识增强技术总结/2023-03-15-11-00-52.png)
现在以及以后，KEPLM的**推理能力**将是我们更关注的点，因为它们已经在GLUE/CLUE任务上表现得足够好了。
![](/assets/img/2023-03-15-知识增强技术总结/2023-03-15-11-00-57.png)
<table><tr><td bgcolor=PowderBlue>💡  ChatGPT引入了思维链CoT增强PLM的推理能力</td></tr></table>


# Reference
	标题	文献来源	发表年份
1	A Survey on Knowledge-Enhanced Pre-trained Language Models	IEEE TRANS	2023.01
2	A Survey of Knowledge Enhanced Pre-trained Models		2022.06
3	A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models		2022.02
4	知识图谱构建技术综述	计算机工程	2022
5	新一代知识图谱关键技术综述	计算机研究与发展	2022
6	A Survey on Knowledge Graphs: Representation, Acquisition and Applications	IEEE transactions	2021
7	CoLAKE: Contextualized Language and Knowledge Embedding	ACL	2020
8	KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation	Transactions of ACL	2021
9	Reasoning About Knowledge		2003.01
10	On Commonsense Cues in BERT for Solving Commonsense Tasks	ACL-IJCNLP 2021	2021.08
11	What does BERT learn about the structure of language?	ACL	2019.07
12	A Structural Probe for Finding Syntax in Word Representations	NAACL-HLT 2019	2019.06
13	A Closer Look at How Fine-tuning Changes BERT	ACL	2022.03
14	DirectProbe: Studying Representations without Classifiers	ACL	2021.04
15	Enhancing Self-Attention with Knowledge-Assisted Attention Maps	NAACL 2022	
16	SKILL: Structured Knowledge Infusion for Large Language Models	NAACL 2022	
17	KroneckerBERT: Significant Compression of Pre-trained Language Models Through Kronecker Decomposition and Knowledge Distillation	NAACL 2022	
18	Modularized Transfer Learning with Multiple Knowledge Graphs for Zero-shot Commonsense Reasoning	NAACL 2022	
19	KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning	AAAI	2021.01
20	Chain of Thought Prompting Elicits Reasoning in Large Language Models	NeurIPS	2023.01
21	Training Verifiers to Solve Math Word Problems		2021.09
22	JAKET: Joint Pre-training of Knowledge Graph and Language Understanding	AAAI	2021.03
23	Memory and Knowledge Augmented Language Models for Inferring Salience in Long-Form Stories	EMNLP	2021.08
24	Benchmarking Knowledge-Enhanced Commonsense Question Answering via Knowledge-to-Text Transformation.	AAAI	2021.03
25	Entities as Experts: Sparse Memory Access with Entity Supervision	EMNLP	2020
26	ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation		2021
27	AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts	EMNLP	2020.08
28	Semantics-aware BERT for Language Understanding	AAAI	2020.05
29	KALA: Knowledge-Augmented Language Model Adaptation		2022.04
30	Knowledge-driven Natural Language Understanding of English Text and its Applications	AAAI	2021
31	Common Sense or World Knowledge? Investigating Adapter-Based Knowledge Injection into Pretrained Transformers		2020
32	KgPLM: Knowledge-guided Language Model Pre-training via Generative and Discriminative Learning.		2020
33	KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning	AAAI	2020.11
34	SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis		
35	DKPLM: Decomposable Knowledge-enhanced Pre-trained Language Model for Natural Language Understanding	AAAI	2022
36	Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing.	ACM	2021.07
37	CoLAKE: Contextualized Language and Knowledge Embedding	ACL	2020.08
38	LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention	EMNLP	2020

