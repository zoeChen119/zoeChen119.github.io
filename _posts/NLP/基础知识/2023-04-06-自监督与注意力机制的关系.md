# Transformer、BERT和“自监督学习”、“自注意力机制”的关系

#### Transformer和BERT这两个模型到底算是“自监督学习”还是“自注意力机制”？正确的圈定到底是什么？

> “交流中碰撞思想，沟通中凝聚共识”，今天跟王博的讨论时启发我关注到自监督学习这一块，之前对于BERT家族的理解基本都是从注意力机制这一条路探究的，受益匪浅，原来预训练任务还可以从自监督学习这个方向去定义。

自监督和自注意力应该是属于两个山头，在AAAI2020大会上，Hinton、LeCun和Bengio分别发表了主旨演讲：**Hinton**主张无监督版本的**Capsule网络**；**LeCun**主张**自监督学习**；**Bengio**主张**注意力机制**。
![](/assets/img/2023-04-06-自监督与注意力机制的关系/2023-04-06-11-09-02.png)
> 插嘴一下，**LeCun**是**Hinton**的博士后，在麻省理工学院时**Bengio**又是**Jordan**的得意门生，随后**Bengio**在贝尔实验室与**LeCun**成为同事。Hugo Larochelle在**Bengio**下面读的博士，后成为**Hinton**的博士后；**LeCun**的一位博士生MarcAurelio Ranzato，后也成为的**Hinton**的博士后。


我查阅了在Transformer的原文《Attention is All You Need》中全文没有提到自监督学习，同样在BERT的原文《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》中也是全文没有提到自监督学习。这两篇Paper都是Google AI Lab提出的，而Google Brain的机器学习科学家是**Bengio**的兄弟**Samy Bengio**。

![](/assets/img/2023-04-06-自监督与注意力机制的关系/2023-04-06-10-50-45.png)

截至2023年，大家公认的预训练语言模型的关键技术分为三大块：
1. 网络结构上说：Encoder-Decoder架构 & **多头自注意力机制**
2. 预训练任务上说：隶属于“**自监督学习**”机制的预训练任务MLM和NSP
3. 下游任务上说：Fine-tune和Prompt-predict

所以，
AI江湖上称：
> **Hinton**是AI教主，始作俑者，开创先河；
> **Lecun**是独行侠，负责东搞西搞，工业学术两不耽误；
> **Bengio**是金牌打手，坚守学术界阵地，做理论实验支持。

相当有道理啊，现在主导AI的Transformer/BERT/GPT这一个家族的模型都是以**自注意力机制**为“骨”，以**自监督学习**为“血液”，而**自注意力机制**是Lecun提出的，**自监督学习**则是Bengio提出的。

----
#### Reference：
1. [预训练语言模型的进展与趋势](https://www.h3c.com/cn/d_202201/1763577_233453_0.htm)
2. [【AI大咖】扒一下低调的Yoshua Bengio大神](https://zhuanlan.zhihu.com/p/66259338)
3. [三巨头共聚AAAI:Capsule没有错，LeCun看好自监督，Bengio谈注意力](https://baijiahao.baidu.com/s?id=1658221832783032723&wfr=spider&for=pc)
4. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding ](https://readpaper.com/paper/2963341956)
5. [Attention Is All You Need](https://readpaper.com/paper/2963403868)