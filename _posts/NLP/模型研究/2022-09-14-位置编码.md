---
title: 深入研究BERT位置编码的原理
date: 2022-09-14 10:10:56 +/-0800
categories: [NLP,模型研究]
tags: [nlp,语言学,BERT]     # TAG names should always be lowercase
---
# 深入研究BERT位置编码的原理

记录日期：2022/08/29
                                                                                              

* * *



    📃参考文章
        [1] 苏建林博客
        [2] Transformer学习笔记一：Positional Encoding（位置编码）
        [3] 知乎.实践中BERT如何对长度大于500的文本进行处理？
        [4] 位置编码全面研究-好文！💎
        
        [5] how to generate position embedding heatmap
        [6] The Illustrated Transformer
        [7] BERT的三个Embedding详解

### 0：什么是位置编码？为什么破解BERT处理长文本的问题要研究位置编码？
#### 研究位置编码的动机
在transformer的encoder和decoder的输入层中，使用了Positional Encoding，使得最终的输入满足：
input=input_embedding+positional_encoding

这里，input_embedding是通过常规embedding层，将每一个token的向量维度从vocab_size映射到d_model，由于是相加关系，自然而然地，这里的positional_encoding也是一个d_model维度的向量。（在原论文里，d_model = 512）

    📌通过常规embedding层，将每一个token的向量维度从vocab_size映射到d_model
    这句话表述的流程如下：

![](/assets/img/2022-09-14-位置编码/2022-09-19-17-29-39.png)


那么，我们为什么需要position encoding呢？在transformer的self-attention模块中，序列的输入输出如下（不了解self-attention没关系，这里只要关注它的输入输出就行）：
![](/assets/img/2022-09-14-位置编码/2022-09-19-17-42-28.png)


    在self-attention模型中，输入是一整排的tokens，对于人来说，我们很容易知道tokens的位置信息，比如：
    （1）绝对位置信息。a1是第一个token，a2是第二个token......
    （2）相对位置信息。a2在a1的后面一位，a4在a2的后面两位......
    （3）不同位置间的距离。a1和a3差两个位置，a1和a4差三个位置....
    但是这些对于self-attention来说，是无法分辩的信息，因为self-attention的运算是无向的。因为，我们要想办法，把tokens的位置信息，喂给模型。
                                                *以上引自[2] Transformer学习笔记一：Positional Encoding（位置编码）*

为什么？
在做NLP的相关任务中，最常见的一个问题便是当输入序列过长时应该如何进行处理。例如在谷歌开源的预训练模型中，最大长度只支持512个Token。

大家都知道，目前的主流的BERT模型最多能处理512个token的文本。导致这一瓶颈的根本原因是BERT使用了从随机初始化训练出来的绝对位置编码，一般的最大位置设为了512，因此顶多只能处理512个token，多出来的部分就没有位置编码可用了。当然，还有一个重要的原因是Attention的O(n2*d)复杂度，导致长序列时显存用量大大增加，一般显卡也finetune不了。[1]

对于原因一应该怎么进行处理呢？是简单的进行截断处理吗？
总的来说，对于这类问题可以有两种方式来进行解决：第1种是从模型入手，例如消除最大长度为512的限制；第2种是从输入入手，改变输入序列的长度重新构造任务。[3]

对于原因二？
稀疏注意力

#### 位置编码[2]：
##### • 绝对位置（最自然的想法）
一种自然而然的想法是，给第一个token标记1，给第二个token标记2...，以此类推。
这种方法产生了以下几个主要问题：
（1）模型可能遇见比训练时所用的序列更长的序列。不利于模型的泛化。
（2）模型的位置表示是无界的。随着序列长度的增加，位置值会越来越大。
##### • 用浮点数表示（3个字：[0,0.5,1]）
为了解决整型值带来的问题，可以考虑将位置值的范围限制在[0, 1]之内，其中，0表示第一个token，1表示最后一个token。比如有3个token，那么位置信息就表示成[0, 0.5, 1]；若有四个token，位置信息就表示成[0, 0.33, 0.69, 1]。
但这样产生的问题是：
（1）当序列长度不同时，token间的相对距离是不一样的。
    例如在序列长度为3时，token间的相对距离为0.5；在序列长度为4时，token间的相对距离就变为0.33。
因此，我们需要这样一种位置表示方式，满足于：
（1）它能用来表示一个token在序列中的绝对位置
（2）在序列长度不同的情况下，不同序列中token的相对位置/距离也要保持一致
（3）可以用来表示模型在训练过程中从来没有看到过的句子长度。
##### • one-hot编码（[0,1]标记）
考虑到位置信息作用在input embedding上，因此比起用单一的值，更好的方案是用一个和input embedding维度一样的向量来表示位置。
input embedding和位置编码怎么concat见：下一节 | 先说Transformer
这时我们就很容易想到二进制编码。如下图，假设d_model = 3，那么我们的位置向量可以表示成：
![](/assets/img/2022-09-14-位置编码/2022-09-19-17-44-17.png)

这下所有的值都是有界的（位于0，1之间），且transformer中的d_model本来就足够大，基本可以把我们要的每一个位置都编码出来了。

但是这种编码方式也存在问题：
（1）这样编码出来的位置向量，处在一个离散的空间中，不同位置间的变化是不连续的。这样无法体现相对位置关系。

    假设d_model = 2
    input sequence=4
    sequence vector=[
                    [0,0,1,...,0]1 * vocab_size , 
                    [0,0,...,1,0]1 * vocab_size , 
                    [0,1,0,...,0]1 * vocab_size , 
                    [0,0,...,0,1]1 * vocab_size
                    ]
    sequence vector经过embedding层变成
    token embedding vector=[
                    [0.xx,0.xx]1 * d_model=1 * 2 ,
                    [0.xx,0.xx]1 * d_model=1 * 2 ,
                    [0.xx,0.xx]1 * d_model=1 * 2 ,
                    [0.xx,0.xx]1 * d_model=1 * 2 ,
                    ]
                                
我们有4个位置需要编码，这四个位置向量可以表示成[0,0],[0,1],[1,0],[1,1]。
然后我们就可以把token embedding vector和位置向量加起来。
我们把它的位置向量空间做出来：

    🌀按理说，[0,0]和[1,0]的距离是2，[0,0]和[1,1]的距离是3，[0,1]和[1,0]的距离是1，但显然图上黑线不是这样表示相对距离的，我认为（索引）这种一维序列本身就不适合用高维向量表示。
<u>如果我们能把离散空间（黑色的线）转换到连续空间（蓝色的线）</u>，那么我们就能解决位置距离不连续的问题。同时，我们不仅能用位置向量表示整型，我们还可以用位置向量来表示浮点型。
##### • 连续空间的编码（一维表示“索引”）
![](/assets/img/2022-09-14-位置编码/2022-09-19-17-46-01.png)

用连续函数编码，那么要保证①每个位置都有独一无二的编码，②编码空间不是无限域。
出于②的原因，正余弦函数是一个很好的选择，值域不是无穷区间，在[-1,1]。
如果我们是用绝对位置也就是0，1，2，3，...这些用正余弦函数sin,cos来编码，自然会出现不同位置有相同的编码结果值。例如下面图中的第0位和第6位就是相同的编码结果。

###################################################

正弦图像是具有周期性的，不巧的话就会出现不同的位置是一个值，真是那样的话，位置编码的意义就失去了，如何让不同词之间分开是位置编码的目的，接下来引入i，依照公式，只有i为偶数才为正弦图，引入i之后代表进一步细分，不止是按照一个一个词来分，<u>而是按照词和词嵌入的特征来分</u>，其实可以想想，人其实大多数情况的行为举止都是差不多的，细分才能看出不同，这里用了 p0 和 p6 第1，3，5个特征：
<u>按照词和词嵌入的特征来分</u>：也就是说，如果我们用维度当做正选函数的周期，位置越远正余弦函数的震荡周期越大，那么即使是同一个位置在不同词的表征向量中也能得到不同的数值。

                     位置1   位置2     位置3
    tensor([[ 0.4924,  0.3564,  0.4850],词1的表征向量
            [ 1.0244, -0.0162, -0.0017],词2的表征向量
            [ 0.6738,  0.7967,  0.6629]])词3的表征向量
![f30695e06a59a713fa45ceb33544cc4d.jpeg](en-resource://database/517:1)

不难发现，不仅不同词之间分开了，同一个词不同的特征之间也能区分出来
当两个参数都变得很大的时候，又会发生什么呢？
transformer默认的词嵌入维度是512，这里举个特例，假如句子很长，以第101个词为例
```math
y = sin( \frac {100} {10000^{(2i/512)}})
```
![5d29a3a43b46ed8963bf24de6029e8ce.png](en-resource://database/519:1)

显然，前期有大量特征会出现重合，且会出现较高频率变换，后期几乎趋于一致（奇数情况也如此），我们其实可以猜想出后面的其实几乎不带有位置信息，下图为奇偶混合的情况，可以看出有很多重合，当i较大时，两者都不带任何信息（第三节会有验证）
下图为小demo的位置编码的可视化结果
![0ed7dcc56e0e08aa8f45a9e85400ab95.png](en-resource://database/521:1)

深度是一个词的特征维度，Position代表的是哪一个词
>为什么会出现：当i较小时，特征之间重合较大，而且离得近的位置之间特征变化很快，当i大到一定程度，几乎没有任何区别

我的猜想是当词与词之间离得很近时（pos差得小）或者对于一个词不同特征来说，携带信息相同其实是符合逻辑的。在word2vec里面，如果两个词意相同，转换为坐标空间内的向量就会离得近（对应于一个词不同特征），有时候，一个句子之间离得近的词可能有关联效果，如"I love reading books."，reading和books它们一起出现的概率是较大的（对应于离得近的词）。
>位置编码是加上去的，为什么不拼接上去呢？

拼接的话会扩大参数空间，占用内存增加，而且不易拟合，而且其实没有证据表明拼接就比加来的好
>那么，既然有了公式，那位置编码是算出来的还是学出来的呢？

其实算出来和学出来的效果差不多，但是考虑到算出来的可以接受更长的序列长度而不必受训练的干扰，所以在这个模型中，位置编码是通过公式算出来的。

* “不足”：

先对位置编码进行可视化（见下图，这里深度变成了512，与Transformer默认参数保持一致）
先看第一列，即每个词的第一个维度，可以发现相似度变化很大，有差不多相似的，但也有几乎无关的，相似的原因猜测是序列所需要的（之所以不用决定是因为提前公式决定的和学出来的差不多），每个句子中都有一个词与另一个词词义相近。因而一开始看好像在低维时位置编码似乎不错，可是当维度显著提升后，300维之后全部都一模一样的，这显然不符合位置编码设计的初衷（捕捉句子间的序列关系），不妨猜测可能是真正起作用的是较小维度的值或者是经过dropout之后将一部分完全相同的值给剔除掉来一定程度提升模型，可是经过我的语言翻译模型实验发现其实加不加结果差不多。
>总结就是当嵌入维度显著上升后，模型捕捉词与词之间关系的能力大大下降。

![e0fecf04072c55c51b03de2016c3f4f3.jpeg](en-resource://database/523:1)

>既然400维往后都一样，全部不要怎么样？

接下来我将400维及其后面的全部重置为0（见上），进行了seq2seq法译英任务（基于Transformer），结果是几乎无影响，左图为未做改变的训练和测试准确率图，下面翻译的结果也很丝滑。结果放到了colab，对结果存有疑虑可以去复现一下，请确保你对每一个部分都很了解，复制代码到你本地或者云端，按照注释来添加不同的对照组。

![5f05e01b01adcc399f7583100ca3c660.jpeg](en-resource://database/525:1)

`input: je pars en vacances pour quelques jours .
target: i m taking a couple of days off .
pred: <start> i m leaving on vacation days .

input: je recherche un assistant .
target: i am looking for an assistant .
pred: <start> i am looking for an assistant .`
因为一开始为了收敛迅速选取了长度小于10的中短句子，为了防止因为句子因素而造成的影响，下面特地选取了长度在 [11,17] 的句子，（注意这次实验并没有短句子）作为对照组（考虑到这个数据集大部分都是中短句子，长句子并不多，因而模型未充分拟合，正确率维持在 74% ，好在不影响实验），左图为原始的：
![31968bb03715b5d009515a62aedbf250.jpeg](en-resource://database/527:1)

考虑到即使位置不对，词对了准确率依然很高，所以还随机地让模型翻译句子，结果表明如果句子长度是中短型的，似乎模型可以学习足够的位置信息而不依赖于位置编码，可是当句子过长时，即使是位置编码的400维以后也发挥了作用。下面选取了两个例子，一个是较短的，另一个较长（上面的图为原始的）：
有趣的是，位置编码的400维以后的信息依然发挥了作用，可以看见注意力图相对来说左侧更加整齐，同时因为两者整体说都不整齐，说明短句子的位置信息没有那么的明确，而400维置为0的也学到了较为重要的位置信息，因而可以翻译的与原始的相当：
![afb54d5924f96b78de1149214099cbb6.png](en-resource://database/529:1)

可是，句子较长的时候，模型不能够依赖自学而获取位置信息，而原始的却依然能够沿轴线对齐，保持位置信息：
![425dee9fc441345dc0d4fed9c7a9a2cf.jpeg](en-resource://database/531:1)

>如果直接不要位置编码，会怎么样？

在短句子中两者无论从准确度还是翻译上来看都是相当的。这也说明了位置编码在短句子（长度小于10）和固定开头的句子中并非发挥应有的作用，句子简单时很可能机器能够自己学会位置信息，这也可以推出短句时加完位置编码后根本没必要Dropout，因为信息本来就不重要嘛
![a703cdf7147488723ddd3a7d3cb4bf3a.jpeg](en-resource://database/533:1)

`input: je pars en vacances pour quelques jours .
target: i m taking a couple of days off .
pred: <start> i m taking a couple of days off .

input: je recherche un assistant .
target: i am looking for an assistant .
pred: <start> i am looking for an assistant .`

可是当遇到长句子时，位置变多，没有位置编码几乎都是随机排列的，准确率几乎一致，说明词还是都能猜的差不多，猜词和位置编码没关系：
![211466cc3124a214795b0b759cf79893.jpeg](en-resource://database/535:1)

虽然是乱序，但是模型在较短的时候还是能够自己独立学会位置信息
![baaa2f733c7bb0b8bb228227f9630b79.png](en-resource://database/537:1)

可是真正遇到长句子时，没有位置编码就完全乱套了
![c83083e800726586875bfe8e2f73767b.jpeg](en-resource://database/539:1)

<center> **以上引自[4]** </center>



#### 01：BERT和Transformer的位置编码的区别
Transformer中的位置编码（PE）

在Transformer中，位置编码是由sin/cossin/cos函数生成的固定值。

具体做法：<u>用不同频率的正余弦函数对位置信息进行编码</u>，位置编码向量的维度与文本编码向量的维度相同。因此二者（位置编码向量，文本编码向量）可以直接相加作为token最终的编码向量。

#### 02：Experiment

实验一：sin&cos交叉、纯sin、纯tanh
说明：位置编码的函数三种设置方案，预训练之后position embedding的向量结果可视化对比

    🔮 
         基础模型：google_zh_model
         预训练数据：corpora/book_review_bert.txt
         预训练后得到的模型： models/book_review_model.bin

512长度的长文本预训练后的位置编码图像
sin&cos交叉
![4787ba3e5cadcf7aa27c2eaf0e67a010.jpeg](en-resource://database/545:2)
sin
![4787ba3e5cadcf7aa27c2eaf0e67a010.jpeg](en-resource://database/545:2)
tanh
![f55190277b067b663eb3a95cdfff3682.jpeg](en-resource://database/547:1)


