---
title: BERT百科大全
date: 2022-04-14 13:10:56 +/-0800
categories: [NLP,模型研究]
tags: [nlp,BERT,面经]     # TAG names should always be lowercase
---

# BERT百科大全

## Attention
1. attention结构的神经网络与递归神经网络相比有什么优点
   答：
   * attention结构的输入是句子即可，递归神经网络的输入需要包含句子和句子结构
   * 训练Recursive neural net之前，你需要句法树；句法树是一个离散的决策结果，无法连续地影响损失函数，也就无法简单地利用反向传播训练Recursive neural net。
2. attention结构的神经网络和循环神经网络相比有什么优点？
   * 循环神经网络是时序的，串行的，不能并行的同时处理句子的所有词，而attention结构是并行的，利用点乘同时计算所有token之间的相似度。
   * 循环神经网络会有梯度爆炸和梯度消失的问题，因为它是前面的不断累乘，可能会越乘越大乘到正无穷，也可能会越乘越小乘到0，但是因为attention是并行点乘的，不会有梯度累计的问题。
   * 也是由于同样的原因，循环神经网络面对长文本的时候会出现遗忘，忘掉比较远的位置的信息。attention不会遗忘，但是由于计算成本，BERT也是设定了512字节的长度限制。
   * RNN只考虑左边的信息，除非用双向RNN，不过双向RNN需要完整的数据序列，才能预测任意位置。比如说你要用双向RNN模型构建一个语音识别系统，你需要等待这个人说完，然后获取整个语音表达才能处理这段语音，并进一步做语音识别。

 
3. attention结构中Q、K、V的含义及作用
   答：Q=Query，K=Key，V=Value，Q点乘K得到Q中每个token的重要性权重，然后再点乘V，这样就能起到给V中每个token加一个权重，使模型学会“有的放矢”。
4. :paw_prints:transformer网络中一共有多少种不同的attention（3种）
   答：[^3种attention]
   * multihead-selfattention:encoder中，Q=K=V。
   * masked-multihead-selfattention: Decoder中，当前位置只能注意到其位置之前的信息，通过将注意力矩阵做mask实现，如图1所示。
   * Cross-attention: query 来自于decoder中上一层的输出，而K 和V使用的是encoder中的输出。
    ![](/assets/img/2022-10-10-BERT百科/2022-10-10-15-57-51.png)

5. 注意力机制有哪几种？
   答：
   1.intra attention和inter attention
   intra attention就是self attention就是Q=K=V，inter attention就是Q和kV不相等，也就说计算两个句子的相似度，比如transformer的decoder部分的cross attention

   2.global attention和local attention
   注意力机制计算的是所有token，local attention仅关注部分token，比如只关注当前词前k个和后k个

   3.soft attention和hard attention
   如果注意力机制能跟随神经网络后向传播的过程中得到优化那么，就是soft attention，否则就是hard attention。


## BERT
1. bert的原理、结构
   答：bert的目标是生成一个更好的语义表征向量，它创新点在于利用注意力机制替代之前的循环神经网络，注意力机制的原理是将输入文本当作三个相同的向量QKV，以Q点乘K计算出该文本中各部分的权重，把这个权重再点乘V，得到对这个句子各部分的一个“有的放矢”的向量表征。
   结构上，bert是Transformer的encoder部分，第一层是embedding层，包含position embedding，segment embedding，token embedding，position embedding是位置编码，采用sin/cos交替的方法编码绝对位置，segment embedding是为了应对句对任务的，标识出token属于哪一个句子，token embedding是一个随机初始的各个token的表征向量；第二层是multihead-selfattention层，这一层的输入是三个embedding concat拼接的一个向量，输出是一个中间语义向量，第三层是残差层和归一化层，残差层是为了:o:解决深度神经网络的退化问题[^残差网络1][^残差网络2]，归一化层是为了将当前层输出的向量做一个限定[^归一化层]，限定其最大值和最小值，这样有助于网络快速收敛，防止梯度爆炸和梯度消失；第四层是一个前馈神经网络；第五层又是一个残差层和归一化层。
   ![](/assets/img/2022-10-10-BERT百科/2022-11-17-10-12-43.png)
2. bert中哪些结构（layer或block）的训练参数比较多
   答：从BERT的时间复杂度就可以看出，它参数最多的地方在注意力块矩阵点乘的地方。
   > 以下为引用[^BERT中的参数]
   总体来说bert模型的参数主要包含3部分：Embeddding层的参数，transformer blocks的参数和最后输出的全连接参数。
   第一部分的参数：
   30522*768+512*768+4*768
   第二部分参数：
   【（768*768+768）*4+（768*2）+（3072*768*2+3072）+768*3】*12
   第三部分参数：
   768*768+768
   参数个数总计：109482240~1.09亿
   而BERT-Base, Chinese BERT-Base, Chinese总是约为1.02亿。
   
3. bert模型采用mask的具体策略，以及为什么要这么设计
答：把输入句子的15%的token替换掉：
   其中被替换的token有80%的概率被替换成[MASK]
   10%的概率被替换成任意一个其他token
   10%的概率原封不动

   让模型预测和还原被遮盖掉或替换掉的部分，损失函数只计算随机遮盖和替换部分的Loss。
   这样做的原因类似于word2vec中的CBOW：给出上下文预测核心词。


4. bert及其变体中常用的mask方式及特点（dynamic mask，whole word mask，phrase mask， entity mask）
答：whole word mask：起源于谷歌的bert-wwm模型，是一种全词屏蔽的方法，这个主要是针对中文的，因为中文的字蕴含的意义远远少于词，并且字屏蔽的话会导致信息泄露，全词屏蔽能够有效的让模型学习语义。百度进一步提出引入命名实体的外部知识，也就是实体屏蔽，在完形填空的测试上，显著提升。
entity mask：实体屏蔽，随机屏蔽句子中的实体，有效的是模型学习句法关系。
动态屏蔽：每次向模型输入一个序列时都会生成新的掩码模式。这样，在大量数据不断输入的过程中，模型会逐渐适应不同的掩码策略，学习不同的语言表征。huggingface中的data allcator也是动态屏蔽，它是每一个epoch的mask策略不同。[^mask的变种]

5. 中文bert-base预训练模型所有的参数量，细分到每一个结构的参数量
答：![](/assets/img/2022-10-10-BERT百科/2022-10-11-21-57-00.png)
https://zhuanlan.zhihu.com/p/452369195
embedding层：(21128+512+2)*768
attention层：()*12

6. bert结构中embedding部分具体是怎么样的（word embedding+position embedding+segment embedding）
答：略

7. 绝对位置和相对位置的区别
答：绝对位置就是1-512，相对位置就比较复杂，可能就需要包含多种标识，比如一种方法是按分词或词组标记位置，一个词内0，1，2...5，后一个词0,1,2.这样也可以用一个start position，一个end position同时标识一个token，标识这个token所在词语的起终位置。
相对位置能帮助模型突破512长度的限制，在处理长文本数据的时候，往往需要这个策略。
相对位置的代表是Nezha，他的创新点就是函数式相对位置编码，和transformer的sincos位置编码的区别是，sincos是sin（绝对位置/10000的d分之2-k次方）而nezha是sin（j-i的相对错位差/10000的d分之2-k次方）[^Nezha相对位置编码]

8. position embedding的实现方式有哪两种
   （functional position embedding，如transformer和华为的NEZHA；parametric position embedding，如bert）
答：函数相对位置编码：Nezha
   函数绝对位置编码：Transformer
   参数绝对位置编码：Bert

## Transformer
1. transformer decoder部分的inference过程
重点看beam search的实现，使用tensorflow或torch框架（尽量按google源码的思路）
答：

2. transformer网络中一共有多少种不同的attention（3种）
答：3种[^3种attention]
（1）self-attention：encoder种的Q=K=V
（2）masked self-attention：decoder中的，当前位置只能看到它前面的token
（3）cross-attention：query来自于decoder中上一层的输出，而K和V来自于Encoder的输出。
## GPT
1. 如何用gpt-3的prompt机制挖掘以新冠疫情为主题的稀疏文本以及如何在保险场景实现zero-shot；
2. 简单聊一下gpt-1到gpt-3的发展历程
   答：gpt-1:相比于bert，多头自注意力机制被替换成了masked多头自注意力。仍然是无监督预训练，有监督微调
   gpt-2:相比于gpt-1,去掉了微调层，通过引入多任务学习，不再针对下游任务微调模型，gpt2能自动识别是什么任务然后完成任务
   修改了layer normalization的位置，放在了sub-block之前，并在最后一层selfattention层之后增加了一层layer normalization
   gpt-3进一步加大了参数量175billion


## ERNIE
1. 简单聊一下ERNIE1.0到3.0发展历程
   答：1.0版本使用了三种mask结合，传统的mask+wwm+entity mask；使用了大量的异质数据预训练
   2.0版本使用持续性的多任务学习，每次有新任务过来，就用上一个任务训练的参数，同时训练新任务和旧任务
   3.0版本引入了知识图谱

## 拓展模型

1. deberta中的两个优化点是什么？
   disentangled attention和enhanced mask decoder
2. RoBERTa相比与BERT的改进？
   答：1.去掉了NSP任务
   2.使用了动态mask
   3.更大更细粒度的词汇表，用更大的 byte 级别 BPE 词汇表来训练 BERT，这一词汇表包含 50K 的 subword 单元，且没有对输入作任何额外的预处理或分词。
3. SKEP相比BERT的改进？
   答：SKEP主要考虑的语言模型在情感词的解析能力，它通过在mask上引入情感词mask，属性词-情感词对mask，mask通用字来促使模型习得情感词的语义。[^SKEP]
4. Nezha的改进？
   答：1.函数式相对位置编码
   2.Nezha使用jieba分词完成wwm
   3.混合精度训练
   4.Lamb优化器
5. XLNet
   自回归模型解决mask的问题
   引入transformer-xl
6. transformer-xl
   答：首先取第一个段进行计算，然后把得到的结果的隐藏层的值进行缓存，第二个段计算的过程中，把缓存的值拼接起来再进行计算

## 优化方法
1. 模型蒸馏的具体实现过程

2. 损失函数是什么？
   答：损失函数是通过样本来计算模型分布和目标分布之间的差异

3. KL散度和交叉熵的联系与区别
   答：如果目标分布是常数，比如分类的分布是已知且不变的，这时就用交叉熵来作为损失函数。
   KL散度更通用的一种计算两个分布差异，但是交叉熵运算更简单。
   > KL散度和交叉熵都可以用来作为模型的loss函数，但二者的使用场景不一样。在这里引申一下模型loss的含义：“通过样本来计算模型分布与目标分布间的差异。”，这就是KL散度的作用。但有时候我们的目标分布会是常数，也就是这个分布是已知且不变的，例如分类任务，这个时候我们就会使用交叉熵来衡量模型的预测分布与实际分布之间的差异。
   
4. 常用的文本数据增强方法
   （全面完整的回答可以从语种层面，letter、subword、word和语序层面，利用mlm过程、利用wordnet近义词替代等角度）
答：
（1）中英文：利用同义词反义词词典替换、语序颠倒、增加删除否定词、反向翻译
（2）英文：利用字母替换，词根词缀替换
4. 如何在一个3GB内存空间中部署一个深度学习模型；
5. 说一下对多任务训练（multi task learning）和多领域训练（multi domain learning）的理解，最好举一个例子
答：多任务学习：[^多任务学习]
![](/assets/img/2022-10-10-BERT百科/2022-10-11-22-34-07.png)
单任务学习（single task learning）：一个loss，一个任务，例如NLP里的情感分类、NER任务一般都是可以叫单任务学习。
多任务学习（multi task learning）：简单来说有多个目标函数loss同时学习的就算多任务学习。也就是一个模型，后面分别接几个特定层分别计算不同的目标loss，有硬参数共享，也就是共享层训练，特定层隔离自己训练自己的，有软参数共享，也就是特定层每一层都在训练过程中互相交互着。
多任务学习的好处是：有效节省计算资源的前提下，学习数据的多维度特征。
多领域学习：
多任务学习有一些弊端，一是需要样本标注好多个维度的标签，耗费人力，二是机器学习训练数据遵循独立同分布假设，可是需要多角度学习数据那肯定是有各自不同的侧重，也就是违背了同分布的假设。
多领域学习引入了MMD-loss，来对多领域的特征数据进行分布约束，使不同领域的数据在特征空间趋向独立同分布。[^多领域学习]


## WWM-Whole word mask
这篇文章讲了wwm怎么实现的：
https://zhuanlan.zhihu.com/p/268515387?utm_source=qq
这是哈工大wwm模型github下面的一个issue，网友提了一个问题：
> 在你们的工作中，比如mask词的时候，一个词为哈利波特，那么在你们的方法中，是不是只要这个词被mask，那一定是[mask][mask][mask][mask]的形式，还是偶尔会出现[mask]利[mask][mask]的形式，不知道你们是如何设置的（不考虑那个mask80%10%10%的那个随机概率），如果是前者，那么这种完全避免局部共现的设置会不会对结果有影响。
https://github.com/ymcui/Chinese-BERT-wwm/issues/4


## Reference：
[^3种attention]:https://baijiahao.baidu.com/s?id=1724007436725564706&wfr=spider&for=pc
[^残差网络1]:https://zhuanlan.zhihu.com/p/469567586
[^残差网络2]:https://zhuanlan.zhihu.com/p/449792026
[^归一化层]:https://www.jianshu.com/p/3e170cf27b51
[^mask的变种]:https://zhuanlan.zhihu.com/p/360982134
[^Nezha相对位置编码]:https://zhuanlan.zhihu.com/p/341397170
[^多任务学习]:https://zhuanlan.zhihu.com/p/348873723
[^多领域学习]:https://zhuanlan.zhihu.com/p/477942727
[^SKEP]:https://zhuanlan.zhihu.com/p/267837817