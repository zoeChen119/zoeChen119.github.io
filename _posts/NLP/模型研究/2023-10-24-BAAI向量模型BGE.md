# BAAI 向量模型BGE

## I 为什么要研究“向量模型”？

embedding模型的应用主要包含以下几点:

- embedding模型可以将各种数据（语言、图片等）转化为向量，并使用向量之间的距离来衡量数据的相关性。
- **在大模型时代，这种技术有助于解决大模型在回答问题时可能出现的问题，可以帮助大模型获取最新的知识。**
- OpenAI、Google、Meta等大厂也都推出了自己的语义向量模型和API服务，催生了大量的应用和工具，如LangChain、Pinecone等 [^1]

如BAAI最新的论文《**Retrieve Anything To Augment Large Language Models**》[^2]所述，LLM的关键问题①幻觉；②指令敏感；③长上下文处理能力差的问题 源自于LLM的三个 <u>边界</u>：

1. 知识边界：

   ①有限的模型参数无法存储无限的世界知识。

   ②参数存储的知识是静态的。

   ③LLM更新知识是通过使用高频公用数据进行训练，因此不能适应特定领域的、长尾知识。

2. 内存边界：略

3. 能力边界：

   ①局限在语言空间，不能在物理世界进行有效的互动

   ②依赖于人类指令指导，缺乏自主性

**∴ 向量模型在LLM时代的作用——连接LLM内部知识和外部世界知识的桥梁，以检索增强器的形态为LLM更新知识注入知识。**

参考：

[^1]:https://zhuanlan.zhihu.com/p/658112595
[^2]:https://readpaper.com/pdf-annotate/note?pdfId=4810140006232358913&noteId=2018329187523227136

## II BGE和传统的embedding模型有什么区别？

BGE立足于 **检索任务**，根据《**Retrieve Anything To Augment Large Language Models**》[^2]所述，

检索任务：捕捉不同的语义关系；

面对的困难：往往受到相互干扰。

LLM Embedder是核心是系统优化**”训练方法“**来改善向量模型的检索能力，具体包括四个点：

①基于llm反馈的奖励制定

②知识蒸馏的稳定

③使用**显式指令的多任务微调**

④使用同质的批内**负采样**

LLM Embedder是BGE的更新版本，显然，和BGE的技术落点一脉相承（BGE的技术落点见`III BGE是怎么做的？`）。



## III BGE和M3E的异同

见`2023-11-02-详解RetroMAE算法`

## V BGE 的预训练和微调

已知，BGE的预训练阶段使用的是**RetroMAE算法**，微调阶段使用的是**对比学习**。另外，在FlagEmbedding库中除了BGE模型还提供了一个BGE Reranker模块（交叉编码器将对查询和答案实时计算相关性分数，这比向量模型(即双编码器)更准确，但比向量模型更耗时。数据格式与向量模型相同，因此您可以根据我们的[示例](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker) 轻松地对其进行微调。）

[微调的教程](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune)：

1. 基于对比学习的微调，训练数据的格式如下

```json
{"query": str, "pos": List[str], "neg":List[str]}
```

![image-20231107114610684](E:\Zoe\zoeChen119.github.io\assets\img\2023-10-24-BAAI向量模型BGE\image-20231107114610684.png)

提供了一个生成难负例的代码：

```
python -m FlagEmbedding.baai_general_embedding.finetune.hn_mine \
--model_name_or_path BAAI/bge-base-en-v1.5 \
--input_file toy_finetune_data.jsonl \
--output_file toy_finetune_data_minedHN.jsonl \
--range_for_sampling 2-200 \
--use_gpu_for_searching
```

2. 微调训练

```
torchrun --nproc_per_node {number of gpus} \
-m FlagEmbedding.baai_general_embedding.finetune.run \
--output_dir {path to save model} \
--model_name_or_path BAAI/bge-large-zh-v1.5 \
--train_data ./toy_finetune_data.jsonl \
--learning_rate 1e-5 \
--fp16 \
--num_train_epochs 5 \
--per_device_train_batch_size {large batch size; set 1 for toy data} \
--dataloader_drop_last True \
--normlized True \
--temperature 0.02 \
--query_max_len 64 \
--passage_max_len 256 \
--train_group_size 2 \
--negatives_cross_device \
--logging_steps 10 \
--query_instruction_for_retrieval "为这个句子生成表示以用于检索相关文章：" 
```

3. 使用微调后的模型

和直接调用原版BGE一样的调用。唯一一点需要注意的是：如果在微调训练时设置了`query_instruction_for_retrieval`这个参数，那调用的时候也要设置。

[预训练的教程](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/pretrain) ：

1. 基于RetroMAE的预训练，数据格式与传统预训练一致，如下

```{"text": str}```

![image-20231107115851872](E:\Zoe\zoeChen119.github.io\assets\img\2023-10-24-BAAI向量模型BGE\image-20231107115851872.png)

2. 预训练

```bash
torchrun --nproc_per_node {number of gpus} \
-m FlagEmbedding.baai_general_embedding.retromae_pretrain.run \
--output_dir {path to save model} \
--model_name_or_path BAAI/bge-large-en \
--train_data toy_pretrain_data.jsonl \
--learning_rate 2e-5 \
--num_train_epochs 2 \
--per_device_train_batch_size {batch size; set 1 for toy data} \
--dataloader_drop_last True \
--max_seq_length 512 \
--logging_steps 10 \
--dataloader_num_workers 12
```

预训练后的模型保存在`{output_dir}/encoder_model`。



参考：

[^3]:为什么bert这么难理解？ - TRiddle的回答 - 知乎 https://www.zhihu.com/question/510738704/answer/2671000185
[^4]:https://readpaper.com/pdf-annotate/note?pdfId=4683230553864929281&noteId=2018415919421572352