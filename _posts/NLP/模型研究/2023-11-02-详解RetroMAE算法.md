# 详解RetroMAE算法

## I BGE和M3E的异同

区别1：基础模型不同

BGE：RetroMAE

M3E：RoBERTa-small & RoBERTa-large



区别2：预训练算法不同

BGE：RetroMAE的预训练算法

M3E：MLM



区别3：微调方法不同

BGE：2种对比学习策略（in-batch 负采样、cross-device 共享负样本）+ instruct tuning

M3E：1种对比学习策略（in-batch 负采样）+ instruct tuning



相同点：训练规模大致应该差不多，没有找到完整的资料，网友普遍猜测。

![image-20231102162114760](..\..\..\..\zoeChen119.github.io\assets\img\2023-11-02-详解RetroMAE算法\image-20231102162114760.png)

![image-20231102162155649](..\..\..\..\zoeChen119.github.io\assets\img\2023-11-02-详解RetroMAE算法\image-20231102162155649.png)

共使用了29个数据集。





## II RetroMAE 与 Transformer对比

<img src="..\..\..\..\zoeChen119.github.io\assets\img\2023-11-02-详解RetroMAE算法\transformer_.png" alt="transformer_frame" style="zoom:50%;" />

区别1：输入输出不同。

Transformer：输入=句子1、输出=句子2

RetroMAE：输入=句子1、输出=句子1



区别2：交叉注意力（cross attention）公式中的Q、K、V不同。

Transformer：$Q=(y+pe)W^Q$   $K=cW^K$    $V=cW^V$

RetroMAE：$Q=H_1W^Q=(c+pe)W^Q$         $K=H_2W^K=(c,x+pe)W^Q$          $V=H_2W^V=(c,x+pe)W^Q$



区别3：损失函数不同。

Transformer：基于对数似然的交叉熵

RetroMAE：条件交叉熵



区别4：Decoder部分掩码策略不同。

Transformer：mask后面的全部

RetroMAE：位置掩码，在Decoder的输入序列中按照一定的概率，从左到右依次掩盖每个词，然后让模型预测被掩盖的词。![image-20231102160321393](..\..\..\..\zoeChen119.github.io\assets\img\2023-11-02-详解RetroMAE算法\image-20231102160321393.png)

##  III 研究1：Encoder15%~30%掩码 & Decoder 50%~70%掩码

如图(A)和图(B)。对比传统的Transformer-Decoder思考，区别在图(B)中mask的比例，mask的比例更大给Encoder加高了理解的难度，迫使它提炼出更高质量的句子嵌入，从而使原始输入能够在恢复的时候保持良好的保真度。

![retromae_frame](..\..\..\..\zoeChen119.github.io\assets\img\2023-11-02-详解RetroMAE算法\image-20231024114336162.png)

## V 研究2：Enhanced Decoder

在研究1的基础上进行了进一步的大胆创新，重构Decoder的输入，研究1只是修改了mask的比例，研究2修改了Attention机制，创新了一个新的Attention公式 $A$，如下：

<img src="..\..\..\..\zoeChen119.github.io\assets\img\2023-10-24-BAAI向量模型BGE.assets\image-20231024150747347.png" alt="image-20231024150747347" style="zoom:67%;" />

<img src="..\..\..\..\zoeChen119.github.io\assets\img\2023-10-24-BAAI向量模型BGE.assets\image-20231024151245016.png" alt="image-20231024151245016" style="zoom:67%;" />

其实Attention公式 $A$ 的形式依然是传统的Transformer中的Self-Attention的形式，研究2的创新点在修改了其中的 $Q$、$K$、$V$ 的取值，传统的Transformer的取值可以简单理解为TE+SE+PE（暂不提dropout、归一化层等），即input-embedding层的初始编码结果。

损失函数-条件交叉熵

![image-20231102110643520](..\..\..\..\zoeChen119.github.io\assets\img\2023-11-02-详解RetroMAE算法\image-20231102110643520.png)





## VI 拓展

RetroMAE并没有利用到知识库的信息，它主要是为目前的检索器模型提供一个更适合检索任务的句子嵌入而已。

而另一个模型——**RETRO Transformer**在预训练过程中的确利用到了知识库的信息，它是在解码器中增加了RetroDecoder模块通过分块交叉注意力CCA来从两个临近块中检索信息。 DeepMind 的 RETRO Transformer模型的性能与 GPT-3 相当，尽管其大小只有 ~~4%~~（75 亿个参数，而 ~~GPT-3 Da Vinci 的参数为 1850 亿个~~）。

并且该模型与Google的WebGPT的论文都论证了“如果我们通过搜索/查询信息的方式来增强较小的生成语言模型，其性能可以与大规模模型相媲美。”

<img src="https://jalammar.github.io/images/retro/Large-GPT-vs-Retro-transformer-world-knowledge-information.png" alt="img" style="zoom: 33%;" />

