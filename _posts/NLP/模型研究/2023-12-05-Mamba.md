---
title: Mamba
categories: [NLP,模型研究]
tags: [nlp,Mamba,模型研究]     # TAG names should always be lowercase
---

# Mamba：Transformer的挑战者

Mamba: Linear-Time Sequence Modeling with Selective State Spaces

Mamba：具有选择性状态空间的线性时间序列建模



## 摘要：

目前深度学习的应用都是基于 **Transformer架构（及其注意力模块）**。Transformer的一个致命问题是不善于长序列计算。有许多subquadratic-time architectures（二次时间架构），例如：线性注意力、门控卷积、循环模型、结构化状态空间模型（SSM）。这些模型在语言等重要模态上的表现都不如注意力机制。我们发现这类模型的一个关键弱点是它们无法执行基于内容的推理，并针对这个弱点进行了改进。

具体，首先简单地让SSM参数是func(input)，就可以用离散模态来解决它们地弱点，允许模型根据当前token沿着序列长度维度选择性地传播/忘记信息。其次，尽管这种变化阻止了高效卷积的使用，但我们在递归模式下设计了一种硬件感知的并行算法。我们将这些选择性SSM集成到一个简化的端到端神经网络架构中，而无需注意力，甚至无需MLP块（Mamba）。

（以下为表现）

Mamba具有快速推理（比Transformers高5倍的吞吐量）和序列长度的线性缩放，其性能在高达百万长度的真实数据序列上得到了提高。

作为通用序列模型的主干，Mamba在语言、音频和基因组学等多种模式中实现了最先进的性能。

在语言建模方面，我们的Mamba-3B模型在预训练和下游评估方面都优于相同大小的Transformers，并与两倍于其大小的Transformer相匹配。