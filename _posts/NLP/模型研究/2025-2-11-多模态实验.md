# 多模态实验过程记录



## 实验1：colpaligemma-3b-pt-448-base测试

![image-20250211185813569](./../../../assets/img/2025-2-11-多模态实验/image-20250211185813569.png)

```json
PaliGemmaForConditionalGeneration(
  (vision_tower): SiglipVisionModel(
    (vision_model): SiglipVisionTransformer(
      (embeddings): SiglipVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)
        (position_embedding): Embedding(1024, 1152)
      )
      (encoder): SiglipEncoder(
        (layers): ModuleList(
          (0-26): 27 x SiglipEncoderLayer(
            (self_attn): SiglipSdpaAttention(
              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)
              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)
              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)
              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)
            )
            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
            (mlp): SiglipMLP(
              (activation_fn): PytorchGELUTanh()
              (fc1): Linear(in_features=1152, out_features=4304, bias=True)
              (fc2): Linear(in_features=4304, out_features=1152, bias=True)
            )
            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
    )
  )
  (multi_modal_projector): PaliGemmaMultiModalProjector(
    (linear): Linear(in_features=1152, out_features=2048, bias=True)
  )
  (language_model): GemmaForCausalLM(
    (model): GemmaModel(
      (embed_tokens): Embedding(257216, 2048, padding_idx=0)
      (layers): ModuleList(
        (0-17): 18 x GemmaDecoderLayer(
          (self_attn): GemmaAttention(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (k_proj): Linear(in_features=2048, out_features=256, bias=False)
            (v_proj): Linear(in_features=2048, out_features=256, bias=False)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          )
          (mlp): GemmaMLP(
            (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)
            (up_proj): Linear(in_features=2048, out_features=16384, bias=False)
            (down_proj): Linear(in_features=16384, out_features=2048, bias=False)
            (act_fn): PytorchGELUTanh()
          )
          (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)
          (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)
        )
      )
      (norm): GemmaRMSNorm((2048,), eps=1e-06)
      (rotary_emb): GemmaRotaryEmbedding()
    )
    (lm_head): Linear(in_features=2048, out_features=257216, bias=False)
  )
)
```



**图中左侧的Vision LLM**：PaliGemma-3B 是一种视觉-语言模型（VLM），该模型使用的ViTs模型是SigLIP-So400m/14，它可以生成高质量的图像嵌入。

**1》**processor = **AutoProcessor**.from_pretrained(model_name)

inputs = processor(images=image, text=prompt,  return_tensors="pt")

> inputs['input_ids'].shape = torch.Size([1, 1034])
>
> inputs['attention_mask'].shape = torch.Size([1, 1034])
>
> inputs['pixel_values'].shape = torch.Size([1, 3, 448, 448])

**2》**batch_images = **ColPaliProcessor**.process_images(RGB图像列表)

> batch_images = processor.process_images(images).to(model.device)
>
> batch_queries = processor.process_queries(queries).to(model.device)
>
> 
>
> batch_images ['input_ids'].shape = torch.Size([2, 1030])
>
> batch_images ['attention_mask'].shape = torch.Size([1, 1030])
>
> batch_images ['pixel_values'].shape = torch.Size([2, 3, 448, 448])
>
> 
>
> batch_queries['input_ids'].shape = torch.Size([2, 27])
>
> batch_queries['attention_mask'].shape = torch.Size([2, 27])



image_embeddings = model(**batch_images) # with torch.no_grad():

![image-20250211191729015](./../../../assets/img/2025-2-11-多模态实验/image-20250211191729015.png)

proj就是一层用于降维的线性层。







**图中右侧的LLM：**该模型使用的LLMs模型是Gemma-2B。一个特别有趣的特性是，PaliGemma-3B 的文本模型是通过带有前缀（指令文本和图像标记）的全块注意力机制进行微调的。

batch_queries = **ColPaliProcessor**.process_queries(文本句子列表)

query_embeddings = model(**batch_queries) # with torch.no_grad():





**应用：**

scores = **ColPaliProcessor**.score_multi_vector(query_embeddings, image_embeddings)