# 多模态实验过程记录



## 实验1：colpaligemma-3b-pt-448-base测试

![image-20250211185813569](./../../../assets/img/2025-2-11-多模态实验/image-20250211185813569.png)

```json
PaliGemmaForConditionalGeneration(
  (vision_tower): SiglipVisionModel(
    (vision_model): SiglipVisionTransformer(
      (embeddings): SiglipVisionEmbeddings(
        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)
        (position_embedding): Embedding(1024, 1152)
      )
      (encoder): SiglipEncoder(
        (layers): ModuleList(
          (0-26): 27 x SiglipEncoderLayer(
            (self_attn): SiglipSdpaAttention(
              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)
              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)
              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)
              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)
            )
            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
            (mlp): SiglipMLP(
              (activation_fn): PytorchGELUTanh()
              (fc1): Linear(in_features=1152, out_features=4304, bias=True)
              (fc2): Linear(in_features=4304, out_features=1152, bias=True)
            )
            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
          )
        )
      )
      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)
    )
  )
  (multi_modal_projector): PaliGemmaMultiModalProjector(
    (linear): Linear(in_features=1152, out_features=2048, bias=True)
  )
  (language_model): GemmaForCausalLM(
    (model): GemmaModel(
      (embed_tokens): Embedding(257216, 2048, padding_idx=0)
      (layers): ModuleList(
        (0-17): 18 x GemmaDecoderLayer(
          (self_attn): GemmaAttention(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (k_proj): Linear(in_features=2048, out_features=256, bias=False)
            (v_proj): Linear(in_features=2048, out_features=256, bias=False)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
          )
          (mlp): GemmaMLP(
            (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)
            (up_proj): Linear(in_features=2048, out_features=16384, bias=False)
            (down_proj): Linear(in_features=16384, out_features=2048, bias=False)
            (act_fn): PytorchGELUTanh()
          )
          (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)
          (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)
        )
      )
      (norm): GemmaRMSNorm((2048,), eps=1e-06)
      (rotary_emb): GemmaRotaryEmbedding()
    )
    (lm_head): Linear(in_features=2048, out_features=257216, bias=False)
  )
)
```



**图中左侧的Vision LLM**：PaliGemma-3B 是一种视觉-语言模型（VLM），该模型使用的ViTs模型是SigLIP-So400m/14，它可以生成高质量的图像嵌入。

**1》**processor = **AutoProcessor**.from_pretrained(model_name)

inputs = processor(images=image, text=prompt,  return_tensors="pt")

> inputs['input_ids'].shape = torch.Size([1, 1034])
>
> inputs['attention_mask'].shape = torch.Size([1, 1034])
>
> inputs['pixel_values'].shape = torch.Size([1, 3, 448, 448])

**2》**batch_images = **ColPaliProcessor**.process_images(RGB图像列表)

> batch_images = processor.process_images(images).to(model.device)
>
> batch_queries = processor.process_queries(queries).to(model.device)
>
> 
>
> batch_images ['input_ids'].shape = torch.Size([2, 1030])
>
> batch_images ['attention_mask'].shape = torch.Size([1, 1030])
>
> batch_images ['pixel_values'].shape = torch.Size([2, 3, 448, 448])
>
> 
>
> batch_queries['input_ids'].shape = torch.Size([2, 27])
>
> batch_queries['attention_mask'].shape = torch.Size([2, 27])



image_embeddings = model(**batch_images) # with torch.no_grad():

![image-20250211191729015](./../../../assets/img/2025-2-11-多模态实验/image-20250211191729015.png)

proj就是一层用于降维的线性层。







**图中右侧的LLM：**该模型使用的LLMs模型是Gemma-2B。一个特别有趣的特性是，PaliGemma-3B 的文本模型是通过带有前缀（指令文本和图像标记）的全块注意力机制进行微调的。

batch_queries = **ColPaliProcessor**.process_queries(文本句子列表)

query_embeddings = model(**batch_queries) # with torch.no_grad():





**应用：**

scores = **ColPaliProcessor**.score_multi_vector(query_embeddings, image_embeddings)



## Base模型选择

### （1）

**Metric-AI/ColQwen2.5-3b-multilingual-v1.0**

1. Vidore榜单排名第2

   ![image-20250310090129656](../../../assets/img/2025-2-11-多模态实验/image-20250310090129656.png)

2. 是lora微调后的模型

> 是否能把lora微调后的和没有lora微调的模型放在一起对比？

3. 这是一个多语言模型

**Alibaba-NLP/gme-Qwen2-VL-2B-Instruct**

1. Vidore榜单排名第13

![image-20250310090934976](../../../assets/img/2025-2-11-多模态实验/image-20250310090934976.png)

2. 原模型

> 排名靠前的都是lora微调后的模型，就这一个阿里的不是





### （2）论文1：多模态综述

2025年2月的

![image-20250310092332565](../../../assets/img/2025-2-11-多模态实验/image-20250310092332565.png)

#### 4.4 Generation Techniques

![image-20250310103855124](../../../assets/img/2025-2-11-多模态实验/image-20250310103855124.png)

#### 6.3 Agent-Based and Self-Guided Systems

将强化学习和端到端与人一致的反馈结合到多模态rag中，在很大程度上仍未被探索，但在增强这些系统方面具有巨大的潜力。

### （3）论文2：SK-VQA

SK-VQA: Synthetic Knowledge Generation at Scale for Training Context-Augmented Multimodal LLMs

情景增强的MLLM，具体创新是一种能够用于大规模收集自然和多样化数据。

没有依靠模板的方法（template-based methods）为真实数据构建QA对，就是利用GPT4来为给定图像生成相关上下文文档和多个问答对，以此创建了一个SK-VQA数据集（迄今为止最大的KBVQA数据集，包含超过200万个问答对）。

流程：

1. 数据集生成
2. 图像参考(ImRef)过滤
3. 上下文答案存在 (CAP) 过滤

### （4）论文3：R1-Searcher

R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning

核心思路：基于结果的两阶段强化学习方法，旨在增强LLMs的搜索能力。

**开源链接**：

- 代码仓库：https://github.com/SsmallSong/R1-Searcher

- 模型：

- - Qwen-2.5-7B-Base-RAG-RL: https://huggingface.co/XXsongLALA/Qwen-2.5-7B-base-RAG-RL
  - Llama-3.1-8B-Instruct-RAG-RL: https://huggingface.co/XXsongLALA/Llama-3.1-8B-instruct-RAG-RL

- 训练数据：https://huggingface.co/datasets/XXsongLALA/RAG-RL-Hotpotqa-with-2wiki

我们使用两阶段结果监督强化学习，整体基于Reinforce++算法。在第一阶段，模型被训练以有效利用外部检索系统，在第二阶段，模型被训练在推理过程中整合检索，以准确解答问题。我们通过奖励设计实现两阶段训练：

- 第一阶段，reward由retrieval-reward和format-reward组成，如果模型在推理过程中进行了检索，就会得到retrieval-reward，旨在让模型学会调用工具的格式；
- 第二阶段，retrieval-reward被替换为answer-reward，让模型更自由地进行探索，answer-reward是标准答案和预测答案的F1-Score，旨在让模型学会正确调用工具解决问题。

另外，我们对Reinforce++算法进行了修改以适应检索增强生成场景。我们的目标是让模型在面对不确定性时能够自主获取外部知识，从而有效整合推理和检索。为了无缝整合检索到的文档并确保模型优化的合理性，我们对原始算法进行了两项改进：RAG-based Rollout和Retrieval Mask-based Loss Calculation：

- RAG-based Rollout： 我们使用标签<begin_of_query>...<end_of_query>来引导模型在生成过程中调用外部检索系统。捕捉到模型需要进行检索时，推理暂停并进行检索。检索到的文档被封装在<begin_of_documents>...<end_of_documents>标签中，并整合到模型的推理过程中。这种方法确保检索无缝融入推理过程，使模型能够基于检索到的文档继续推理，而不被打断。
- Retrieval Mask-based Loss Calculation：当模型执行检索时，检索到的文档作为环境观察的一部分被整合到推理过程中。然而，模型并不需要自主生成这些文档。为了减少环境的影响，我们将<begin_of_documents>...<end_of_documents>指定为特殊标记，并在训练中对其进行掩码处理。这可以防止这些外部标记影响损失计算，确保检索到的文档不会干扰模型的内在推理和生成过程。

## **实验结果**

🚀 如下表所示，我们的方法R1-Searcher：

- 在多跳问答任务上实现显著的性能提升：相比于最好的基线ReARTeR，R1-Searcher，使用相同的LLaMA-3.1-8B-Instruct作为backbone，实现了显著的性能提升：在HotpotQA上提升了**48.2%**，在2WikiMultiHopQA上提升了**21.7%**，在Bamboogle上提升了**4.0%**（LLM-as-Judge）。这表明我们的方法可以有效地促进模型在推理过程中进行准确的检索调用。
- 从基础LLM开始进行RL学习，无需冷启动：我们从头开始使用强大的基础模型（如Qwen-2.5-7B-Base）进行RL学习。令人惊讶的是，我们能够取得更好的结果，并在大多数领域内和领域外的数据集上获得最佳性能，甚至超过了闭源的LLM，如GPT-4o-mini。这些结果展示了我们的两阶段RL方法在指导LLMs学习过程中的有效性。
- 保持泛化能力：我们仅使用HotpotQA和2WikiMultiHopQA训练集中的8148个样本进行RL训练。该模型不仅在这些领域内数据集上表现出色，还在领域外数据集（如Musique和Bamboogle）上展示了强大的泛化能力。这表明模型通过在RL训练期间的探索，有效地学习了检索并将其与推理相结合，从而在需要检索的新测试数据集上实现稳健的性能。![img](https://mmbiz.qpic.cn/mmbiz_png/G7ia3FZ0o0OowFv80G6bBpr2WPtLjaNKkcYNVbCE3j3ZkAKJcVusiardo8yhmQT010rb6sZKNRvI6qPghTE4DwLA/640?wx_fmt=png&from=appmsg)

另外，为了评估模型对于联网搜索泛化能力，我们在最新提出的Bamboogle任务上进行联网搜索的测试，这种设定在RL训练期间并未遇到。如下图所示，我们的模型相较于使用相同Qwen-2.5-7B-Base作为backbone的本地检索系统，性能提升了18.2%。此外，与使用相同在线搜索但骨干模型更大的32B的Search-o1相比，我们的模型性能提升了11.4%。这表明我们的模型能够适应在线搜索场景，并且R1-Searcher使模型能够在推理过程中检索信息，而不仅仅是记忆响应格式。

![img](https://mmbiz.qpic.cn/mmbiz_jpg/G7ia3FZ0o0OowFv80G6bBpr2WPtLjaNKkA6Q7oIlofAibeVYdicyOibk4acPqywHcyJ93y6mMZFqAMB2rsX2LoHJDg/640?wx_fmt=jpeg&from=appmsg)

我们针对以下问题进行了更详细的实验和分析，完整的分析请看原论文：

1. GRPO和Reinforce++算法的比较

- 结论：GRPO的生成solution更长和检索频率更高。GRPO在领域外测试数据集（如Bamboogle）上也展现出更好的性能；而Reinforce++在领域内测试集（如HotpotQA和2Wiki）上表现更优。

1. RL和SFT的比较

- 结论：RL在领域内和领域外的测试集上均优于SFT。SFT能够帮助模型生成检索查询，但这些查询的时机和相关性不如通过RL训练生成的查询。

1. Reward的设计对训练的影响

- 结论：基于F1的答案奖励能够产生更长的回答长度和更优的最终结果；基于EM的奖励在训练过程中导致回答长度较短，并且在测试时表现不如基于CEM或F1的奖励；基于CEM的奖励会生成带有不必要信息的偏长的answer。

1. 数据难度分布和数据多样性对训练的影响

- 结论：使用混合数据集训练的模型在检索次数和生成回答长度上都有所增加，并且在测试集上取得了更高的分数；训练数据中混入较高难度的数据可以在领域内和领域外的测试集上均取得更好的效果。

![img](https://mmbiz.qpic.cn/mmbiz_jpg/G7ia3FZ0o0OowFv80G6bBpr2WPtLjaNKkH0sP0Cicia9oZHjMAYCxIwt87NicLSbO3kichfhJObLJdyV6dB62XMR42g/640?wx_fmt=jpeg&from=appmsg)

![img](https://mmbiz.qpic.cn/mmbiz_jpg/G7ia3FZ0o0OowFv80G6bBpr2WPtLjaNKkn9uNKHFGpOqjL8iaNg3iaXQocGe68FoaToORaOYHwiaFPOtViaeaxq7lAQ/640?wx_fmt=jpeg&from=appmsg)

## **案例展示**

![img](https://mmbiz.qpic.cn/mmbiz_png/G7ia3FZ0o0OowFv80G6bBpr2WPtLjaNKkMYBrFibS2MMichA8qQfg4icYmnqBRtSXQjiaaPawW6RF3UVUhIwyuuKtYw/640?wx_fmt=png&from=appmsg)



## Idea

由于计算资源有限，就在别人训练好的模型（比如Qwen2-VL）的基础上用DQN/PPO微调，强化学习用的数据集就是DocVQA这种常见的多模态测试数据集，DocVQA数据集中的标准答案就当作是反馈（如果MLLM生成的responce和这个答案一致就点赞，否则就纠正），query就是查询，image就是图像。

### 可行性分析

1. **基础模型选择**：选择一个性能优越的预训练模型作为起点，可以大大减少所需的计算资源，并缩短开发周期。Qwen2-VL这样的多模态模型能够很好地理解文本和图像信息，为后续的任务提供了坚实的基础。
2. **数据集选择**：DocVQA是一个非常适合用于此目的的数据集，它包含了大量文档图像及其对应的问题与答案，有助于训练模型准确理解和回答基于图像的问题。
3. **反馈机制设计**：将DocVQA中的标准答案用作反馈信号，通过比较生成的回答与标准答案的一致性来提供奖励（点赞或纠正），这种方法直观有效。这不仅简化了奖励函数的设计，还确保了反馈的质量。

### 实施建议

1. **环境设置**：
   - **状态空间**：定义状态空间为当前查询、图像以及模型生成的回答。
   - **动作空间**：动作空间可以是模型参数的调整方向和幅度，或者是直接对回答进行修正的选择。
2. **强化学习算法选择**：
   - **PPO**可能更适合这种场景，因为它在处理高维状态和动作空间时表现良好，并且能更稳定地更新策略，避免了DQN中可能出现的过估计问题。
3. **奖励设计**：
   - 除了简单的“点赞/纠正”之外，还可以考虑引入部分奖励机制，即即使回答不完全正确，但如果包含了正确的关键信息，也可以给予一定的正向激励。
   - 对于纠正的情况，可以尝试给出具体的修改建议，而不是仅仅标记为错误，这样可以帮助模型更快地学习到正确的模式。
4. **实验与评估**：
   - 在开始全面训练之前，先在一个较小规模的数据子集上进行初步实验，以验证整个框架的有效性。
   - 使用交叉验证等技术评估模型性能，确保改进措施确实提升了模型的整体表现。
5. **持续优化**：
   - 根据实验结果不断调整强化学习策略和参数设置，逐步逼近最优解。
   - 考虑引入用户实际交互数据，进一步丰富训练样本，提升模型的实际应用效果。