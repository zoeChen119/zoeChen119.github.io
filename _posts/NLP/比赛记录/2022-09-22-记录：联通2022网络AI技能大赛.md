---
title: 记录：联通2022网络AI技能大赛
date: 2022-09-20 14:10:56 +/-0800
categories: [NLP,比赛记录]
tags: [nlp,文本分类,情感分类]     # TAG names should always be lowercase
---
# 中国联通2022网络AI技能大赛复赛赛题（NLP赛道）

* ## 简介

任务：用户评论情感分类

赛题意义：从①各大网站的用户评论区中；②用户和客服的聊天记录中，挖掘用户的情绪。

数据集：本赛题需要参赛者解决的是文本情感分类问题。由于运营商的工单含有运营商用户敏感信息，基于公司用户隐私相关规范，本次比赛暂不提供真实用户数据，所以我们选取了<u>外卖、酒店、旅游、美食评论、电影评论等多个生活中常用的领域第三方APP下的评论数据</u>作为本次赛题的数据集。

输入输出形式：
    input:一段文字
    output：1|0（其中“积极”用1表示，“消极”用0表示）

内容格式：
    每一句是一个单独的一行。
    review是用户评论数据，rating字段是输出。

文件格式：
    csv

数据分类：
1. 电影评论 movie_train.csv
   movie_train.csv的字段名称为：userId,movieId,rating,timestamp,comment,like，分别代表用户ID，电影ID，用户评分（只保留1和5分的记录，其中5分是积极，1分是消极），评分时间戳，评论内容，是否喜欢等。
   数据量：663226条
      
2. 用户购物消费评论 shopping_train.csv
   shopping_train.csv的字段名称为：cat（物品类型）、label（评分0-1）、review（评论文字）
   数据量：50219条    
        
3. 用户外卖评论 waimai_train.csv
   waimai_train.csv的字段名称为：label（评分0-1）、review（评论文字）
   数据量：9589条

注：由于训练集中的数据形式不一样，**需要用户自行进行统一处理后送入模型训练**。

数据集中可能存在缺失、重复、脏数据、换行等少部分干扰数据，需要参赛者自己**构建策略予以识别或者滤除**。

评分标准：按照准确率的**F1值**从高到低进行排名。

提交结果格式：
**提交的答案形式也是csv形式，以标题栏为label，然后用1和0表示积极和消极即可。** 例如在如下例子中：

|label |
|-------|
| 0 |
| 1 |
| 0 |
| 1 |
| 0 |
| 1 |

代表第1、3、5句识别结果是消极情绪，第2、4、6句识别结果是积极情绪。

:trollface:
<font color=DarkCyan>
使用到的优化方法：
1. Word2Vec+TFIDF提取关键词，以便于数据增强时替换词语不替换掉关键词
2. 数据增强：基于同义词词林替换词语
3. 调整交叉熵分类的阈值
4. 使用SKEP模型微调
   
</font>

-----

* ## 数据预处理&可视化分析
### 1. 数据可视化
训练集1 movie_train.csv 概况

```python
import pandas as pd

movie_df = pd.read_csv("../train/movie_train.csv",sep=",",header=0,index_col=None)
print(len(movie_df))

key = "comment"
key_len = key+"_len"
movie_df[key_len]=movie_df[key].apply(len)
# 查看comment文本情况
print("数据长度概览：")
print(movie_df[key_len].describe(percentiles=[0.1,0.25,0.75,0.8,0.9,0.95,0.999])) # percentiles指定排前%的取值，默认值是25 45 75
print("中位数是：")
print(movie_df[key_len].median())
print("数据各个长度分别有多少条：")
print(movie_df[key_len].value_counts()) 

# 查看rating的分布
print("不同评分分别有多少条：")
print(movie_df["rating"].value_counts())
```

![](/assets/img/记录：联通2022网络AI技能大赛/2022-09-20-15-22-41.png)

> movie_train.csv
> 总行数：663226
> comment列平均长度：37
> comment列最短长度：1
> comment列最长长度：200
> comment列95%长度小于等于128
> comment长度=5的最多：20922条

> rating列分布：
> =5 476130条 71.79%
> =1 187096条 28.21%


测试集 test_set_data.csv 概况

```python
'''
查看test_set_data.csv文件
'''
movie_df = pd.read_csv("../test/test_set_data.csv",header=0,index_col=None)
print(len(movie_df))

key = "review"
key_len = key+"_len"
movie_df[key_len]=movie_df[key].apply(str).apply(len)
# 查看review文本情况
print("数据长度概览：")
print(movie_df[key_len].describe(percentiles=[0.1,0.25,0.75,0.8,0.9,0.95,0.999])) # percentiles指定排前%的取值，默认值是25 45 75
print("中位数是：")
print(movie_df[key_len].median())
print("数据各个长度分别有多少条：")
print(movie_df[key_len].value_counts()) 
```
![](/assets/img/记录：联通2022网络AI技能大赛/2022-09-20-15-35-32.png)

> test_set_data.csv
> 总行数：22833
> comment列平均长度：45
> comment列最短长度：1
> comment列最长长度：733
> comment列95%长度小于等于143
> comment长度=15的最多：599条

```python
def scan_all():
    '''
    查看raw_data.csv文件
    '''
    movie_df = pd.read_csv("../code/raw_data.csv",sep=",",header=0,index_col=None)
    print(len(movie_df))

    key = "text"
    key_len = key+"_len"
    movie_df[key_len]=movie_df[key].apply(len)
    # 查看comment文本情况
    print("数据长度概览：")
    print(movie_df[key_len].describe(percentiles=[0.1,0.25,0.75,0.8,0.9,0.95,0.999])) # percentiles指定排前%的取值，默认值是25 45 75
    print("中位数是：")
    print(movie_df[key_len].median())
    print("数据各个长度分别有多少条：")
    print(movie_df[key_len].value_counts()) 

    # 查看rating的分布
    print("不同评分分别有多少条：")
    print(movie_df["label"].value_counts())
```

> 总行数：723034
> text列平均长度：38
> text列最短长度：1
> text列最长长度：2876
> text列95%长度小于等于130
> text长度=5的最多：21371条

> label列分布：
> =disnegative 504703条 69.80%
> =negative 218331条 30.20%

![](/assets/img/记录：联通2022网络AI技能大赛/2022-09-20-19-10-32.png)

### 2. 数据预处理
将训练数据集合并，把label变成int

### 3. 数据清理
训练和验证数据是队友直接合并的。
测试数据有一行是Nan
```python
# 返回test_df中含有空值的行
new_df = test_df[test_df.isnull().any(axis = 1)==True]
# 返回test_df中不含有空值的行
new_df = test_df[test_df.isnull().any(axis = 1)==False]

# 返回test_df中含有空值的列
new_df = test_df[test_df.isnull().any(axis = 0)==True]
# 返回test_df中不含有空值的列
new_df = test_df[test_df.isnull().any(axis = 0)==False]
```
![](/assets/img/记录：联通2022网络AI技能大赛/2022-09-21-16-21-17.png)
删除这一行，并生成新文件替换掉含有空值的旧文件
```python
def clean_test():
    test_df = pd.read_csv("../test/test_set_data.csv",header=0,index_col=None)
    print(len(test_df))
    new_df = test_df[test_df.isnull().any(axis = 1)==False]
    new_df.to_csv("../test/test_set_data1.csv",index=False)
    return new_df

clean_test()
```
![](/assets/img/记录：联通2022网络AI技能大赛/2022-09-21-16-25-16.png)

---
* ## Method1：Bert-Base-Chinese作为预训练模型
队友做的93% F1
* ## Method2：SKEP作为预训练模型
* <font color="LightSeaGreen">用Accuracy作为train过程中的evaluate函数的metric，测试的时候再用F1score</font>
### 2.1 batch_size=16 Acc=63.85%
其他超参数：
    max_seq_length = 128
    batch_size = 16
    learning_rate = 4e-5
    epochs = 3
    warmup_proportion = 0.1
    weight_decay = 0.01
开始训练：2022-9-20 21:03
结束训练：2022-9-21 15:18 手动停止
进度：
> global step 12000, epoch: 1, batch: 12000, loss: 0.78122, accu: 0.66812, speed: 1.55 step/s

一个epoch没训练完，实在太慢了
![](/assets/img/记录：联通2022网络AI技能大赛/2022-09-21-15-26-12.png)
![](/assets/img/记录：联通2022网络AI技能大赛/2022-09-21-15-26-47.png)

测试一下test_set_data.csv用当前保存的acc最高的一套模型参数预测,得到预测结果test_set_predict.csv，和test_set_answer.csv对比，计算Acc和F1-score。
```python
import paddle
paddle.device.set_device('gpu:1')
print(paddle.device.get_device())

import pandas as pd
from paddlenlp.datasets import load_dataset
from paddle.io import Dataset, Subset
from paddlenlp.datasets import MapDataset
 
'''
0. loading data...
'''

test = pd.read_csv('../test/test_set_data.csv')

def read_test(pd_data):
    for index, item in pd_data.iterrows():       
        yield {'text': item['review'], 'label': 0}
 
test_ds =  load_dataset(read_test, pd_data=test,lazy=False)

# 在转换为MapDataset类型
def convert_example(example,
                    tokenizer,
                    max_seq_length=512,
                    is_test=False):
   
    # 将原数据处理成model可读入的格式，enocded_inputs是一个dict，包含input_ids、token_type_ids等字段
    encoded_inputs = tokenizer(
        text=example["text"], max_seq_len=max_seq_length)
 
    # input_ids：对文本切分token后，在词汇表中对应的token id
    input_ids = encoded_inputs["input_ids"]
    # token_type_ids：当前token属于句子1还是句子2，即上述图中表达的segment ids
    token_type_ids = encoded_inputs["token_type_ids"]
 
    if not is_test:
        # label：情感极性类别
        label = np.array([example["label"]], dtype="int64")
        return input_ids, token_type_ids, label
    else:
        return input_ids, token_type_ids, -1
    
def create_dataloader(dataset,
                      trans_fn=None,
                      mode='train',
                      batch_size=1,
                      batchify_fn=None):
    
    if trans_fn:
        dataset = dataset.map(trans_fn)
 
    shuffle = True if mode == 'train' else False
    if mode == "train":
        sampler = paddle.io.DistributedBatchSampler(
            dataset=dataset, batch_size=batch_size, shuffle=shuffle)
    else:
        sampler = paddle.io.BatchSampler(
            dataset=dataset, batch_size=batch_size, shuffle=shuffle)
    dataloader = paddle.io.DataLoader(
        dataset, batch_sampler=sampler, collate_fn=batchify_fn)
    return dataloader
'''
1. loading model...
'''
from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer

model = SkepForSequenceClassification.from_pretrained(pretrained_model_name_or_path="skep_ernie_2.0_large_en", num_classes=2)
tokenizer = SkepTokenizer.from_pretrained(pretrained_model_name_or_path="skep_ernie_2.0_large_en")

'''
2. define superparameters...
'''
from functools import partial
import numpy as np
import paddle.nn.functional as F
from paddlenlp.data import Stack, Tuple, Pad
batch_size=16
max_seq_length=128
# 处理测试集数据
trans_func = partial(
    convert_example,
    tokenizer=tokenizer,
    max_seq_length=max_seq_length,
    is_test=True)
batchify_fn = lambda samples, fn=Tuple(
    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input
    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment
    Stack() # qid
): [data for data in fn(samples)]
test_data_loader = create_dataloader(
    test_ds,
    mode='test',
    batch_size=batch_size,
    batchify_fn=batchify_fn,
    trans_fn=trans_func)

'''
3. evalue the model...
'''

import os
 
# 根据实际运行情况，更换加载的参数路径
params_path = 'best_checkpoint/best_model.pdparams'
if params_path and os.path.isfile(params_path):
    # 加载模型参数
    state_dict = paddle.load(params_path)
    model.set_dict(state_dict)
    print("Loaded parameters from %s" % params_path)
    
results = []
# 切换model模型为评估模式，关闭dropout等随机因素
model.eval()
for batch in test_data_loader:
    input_ids, token_type_ids, qids = batch
    # 喂数据给模型
    logits = model(input_ids, token_type_ids)
    # 预测分类
    probs = F.softmax(logits, axis=-1)
    idx = paddle.argmax(probs, axis=1).numpy()
    idx = idx.tolist()
    qids = qids.numpy().tolist()
    results.extend(zip(qids, idx))

'''
4. save predict result...
'''    
# 写入预测结果，提交
with open( "submission.csv", 'w', encoding="utf-8") as f:
    # f.write("数据ID,评分\n")
    f.write("label\n")
 
    for (idx, label) in results:
        f.write(str(label)+"\n")
```

![](/assets/img/记录：联通2022网络AI技能大赛/2022-09-21-15-30-44.png)

#####  **<center><font color=Navy>Conclusion</font></center>** 
    分析：结果不好，准确率只有63.85%

#####  **<center><font color=Tomato>Question</font></center>** 
问题1：loss曲线出现了震荡情况，且不收敛

解决方案：根据知乎的解释
a. batchsize太小，过拟合了当前batch，以至于batch上预估的梯度无法有效近似数据集分布上估计的梯度，从而无法泛化到下一个batch上。解决方案: 调大batchsize, 使用带动量的梯度。
<font color=DodgerBlue>batchsize=16，的确有些小，但是已经是带动量的梯度了</font>
~~b. 学习率太大。解决方案：减小学习率。~~
<font color=DodgerBlue>学习率已经是4e-5</font>
~~c. 收敛区域不太好但又跳不出来，想想进入了凹凸不平的盆地。解决方案：周期学习率。~~
<font color=DodgerBlue>学习率已经是周期学习率</font>

### 2.2 batch_size=64 Acc=71.46%(存档1)
其他超参数：
    max_seq_length = 128
    batch_size = 64
    learning_rate = 4e-5
    epochs = 3
    warmup_proportion = 0.1
    weight_decay = 0.01
开始训练：2022-09-21 15:56:28

#### 2.2.1 存档1
当前时间：2022-09-22 09:46:30
进度：
> 模型保存在 16500 步， 最佳eval准确度为0.81037571！
> global step 16190, epoch: 2, batch: 7152, loss: 0.32919, accu: 0.82326, speed: 1.34 step/s

![](/assets/img/记录：联通2022网络AI技能大赛/2022-09-22-09-48-29.png)
 
速度快多了。并且acc收敛了，<font color=>为啥loss还在震荡？！！！</font>。
![](/assets/img/记录：联通2022网络AI技能大赛/2022-09-22-09-54-55.png)  
![](/assets/img/记录：联通2022网络AI技能大赛/2022-09-22-09-55-18.png)

查阅资料[^trainloss震荡]得到以下分析：
> 1.train_loss 不断下降，val_loss(test_lost) 不断下降
> 
> 说明网络训练正常，最好情况
> 
> 2.train_loss 不断下降，val_loss(test_lost) 趋于不变
> 
> 说明网络过拟合，可以添加dropout和最大池化max pooling
> 
> 3.train_loss 趋于不变，val_loss(test_lost) 不断下降
> 
> 说明数据集有问题，建议重新选择
> 
> 4.train_loss 趋于不变，val_loss(test_lost) 趋于不变
> 
> 说明学习遇到瓶颈，需要减小学习率或批量batch数目
> 
> 5.train_loss 不断上升，val_loss(test_lost) 不断上升
> 
> 说明网络结构设计不当，训练超参数设置不当，数据集经过清洗等问题，最差情况

审阅这轮参数的模型给出的预测结果：
```python 
import pandas as pd
import numpy as np

answer = pd.read_csv("../test/test_set_answer.csv",header=0,index_col=None)
predict = pd.read_csv("../test/test_set_predict.csv",header=0,index_col=None)

def analysis(predict,answer):
    if len(predict)!=len(answer):
        return 
    distribution_dict = {'11':[],'01':[],'10':[],'00':[]}
    for idx,(pred,ans) in enumerate(zip(predict,answer)):
        if idx==0:
            continue #跳过首行
        distribution_dict[str(ans)+str(pred)].append(idx)
        
    # 将distribution_dict的所有list填充到一致的长度，用0填充
    pad_dict = {'11':[],'01':[],'10':[],'00':[]}
    for key,value_list in distribution_dict.items():
        arr = np.array(value_list)
        arr_pad = np.pad(array=arr,
                pad_width = (0,max([len(l) for k,l in distribution_dict.items()])-len(value_list)),# 开头填充0个元素，结尾填充（四个list最长长度-当前list的长度）个元素
                mode='constant')
        pad_dict[key] = arr_pad.tolist()
    
    ana_df = pd.DataFrame.from_dict(pad_dict)
    ana_df.to_csv("./bs64/analy_distribution.csv",sep="\t",index=False)    
    return ana_df

def distribution_rate(ana_df):
    count11 = len(ana_df['11'])
    count10 = len(ana_df[~ana_df['10'].isin([0])])
    count01 = len(ana_df[~ana_df['01'].isin([0])])
    count00 = len(ana_df[~ana_df['00'].isin([0])])
    print("答案是1，预测是1：",count11) # 11列没有0
    print("答案是1，预测是0：",count10) 
    print("答案是0，预测是1：",count01) 
    print("答案是0，预测是0：",count00,"\n") 
    
    print("预测是1中，预测对的有：",count11/(count11+count01))
    print("预测是0中，预测对的有：",count00/(count10+count00),"\n")
    
    print("答案是1中，预测对的有：",count11/(count11+count10))
    print("答案是0中，预测对的有：",count00/(count01+count00),"\n")
    
    print("所有预测错的中，（答案=）1的有：",count10/(count10+count01),"所有预测错的中，（答案=）0的有：",count01/(count10+count01))

result = analysis(predict['label'],answer['label'])   
distribution_rate(result)
```
![](/assets/img/记录：联通2022网络AI技能大赛/2022-09-22-10-37-46.png)
很明显，样本不均衡导致预测结果中，积极情感预测效果好，而消极情感预测效果差。
#### 2.2.2 存档2
当前时间：2022-09-22 11:12:30
进度：
> 上一个存档：
> 模型保存在 16500 步， 最佳eval准确度为0.81037571！
> global step 16190, epoch: 2, batch: 7152, loss: 0.32919, accu: 0.82326, speed: 1.34 step/s
> 当前存档：
> 模型保存在 17800 步， 最佳eval准确度为0.81231891！
> global step 17860, epoch: 2, batch: 8822, loss: 0.37988, accu: 0.81094, speed: 1.33 step/s

![](/assets/img/记录：联通2022网络AI技能大赛/2022-09-22-11-14-51.png)
![](/assets/img/记录：联通2022网络AI技能大赛/2022-09-22-11-15-17.png)

####  **<center><font color=Tomato>Question</font></center>** 
问题1：loss曲线出现了震荡情况
解决方案：根据csdn的解释，应当是数据集的问题，样本不均衡。根据知乎的解释，极不平衡要做的，除非小样本数据里有特别特别明显的特征，要不然会训练不出来。
（1）对数据少的类别，要做数据增强。可以去查查，目前有很多数据增强的方法，例如同义词替换、生成、改写。
（2）loss的设大，可以增加少样本类别的loss权重，让模型重视这一类别，而不是一视同仁

### 2.3 batch_size=64 loss_weight=[2.31,1.0]
其他超参数(与1.2保持一致)：
    max_seq_length = 128
    batch_size = 64
    learning_rate = 4e-5
    epochs = 3
    warmup_proportion = 0.1
    weight_decay = 0.01
开始训练：2022-09-22 14:12:28

#### 2.3.1 存档1
当前时间：2022-09-22 19:01:30
在paddle_finetune.py文件中，165行设置如下：
```python 
# 由于样本不平衡，指定交叉熵损失函数每个类别的权重M2
# M1.[1/count(0),1/count(1)]
# M2.[max(count)/count(0),max(count)/count(1)]
weight_data = np.array([2.31,1.0]).astype("float32")
weight = paddle.to_tensor(weight_data)
```
> 当前存档：
> 模型保存在 900 步， 最佳eval准确度为0.72405209！
> global step 3460, epoch: 1, batch: 3460, loss: 0.69813, accu: 0.49036, speed: 0.60 step/s

![](/assets/img/2022-09-22-记录：联通2022网络AI技能大赛/2022-09-22-18-59-18.png)
![](/assets/img/2022-09-22-记录：联通2022网络AI技能大赛/2022-09-22-18-59-38.png)

是不是weight设反了？

```python
由于样本不平衡，指定交叉熵损失函数每个类别的权重M2
M1.[1/count(0),1/count(1)]
M2.[max(count)/count(0),max(count)/count(1)]
weight_data = np.array([1.0,2.31]).astype("float32")
weight = paddle.to_tensor(weight_data)

criterion = paddle.nn.loss.CrossEntropyLoss(weight=weight)  # 交叉熵损失函数
```

> 模型保存在 3500 步， 最佳eval准确度为0.78014204！
> global step 11240, epoch: 2, batch: 2202, loss: 0.37571, accu: 0.70195, speed: 1.11 step/s

![](/assets/img/2022-09-22-记录：联通2022网络AI技能大赛/2022-09-23-10-55-39.png)
![](/assets/img/2022-09-22-记录：联通2022网络AI技能大赛/2022-09-23-10-55-58.png)

为什么还是不行？？
> 5.train_loss 不断上升，val_loss(test_lost) 不断上升
> 说明网络结构设计不当，训练超参数设置不当，数据集经过清洗等问题，最差情况

难道是超参数？



### 2.3 数据增强-增加负样本数据微调
```python 
'''
生成负样本1：1
'''
import os
import jieba
import numpy as np

from gensim.models import KeyedVectors, TfidfModel
from gensim.corpora import Dictionary
from utils.utils import read_samples, write_samples, isChinese
from gensim import matutils
from itertools import islice

class EmbedReplace():
    def __init__(self, sample_path, wv_path):
        self.samples = read_samples(sample_path)
        self.samples = [list(jieba.cut(sample)) for sample in self.samples]
        self.wv = KeyedVectors.load_word2vec_format(wv_path, binary=False)

        if os.path.exists('tfidf_word2vec/tfidf.model'):
            self.tfidf_model = TfidfModel.load('tfidf_word2vec/tfidf.model')
            self.dct = Dictionary.load('tfidf_word2vec/tfidf.dict')
            self.corpus = [self.dct.doc2bow(doc) for doc in self.samples]
        else:
            self.dct = Dictionary(self.samples)
            self.corpus = [self.dct.doc2bow(doc) for doc in self.samples]
            self.tfidf_model = TfidfModel(self.corpus)
            self.dct.save('tfidf_word2vec/tfidf.dict')
            self.tfidf_model.save('tfidf_word2vec/tfidf.model')
            self.vocab_size = len(self.dct.token2id)

    def vectorize(self, docs, vocab_size):

        return matutils.corpus2dense(docs, vocab_size)

    def extract_keywords(self, dct, tfidf, threshold=0.2, topk=5):
        """ 提取关键词

        :param dct (Dictionary): gensim.corpora.Dictionary
        :param tfidf (list):
        :param threshold: tfidf的临界值
        :param topk: 前 topk 个关键词
        :return: 返回的关键词列表
        """
        tfidf = sorted(tfidf, key=lambda x: x[1], reverse=True)

        return list(islice([dct[w] for w, score in tfidf if score > threshold], topk))

    def replace(self, sample, doc):
        """用wordvector的近义词来替换，并避开关键词

        :param sample (list): reference token list
        :param doc (list): A reference represented by a word bag model
        :return: 新的文本
        """
        keywords = self.extract_keywords(self.dct, self.tfidf_model[doc])
        #
        num = int(len(sample) * 0.3)
        new_tokens = sample.copy()
        indexes = np.random.choice(len(sample), num)
        for index in indexes:
            token = sample[index]
            if isChinese(token) and token not in keywords and token in self.wv:
                new_tokens[index] = self.wv.most_similar(positive=token, negative=None, topn=1)[0][0]

        return ''.join(new_tokens)

    def generate_samples(self, write_path):
        """得到用word2vector词表增强后的数据

        :param write_path:
        """
        replaced = []
        for sample, doc in zip(self.samples, self.corpus):
            replaced.append(self.replace(sample, doc))
        write_samples(replaced, write_path, 'a')

if __name__ == '__main__':
    sample_path = 'data/train.txt'
    wv_path = 'tfidf_word2vec/sgns.weibo.bigram-char'
    replacer = EmbedReplace(sample_path, wv_path)
    replacer.generate_samples('data/replaced.txt')

```

```python
'''
合并数据增强的结果到原训练数据中
'''
import pandas as pd

text = []
with open("./NLP-Data-Augmentation-main/data/replaced.txt",'r',encoding="utf-8") as f:
    text = f.readlines()
text= [x.strip() for x in text] # 删除换行符
d = {"text":text}
nega_df = pd.DataFrame(d)
nega_df["label"] = 0

old_df = pd.read_csv("raw_data.csv",sep="\t",header=0,index_col=None)

merge_df = pd.concat([old_df,nega_df],axis=0).sample(frac=1)
merge_df.to_csv("enhanced_data.csv",sep="\t",header=True,index=False)

```
![](/assets/img/2022-09-22-记录：联通2022网络AI技能大赛/2022-09-23-11-05-39.png)
增强后的正负样本比例接近1：1，不再需要调整交叉熵类别权重，weight删除。
```python
criterion = paddle.nn.loss.CrossEntropyLoss()  # 交叉熵损失函数
```
因为上面的实验超参数貌似不合适，全部根据官方文档[^SKEP]修改：

![](/assets/img/2022-09-22-记录：联通2022网络AI技能大赛/2022-09-23-11-08-47.png)
```python
'''
0. define super-parameters
'''
max_seq_length = 128
batch_size = 64
learning_rate = 5e-5
epochs = 3
warmup_proportion = 0.0
weight_decay = 0.00
```

* ## Method3：SKEP-large作为预训练模型
###  3.1 调用API

```python
from paddlenlp import Taskflow 
import pandas as pd

all_df = pd.read_csv("../test/test_set_data.csv",sep="\t",index_col=None)
senta = Taskflow("sentiment_analysis", model="skep_ernie_1.0_large_ch", device_id=1) 

d = {"negative":0,"positive":1}

for text in all_df["review"]:
    # print(senta(text)[0])
    with open("try/test_set_predict.csv","a",encoding="utf-8") as f:
        f.write(str(d[senta(text)[0]['label']])+'\n')

```
![](/assets/img/2022-09-22-记录：联通2022网络AI技能大赛/2022-09-22-19-11-55.png)

####  3.2 数据增强-增加负样本数据微调



####  3.3 根据概率值增加If-Else，逼近答案




## :link:Reference
[^trainloss震荡]:https://blog.csdn.net/qq_44528283/article/details/111768197
[^BERT样本不平衡]:https://www.zhihu.com/question/406049527/answer/1329854918
[^SKEP]:https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/sentiment_analysis/skep