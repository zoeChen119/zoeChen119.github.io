---
title: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models
categories: [NLP,论文笔记]
tags: [nlp,ChatGPT,推理,论文笔记]     # TAG names should always be lowercase
---

# Chain-of-Thought Prompting Elicits Reasoning in Large Language Models

## Background
![](/assets/img/2023-03-13-PaperNote_思维链/2023-03-13-16-09-18.png)
目前，较为常见的 NLP 推理任务大概可以被分成上图这三种：数学推理，常识推理以及符号推理。前两种很容易理解，就是字面意思，而最后一种，即符号推理，通常指的是给定一些特定符号代表的状态，然后对它们做一些操作，问题就是最后的状态是什么样的，例如给定一个列表，对其进行旋转操作，问你最终的状态是什么。下图中列出了这三种类型的任务的一些代表性数据集。
![](/assets/img/2023-03-13-PaperNote_思维链/2023-03-13-16-09-43.png)
语言模型可以用自然语言生成中间推理步骤，相比那些直接输出概率分布的分类模型，这为我们观察模型如何一步步达到最终答案提供了一个窗口。例如一个情感分类问题，语言模型能告诉你某句话里的某个词表达的是不满情绪，因而它将其分为负类，而用类似BERT这样的模型，要么是直接输出一个二分类的概率分布，要么是通过设计模板+完形填空的方式 (This movie is awesome. Sentiment: [MASK])，得到一个看似具有解释性的答案，但是其实模型输出的只不过是从一个二分类概率分布变成了一个全词表的概率分布，经过一些后处理，最终又变成了一个二分类的概率分布。

总体上，我把介绍的相关工作分成三个主要的类型，如下图所示：一个生成器加一个验证器，思维链提示 (CoT Prompt) 以及前两种的混合方法。在第一种类型中，生成器负责生成多个推理路径，验证器用于评估这些生成的解答，并选出最终的答案。

![](/assets/img/2023-03-13-PaperNote_思维链/2023-03-13-16-12-50.png)
思维链提示主要是靠人工编写一些带有详细中间推理步骤的问答范本作为 Prompt 拼在要问的问题前面，模型模仿前面的例子对问题做出详细的推理和解答，这通常被称为语境学习 (In-context learning)。


## Total
思维链：一系列中间推理步骤。
1. 本文探究了“如何通过生成思维链来有效提升大语言模型的复杂推理能力”。
2. 用“思维链prompt”这种方法，展示了大语言模型驯化出推理能力的过程。给了一个演示示例。
3. 用3个实验，证明了“思维链prompt”提高了大语言模型的算术、常识、符号推理任务的性能。
4. 发现模型有惊人的经验收获。例如，仅用8个思维链示例提示PaLM 540B就能在GSM8K数学字谜基准测试中达到最先进的精度，甚至超过了带有验证器的微调GPT-3。

![](/assets/img/2023-03-13-PaperNote_思维链/2023-03-13-13-58-50.png)

## 成果3

针对第3点，介绍三个针对不同推理任务的实验：


### 1. 算术推理
![](/assets/img/2023-03-13-PaperNote_思维链/2023-03-13-14-04-34.png)
![](/assets/img/2023-03-13-PaperNote_思维链/2023-03-13-14-06-13.png)

**3种不同的思维链探究了三种不同的（思维链生效原因）猜测**
* assumption A：它产生了要计算的数学方程
* assumption B：思维链允许模型在更难的问题上花费更多的计算（即中间token）
* assumption C：提示允许模型更好地访问在训练前获得的相关知识（模型是否主要依赖的是思维链prompt）
![](/assets/img/2023-03-13-PaperNote_思维链/2023-03-13-14-11-56.png)

(1) Equation only.
在给出答案之前，模型被提示只输出一个数学方程。图5显示，只有方程式提示对GSM8K没有太大帮助，这意味着GSM8K中问题的语义太具有挑战性，无法在没有思维链中的自然语言推理步骤的情况下直接转换为方程式（模型还不会把方程直接理解为语言）。

(2) Variable compute only.
只提示变量，（将变量计算从思维链的推理过程种解放出来，就是说给模型减轻负担），给模型提示“仅输出一个...”，这个...等于解决问题所需的方程的字节数。
这一变体的表现与基线大致相同，这表明变量计算本身并不是思想链提示成功的原因，而且通过自然语言表达中间步骤似乎很有用。

(3) Chain of thought after answer.
在答案之后给出思维链的提示，这样研究模型是否实际上依赖于产生的思维链来给出最终答案。这一变体的表现与基线基本相同，这表明思想链中体现的顺序推理在激活知识之外的原因上是有用的。



### 2. 常识推理

#### Benchmarks-5个常识数据集
Dataset1：CSQA
提出了关于世界的常识性问题，涉及复杂的语义，通常需要先验知识。

Dataset2：StrategyQA
需要模型推断多跳策略来回答问题。

这两个都来自于BIG-bench的验证集：
Dataset3.1：Date
从给定的上下文中推断日期。

Dataset3.2：Sports
确定与体育相关的句子是否合理或不可信。

Dataset4：SayCan
从离散集合中将自然语言指令映射到一系列机器人动作。

#### Prompts
对于`Dataset1：CSQA`和`Dataset2：StrategyQA`，从训练集中随机采样一些样本，人工构造思维链形成few-shot范例。
**对于那两个来自`BIG-bench`的数据集`Dataset3.1：Date`&`Dataset3.2：Sports`,由于没有训练集，所以选前十个样本作为范例，并报告了剩余验证集中的数量。**
对于`Dataset4：SayCan`，使用来自 Ahn 等人使用的训练集的六个示例。 (2022) 以及手动组合的思维链。


#### Results
![](/assets/img/2023-03-13-PaperNote_思维链/2023-03-13-14-50-59.png)

### 3. 符号推理

#### Tasks
1. Last letter concatenation.
接龙游戏

2. Coin flip.
翻硬币游戏

#### Results
![](/assets/img/2023-03-13-PaperNote_思维链/2023-03-13-15-49-53.png)
out-of-domain (OOD)


## 思维链的鲁棒性
略

## 发散
在此基础上，针对零样本场景，利用推荐关键词“Let's think step by step”生成中间步骤的内容，从而避免人工撰写中间步骤的过程。

尽管思维链模拟了人类推理的思维过程，但仍然不能确定神经网络是真的“推理”了。仍是悬而未决的问题。

虽然少样本中人工给出思维链提示的成本很小，但这种注释成本相对于微调还是令人望而却步（也可以用synthetic data generation合成数据生成, or zero-shot generalization零样本泛化来处理这个问题）。

推理路径不一定正确，不正确的推理路径会导致错误的答案。改进语言模型的事实生成（improving factual generations of language models）是未来的方向。

尽在大型模型上用思维链推理，成本太高，能否使用小模型诱导推理？


## Reference
https://zhuanlan.zhihu.com/p/607212335