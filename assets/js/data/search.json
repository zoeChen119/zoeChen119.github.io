[ { "title": "【NLP入门趣味题】探索语言模型与词向量", "url": "/posts/C25_11%E5%85%A5%E9%97%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%861/", "categories": "AI-Courses, AI-Introduction-202503", "tags": "Courses, nlp", "date": "2025-05-06 15:00:56 +0800", "snippet": "【NLP入门趣味题】探索语言模型与词向量题目名称：“猜猜我是谁？——用词向量玩文字游戏”🎯 任务目标 理解语言模型（Language Model）的基本概念——它能预测下一个词是什么！ 认识词向量（Word Embedding）——让计算机”看懂”词语的数学表达。 通过简单代码体验语义相似度，感受AI如何理解词语关系。🔍 题目内容假设你有一个超级简单的”语言模型”，它知道以下词语的关系（通过词向量表示）： 词语 向量（简化版） 国王 [1, 0, 0] 王后 [0.9, 0.1, 0] 男人 [0.8, 0, 0.2] 女人 [0.7, 0.1, 0.2] 苹果 [0, 1, 0] 香蕉 [0, 0.9, 0.1] 问题1：如果语言模型看到句子”国王 - 男人 + 女人 = ?”，根据词向量计算，结果最接近哪个词？（提示：用向量加减法，比如 [1,0,0] - [0.8,0,0.2] + [0.7,0.1,0.2]）问题2：计算”苹果”和”香蕉”的余弦相似度（公式如下），它们为什么比”苹果”和”国王”更相似？\\(\\text{相似度} = \\frac{A \\cdot B}{|A||B|} = \\frac{\\sum_{i=1}^n A_i B_i}{\\sqrt{\\sum_{i=1}^n A_i^2} \\sqrt{\\sum_{i=1}^n B_i^2}}\\)问题3（编程实践）：用Python代码实现以下功能： 用字典存储上述词向量（如 word_vectors = {\"国王\": [1,0,0], ...}） 写一个函数 similarity(a, b) 计算两个词的余弦相似度 测试”王后”和”女人”的相似度，观察结果是否符合你的直觉？💡 参考答案问题1：国王 - 男人 + 女人 = [1,0,0] - [0.8,0,0.2] + [0.7,0.1,0.2] = [0.9, 0.1, 0] → 最接近”王后”！（这就是著名的”King - Man + Woman = Queen”的简化版！）问题2： “苹果”和”香蕉”都是水果，向量第二维（代表”水果”特征）值高，相似度≈0.99。 “苹果”和”国王”向量完全不同，相似度≈0。问题3代码示例：import numpy as np# 词向量字典word_vectors = { \"国王\": [1, 0, 0], \"王后\": [0.9, 0.1, 0], \"苹果\": [0, 1, 0], \"香蕉\": [0, 0.9, 0.1]}def similarity(a, b): \"\"\"计算余弦相似度\"\"\" va, vb = np.array(a), np.array(b) return np.dot(va, vb) / (np.linalg.norm(va) * np.linalg.norm(vb))# 测试print(similarity(word_vectors[\"王后\"], word_vectors[\"女人\"])) # 输出应接近0.98🌟 知识点总结 语言模型：像完形填空一样预测词语（如”猫喜欢吃___“→”鱼”）。 词向量：词语的”数学身份证”，相似词距离近（如”狗”和”猫”比”狗”和”冰箱”更近）。 语义计算：通过向量加减发现关系（如”中国-北京+东京≈日本”）。🌟 一些小问题？（五一期间 · 作业） 余弦相似度和欧氏距离：请你调研这两种方式的区别？ 他们各自有什么优缺点？ 延伸思考：如果词向量有”性别”和”王室”两个维度，”国王”和”王后”的值会有什么不同？ 😉" }, { "title": "【NLP入门趣味题】肉眼找朋友", "url": "/posts/C25_11%E5%85%A5%E9%97%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%860/", "categories": "AI-Courses, AI-Introduction-202503", "tags": "Courses, nlp", "date": "2025-04-29 15:00:56 +0800", "snippet": "【NLP入门趣味题】肉眼找朋友❓ 问题1：肉眼找朋友观察坐标，回答以下问题（无需计算）： “国王” 和 “王后” 的距离近，还是 “苹果” 和 “香蕉” 的距离近？ 如果 “国王” 的坐标是 (1, 0)，你觉得 “王子” 的坐标可能是？ A. (0.8, 0.2) B. (0, 0.8) 💡 提示：第一个数字代表“王室程度”，第二个数字代表“水果程度”！🐍 问题2：Python小实验30分钟尝试用代码实现👇# 定义一个words字典存储上面四个词语及其向量坐标# 定义函数word_distance(a,b)计算词语a和词语b的距离# 测试 国王和王后的距离；苹果和香蕉的距离；国王和苹果的距离运行以下代码，看看计算机会不会和你答案一致？words = { \"国王\": [1, 0], \"王后\": [0.9, 0.1], \"苹果\": [0, 1], \"香蕉\": [0, 0.9]}# 计算两点的直线距离（简化版）def word_distance(a, b): return ((a[0]-b[0])**2 + (a[1]-b[1])**2)**0.5# 测试print(\"国王和王后的距离:\", word_distance(words[\"国王\"], words[\"王后\"]))print(\"苹果和香蕉的距离:\", word_distance(words[\"苹果\"], words[\"香蕉\"]))输出结果：国王和王后的距离: 0.1414...苹果和香蕉的距离: 0.1...🎯 知识点总结 词向量：词语被表示成坐标（如“国王”→[1,0]），类似游戏地图里的位置。 语义相似：坐标靠近的词语意义相关（如水果类聚在一起）。 计算机如何理解：通过计算坐标距离判断词语关系（距离越小越相似）。🌟 一些小问题？（五一期间 · 作业） 词语之间的距离：请你调研还有哪些计算词语之间距离的公式或方式？ 他们各自有什么优缺点？ 延伸思考：如果加入第三个维度“动物”，你觉得“猫”和“狗”的坐标会是什么样？ 😸→[?, ?, ?]" }, { "title": "多模态实验", "url": "/posts/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%AE%9E%E9%AA%8C/", "categories": "", "tags": "", "date": "2025-02-11 00:00:00 +0800", "snippet": "多模态实验过程记录实验1：colpaligemma-3b-pt-448-base测试PaliGemmaForConditionalGeneration( (vision_tower): SiglipVisionModel( (vision_model): SiglipVisionTransformer( (embeddings): SiglipVisionEmbeddings( (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid) (position_embedding): Embedding(1024, 1152) ) (encoder): SiglipEncoder( (layers): ModuleList( (0-26): 27 x SiglipEncoderLayer( (self_attn): SiglipSdpaAttention( (k_proj): Linear(in_features=1152, out_features=1152, bias=True) (v_proj): Linear(in_features=1152, out_features=1152, bias=True) (q_proj): Linear(in_features=1152, out_features=1152, bias=True) (out_proj): Linear(in_features=1152, out_features=1152, bias=True) ) (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True) (mlp): SiglipMLP( (activation_fn): PytorchGELUTanh() (fc1): Linear(in_features=1152, out_features=4304, bias=True) (fc2): Linear(in_features=4304, out_features=1152, bias=True) ) (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True) ) ) ) (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True) ) ) (multi_modal_projector): PaliGemmaMultiModalProjector( (linear): Linear(in_features=1152, out_features=2048, bias=True) ) (language_model): GemmaForCausalLM( (model): GemmaModel( (embed_tokens): Embedding(257216, 2048, padding_idx=0) (layers): ModuleList( (0-17): 18 x GemmaDecoderLayer( (self_attn): GemmaAttention( (q_proj): Linear(in_features=2048, out_features=2048, bias=False) (k_proj): Linear(in_features=2048, out_features=256, bias=False) (v_proj): Linear(in_features=2048, out_features=256, bias=False) (o_proj): Linear(in_features=2048, out_features=2048, bias=False) ) (mlp): GemmaMLP( (gate_proj): Linear(in_features=2048, out_features=16384, bias=False) (up_proj): Linear(in_features=2048, out_features=16384, bias=False) (down_proj): Linear(in_features=16384, out_features=2048, bias=False) (act_fn): PytorchGELUTanh() ) (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06) (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06) ) ) (norm): GemmaRMSNorm((2048,), eps=1e-06) (rotary_emb): GemmaRotaryEmbedding() ) (lm_head): Linear(in_features=2048, out_features=257216, bias=False) ))图中左侧的Vision LLM：PaliGemma-3B 是一种视觉-语言模型（VLM），该模型使用的ViTs模型是SigLIP-So400m/14，它可以生成高质量的图像嵌入。1》processor = AutoProcessor.from_pretrained(model_name)inputs = processor(images=image, text=prompt, return_tensors=”pt”) inputs[‘input_ids’].shape = torch.Size([1, 1034]) inputs[‘attention_mask’].shape = torch.Size([1, 1034]) inputs[‘pixel_values’].shape = torch.Size([1, 3, 448, 448])2》batch_images = ColPaliProcessor.process_images(RGB图像列表) batch_images = processor.process_images(images).to(model.device) batch_queries = processor.process_queries(queries).to(model.device) batch_images [‘input_ids’].shape = torch.Size([2, 1030]) batch_images [‘attention_mask’].shape = torch.Size([1, 1030]) batch_images [‘pixel_values’].shape = torch.Size([2, 3, 448, 448]) batch_queries[‘input_ids’].shape = torch.Size([2, 27]) batch_queries[‘attention_mask’].shape = torch.Size([2, 27])image_embeddings = model(**batch_images) # with torch.no_grad():proj就是一层用于降维的线性层。图中右侧的LLM：该模型使用的LLMs模型是Gemma-2B。一个特别有趣的特性是，PaliGemma-3B 的文本模型是通过带有前缀（指令文本和图像标记）的全块注意力机制进行微调的。batch_queries = ColPaliProcessor.process_queries(文本句子列表)query_embeddings = model(**batch_queries) # with torch.no_grad():应用：scores = ColPaliProcessor.score_multi_vector(query_embeddings, image_embeddings)Base模型选择（1）Metric-AI/ColQwen2.5-3b-multilingual-v1.0 Vidore榜单排名第2 是lora微调后的模型 是否能把lora微调后的和没有lora微调的模型放在一起对比？ 这是一个多语言模型Alibaba-NLP/gme-Qwen2-VL-2B-Instruct Vidore榜单排名第13 原模型 排名靠前的都是lora微调后的模型，就这一个阿里的不是（2）论文1：多模态综述2025年2月的4.4 Generation Techniques6.3 Agent-Based and Self-Guided Systems将强化学习和端到端与人一致的反馈结合到多模态rag中，在很大程度上仍未被探索，但在增强这些系统方面具有巨大的潜力。（3）论文2：SK-VQASK-VQA: Synthetic Knowledge Generation at Scale for Training Context-Augmented Multimodal LLMs情景增强的MLLM，具体创新是一种能够用于大规模收集自然和多样化数据。没有依靠模板的方法（template-based methods）为真实数据构建QA对，就是利用GPT4来为给定图像生成相关上下文文档和多个问答对，以此创建了一个SK-VQA数据集（迄今为止最大的KBVQA数据集，包含超过200万个问答对）。流程： 数据集生成 图像参考(ImRef)过滤 上下文答案存在 (CAP) 过滤（4）论文3：R1-SearcherR1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning核心思路：基于结果的两阶段强化学习方法，旨在增强LLMs的搜索能力。开源链接： 代码仓库：https://github.com/SsmallSong/R1-Searcher 模型： Qwen-2.5-7B-Base-RAG-RL: https://huggingface.co/XXsongLALA/Qwen-2.5-7B-base-RAG-RL Llama-3.1-8B-Instruct-RAG-RL: https://huggingface.co/XXsongLALA/Llama-3.1-8B-instruct-RAG-RL 训练数据：https://huggingface.co/datasets/XXsongLALA/RAG-RL-Hotpotqa-with-2wiki我们使用两阶段结果监督强化学习，整体基于Reinforce++算法。在第一阶段，模型被训练以有效利用外部检索系统，在第二阶段，模型被训练在推理过程中整合检索，以准确解答问题。我们通过奖励设计实现两阶段训练： 第一阶段，reward由retrieval-reward和format-reward组成，如果模型在推理过程中进行了检索，就会得到retrieval-reward，旨在让模型学会调用工具的格式； 第二阶段，retrieval-reward被替换为answer-reward，让模型更自由地进行探索，answer-reward是标准答案和预测答案的F1-Score，旨在让模型学会正确调用工具解决问题。另外，我们对Reinforce++算法进行了修改以适应检索增强生成场景。我们的目标是让模型在面对不确定性时能够自主获取外部知识，从而有效整合推理和检索。为了无缝整合检索到的文档并确保模型优化的合理性，我们对原始算法进行了两项改进：RAG-based Rollout和Retrieval Mask-based Loss Calculation： RAG-based Rollout： 我们使用标签...来引导模型在生成过程中调用外部检索系统。捕捉到模型需要进行检索时，推理暂停并进行检索。检索到的文档被封装在...标签中，并整合到模型的推理过程中。这种方法确保检索无缝融入推理过程，使模型能够基于检索到的文档继续推理，而不被打断。 Retrieval Mask-based Loss Calculation：当模型执行检索时，检索到的文档作为环境观察的一部分被整合到推理过程中。然而，模型并不需要自主生成这些文档。为了减少环境的影响，我们将...指定为特殊标记，并在训练中对其进行掩码处理。这可以防止这些外部标记影响损失计算，确保检索到的文档不会干扰模型的内在推理和生成过程。实验结果🚀 如下表所示，我们的方法R1-Searcher： 在多跳问答任务上实现显著的性能提升：相比于最好的基线ReARTeR，R1-Searcher，使用相同的LLaMA-3.1-8B-Instruct作为backbone，实现了显著的性能提升：在HotpotQA上提升了48.2%，在2WikiMultiHopQA上提升了21.7%，在Bamboogle上提升了4.0%（LLM-as-Judge）。这表明我们的方法可以有效地促进模型在推理过程中进行准确的检索调用。 从基础LLM开始进行RL学习，无需冷启动：我们从头开始使用强大的基础模型（如Qwen-2.5-7B-Base）进行RL学习。令人惊讶的是，我们能够取得更好的结果，并在大多数领域内和领域外的数据集上获得最佳性能，甚至超过了闭源的LLM，如GPT-4o-mini。这些结果展示了我们的两阶段RL方法在指导LLMs学习过程中的有效性。 保持泛化能力：我们仅使用HotpotQA和2WikiMultiHopQA训练集中的8148个样本进行RL训练。该模型不仅在这些领域内数据集上表现出色，还在领域外数据集（如Musique和Bamboogle）上展示了强大的泛化能力。这表明模型通过在RL训练期间的探索，有效地学习了检索并将其与推理相结合，从而在需要检索的新测试数据集上实现稳健的性能。另外，为了评估模型对于联网搜索泛化能力，我们在最新提出的Bamboogle任务上进行联网搜索的测试，这种设定在RL训练期间并未遇到。如下图所示，我们的模型相较于使用相同Qwen-2.5-7B-Base作为backbone的本地检索系统，性能提升了18.2%。此外，与使用相同在线搜索但骨干模型更大的32B的Search-o1相比，我们的模型性能提升了11.4%。这表明我们的模型能够适应在线搜索场景，并且R1-Searcher使模型能够在推理过程中检索信息，而不仅仅是记忆响应格式。我们针对以下问题进行了更详细的实验和分析，完整的分析请看原论文： GRPO和Reinforce++算法的比较 结论：GRPO的生成solution更长和检索频率更高。GRPO在领域外测试数据集（如Bamboogle）上也展现出更好的性能；而Reinforce++在领域内测试集（如HotpotQA和2Wiki）上表现更优。 RL和SFT的比较 结论：RL在领域内和领域外的测试集上均优于SFT。SFT能够帮助模型生成检索查询，但这些查询的时机和相关性不如通过RL训练生成的查询。 Reward的设计对训练的影响 结论：基于F1的答案奖励能够产生更长的回答长度和更优的最终结果；基于EM的奖励在训练过程中导致回答长度较短，并且在测试时表现不如基于CEM或F1的奖励；基于CEM的奖励会生成带有不必要信息的偏长的answer。 数据难度分布和数据多样性对训练的影响 结论：使用混合数据集训练的模型在检索次数和生成回答长度上都有所增加，并且在测试集上取得了更高的分数；训练数据中混入较高难度的数据可以在领域内和领域外的测试集上均取得更好的效果。案例展示Idea由于计算资源有限，就在别人训练好的模型（比如Qwen2-VL）的基础上用DQN/PPO微调，强化学习用的数据集就是DocVQA这种常见的多模态测试数据集，DocVQA数据集中的标准答案就当作是反馈（如果MLLM生成的responce和这个答案一致就点赞，否则就纠正），query就是查询，image就是图像。可行性分析 基础模型选择：选择一个性能优越的预训练模型作为起点，可以大大减少所需的计算资源，并缩短开发周期。Qwen2-VL这样的多模态模型能够很好地理解文本和图像信息，为后续的任务提供了坚实的基础。 数据集选择：DocVQA是一个非常适合用于此目的的数据集，它包含了大量文档图像及其对应的问题与答案，有助于训练模型准确理解和回答基于图像的问题。 反馈机制设计：将DocVQA中的标准答案用作反馈信号，通过比较生成的回答与标准答案的一致性来提供奖励（点赞或纠正），这种方法直观有效。这不仅简化了奖励函数的设计，还确保了反馈的质量。实施建议 环境设置： 状态空间：定义状态空间为当前查询、图像以及模型生成的回答。 动作空间：动作空间可以是模型参数的调整方向和幅度，或者是直接对回答进行修正的选择。 强化学习算法选择： PPO可能更适合这种场景，因为它在处理高维状态和动作空间时表现良好，并且能更稳定地更新策略，避免了DQN中可能出现的过估计问题。 奖励设计： 除了简单的“点赞/纠正”之外，还可以考虑引入部分奖励机制，即即使回答不完全正确，但如果包含了正确的关键信息，也可以给予一定的正向激励。 对于纠正的情况，可以尝试给出具体的修改建议，而不是仅仅标记为错误，这样可以帮助模型更快地学习到正确的模式。 实验与评估： 在开始全面训练之前，先在一个较小规模的数据子集上进行初步实验，以验证整个框架的有效性。 使用交叉验证等技术评估模型性能，确保改进措施确实提升了模型的整体表现。 持续优化： 根据实验结果不断调整强化学习策略和参数设置，逐步逼近最优解。 考虑引入用户实际交互数据，进一步丰富训练样本，提升模型的实际应用效果。 " }, { "title": "Papernote_colpali", "url": "/posts/PaperNote_ColPali/", "categories": "", "tags": "", "date": "2024-12-31 00:00:00 +0800", "snippet": "ColPali: Efficient Document Retrieval with Vision Language Models摘要本文针对的研究领域是“视觉丰富的文档检索系统”，本文对这个领域进行了基准测试进而提出了视觉文档检索基准ViDoRe。 ViDoRe由跨越多个领域、语言和设置的各种页面级检索任务组成。本文提出了新的检索模型架构ColPali，核心思想是利用CV模型的文档理解能力从“文档页面的图像”生成高质量的上下文嵌入，再结合后期交互匹配机制（即 延迟交互编码 技术），大大优化了端到端多模态文档检索任务的速度。总结，本文主要两个贡献： 提出了视觉文档检索Benchmark——ViDoRe 提出了新的端到端多模态文档检索架构——ColPaliIntroduction首图：案例：​\tQuery：“Which hour of the day had the highest overall eletricity generation in 2019？”（2019年平均每天24小时的发电量中哪个小时产量最高？）​\tImage：图片中可以看出横纵坐标轴、标题、波动曲线都被高亮了图1图注：​\t对于用户查询中的每一个term（词），ColPali 识别最相关的文档图像块（突出显示的区域），并计算query-to-page的匹配分数。然后，从大型预索引语料库中快速检索最相关的文档。 ？query中每一个词都去识别图像中与之相关的图像块吗？用的什么模型，准确率多高？ ？query-to-page的匹配分数=？\\sum每个词与最相关的图像块的相似值对于PDF文档的识别建索引的传统步骤： PDF解析器/OCR（Optical Character Recognition光学字符识别）系统从页面中提取词语 文档布局检测模型分割段落、标题和其他页面对象，例如表、图形和标题。 定义一个分块策略，对具有某种语义连贯性的文本段落进行分组 最新的检索设置还使用了captioning step（集成步骤），就是额外做一步以图生文，生成一个对图像的描述反过来增强模型对图像的理解。本研究的动机是从Table 2这个实验中发现了优化the ingestion pipeline摄取管道在视觉丰富的文档检索上比优化文本嵌入模型能取得更大的性能提升。Contribution 1—ViDoRe：在这项工作中，我们认为不应仅根据文本嵌入模型的能力来评估文档检索系统，也需要考虑到文档检索中的上下文（文本）和视觉元素。换句话说，一个好的文档检索系统不仅要能理解文字内容，还要能够： 理解上下文：考虑到文档的整体背景和语境，而不仅仅是单独的句子或关键词。 处理视觉信息：识别并理解文档中的图片、图表、表格等非文本元素，并且知道这些元素与文本内容的关系。举个例子，如果你在搜索一篇关于“气候变化”的科学报告，理想的检索系统不仅应该能找到包含相关文字描述的文档，还应该能识别出那些含有重要图表或图片的文档，这些图表或图片可能展示了关键的数据趋势或案例研究，对理解文档内容非常重要。Contribution 2—ColPali：ColPali可以纯粹根据文档的视觉特征有效地对其进行索引，从而允许后续与后期交互机制进行快速查询匹配。问题定义与相关工作（Problem Formulation &amp; Related Work）1. 问题定义在本文的设定中，检索系统会对语料库 D中的文档 d 与查询 q的相关性进行评分。预先对语料库中每个文档计算其与query的分数s(q,d)，所有这些分数组成一个集合R，并且按照相关性从高到低排名（索引）。 跟传统RAG一样本研究中重点关注页面级检索：给定一个query，系统能否检索出正确的页面？本研究中的term==document。另外，本研究还对评分系统的延迟设置了限制要求。（这主要是为了模拟实际工业场景的需求）大部分当前的检索系统可以分解为（1）离线索引构建：一个document一个index（2）在线查询阶段：从离线索引库中低延迟的迅速匹配一个query对应的document一个好的文档检索系统需要3R特点：R1（retrieval performance）：高检索性能R2（low latency）：检索时低性能R3（high throughput）：索引期间高吞吐量2. 文本检索方法检索策略：BM25、密集检索（Bi-Encoder、Cross-Encoder）、混合检索、重排序本文只讨论密集检索 双编码器（Bi-Encoder/Dual Encoder）： 文档在离线阶段被转换成密集向量。 查询在线转换成向量后，使用快速的余弦相似度计算来找到最接近的文档向量。 这种方法计算速度快，适合处理大规模数据集。 交叉编码器（Cross-Encoder）： 查询和文档被组合成单一序列，然后一起编码。 对于每一对查询和文档，都会计算它们之间的匹配分数，允许更精细的上下文理解。 虽然这种方法稍微更精确，但由于需要对每个查询文档对都进行编码，因此计算成本更高，效率较低。 延迟交互检索 双编码器：查询和文档分别编码后直接进行相似度计算。 交叉编码器：查询和文档组合成一个序列，一起编码，然后进行匹配。 延迟交互编码：查询和文档分别编码后，通过后期交互机制（如MaxSim）进行更复杂的相似度计算。\\(\\text{Score} = \\text{MaxSim}(\\text{Attention}(\\mathbf{q} \\oplus \\mathbf{d}))\\) 在工业界使用这些模型时，很多性能上的提升并不是来自于模型本身变得更好，而是因为前期的数据准备做得更加充分。换句话说，如果喂给模型的数据质量更高、更合适，那么即使模型没有变化，它的表现也会更好。此外，当人们阅读文档时，往往依赖于其中的图片、表格、图表等视觉元素来更快地理解信息。但是，大多数基于文本的系统（比如搜索引擎或聊天机器人）在处理信息时，只能看到纯文本内容，而忽略了那些对人类读者非常有帮助的视觉线索。综上，虽然我们有各种方法来评估文本嵌入模型，但真正影响它们在实际应用中表现的关键因素之一是前期如何准备和处理数据。同时，现有的文本系统还没有充分利用到文档中丰富的视觉信息，而这部分信息对于提高理解和交互效果是非常重要的。这是第一个基准——通过考虑像人类这样的文本和视觉文档特征来评估文档检索方法。3. 整合视觉特征 基于对比学习来训练理解文本和图片之间关联的模型 一个编码器理解文本，另一个图像编码器专门理解图片，通过改写损失函数（对比损失contrastive losses）来联合训练。 用OCR执行光学字符识别 可以识别图像中的文本，但视觉部分通常不是专门为理解文本设计的。这意味着，尽管模型可以识别出图像中的单词或句子，但它可能不会像专门的文本理解系统那样深入地理解这些文本的意义。 基于细粒度交互的预训练模型 为了解决上述问题，Yao 等人（2021 年）提出了细粒度交互语言-图像预训练（Finegrained Interactive Language-Image Pre-training, FILIP）框架。具体来说，它依赖于最大相似度操作（max similarity operations），即计算文本中的每个词（token）与图像中的小块区域（patch）之间的最大相似度得分。模型可以更细致地捕捉文本和图像之间的对应关系，从而更好地理解两者之间的联系。 视觉丰富的文档理解 将LLMs与ViTs（视觉转换模型）相结合，创造了VLMs（视觉-语言模型）。 ViTs：首先将图像分割成多个小块（补丁、patches），ViTs模型将每个补丁转换成向量（即图像补丁向量）。 ViTs模型的训练方法是基于对比学习来训练模型理解图像中不同部分之间的关系和区别。 ViTs模型得到的图像的向量和LLMs得到的文本向量结合起来。 PaliGemma ​\tPaliGemma-3B 是一种视觉-语言模型（VLM），该模型使用的ViTs模型是SigLIP-So400m/14，它可以生成高质量的图像嵌入。该模型使用的LLMs模型是Gemma-2B。一个特别有趣的特性是，PaliGemma-3B 的文本模型是通过带有前缀（指令文本和图像标记）的全块注意力机制进行微调的。VLMs在视觉问答、字幕理解方面表现优秀，但是没有针对检索任务的优化。基于视觉检索的延迟交互1 整体模型架构1.1 VLMs：① 核心创新点——模态对齐：通过训练，模型能够将不同模态的数据（如文本和图像）映射到一个共享的向量空间中，在这个空间中它们可以被比较或组合。② ColPeli 能够生成类似 ColBERT（Khattab 和 Zaharia, 2020）风格的多向量表示。ColBERT 是一种用于信息检索的方法，它不是简单地将整个文档编码成单个向量，而是为文档中的每个词或短语生成独立的向量表示。这种方法有助于提高检索精度，因为它可以捕捉查询与文档之间的细粒度匹配。③ 额外的投影层：为了保持轻量级的嵌入包（bag-of-embedding）表示，ColPali 添加了一个投影层，该层将输出的语言建模嵌入映射到一个维度 D = 128 的向量空间。1.2 延迟交互：\\[LI(q, d) = \\sum_{i \\in [|1,N_q|]} \\max_{j \\in [|1, N_d|]} \\langle E_q^{(i)} | E_d^{(j)} \\rangle\\]$E_q^{(i)}$ ：query中第i个词/短语的嵌入向量$E_d^{(j)}$ ：document中第j个词/短语的嵌入向量$N_d$：文档中的词或短语总数$N_q$：query中的词或短语总数 $\\max_{j \\in [ 1, N_d ]} \\langle E_q^{(i)} E_d^{(j)} \\rangle$：对于query中的第i个词的嵌入向量$E_q^{(i)}$，逐个计算它与文档中所有词的嵌入向量的点积，找到最大的那个点积值以及对应的（文档中的）词。 对当前query的所有词的最大点积值求和，得到当前query和当前document的$LI(q, d)$。这个公式意味着，对于查询中的每一个词或短语，我们寻找文档中最相关的词或短语，并将这些最相关部分的相似度得分加起来。这样的方法能够捕捉到查询和文档之间更细微的匹配关系，尤其是当文档较长且包含丰富信息时，可以更好地识别出文档中与查询最相关的部分。1.3 对比学习损失 设有b个query-page对，称为${q_k,d_k}_{k \\in [ 1,b ]}$ 定义对比学习损失为​\t正样本得分$s_k^+$：\\(s_k^+ = LI(q_k,d_k)\\)​\t负样本得分$s_k^-$：\\(s_k^- = \\max\\limits_{l,l \\neq k} LI(q_k,d_l)\\)2 模型训练2.1 数据集训练数据：127460条query-page 对2.2 参数设置epoch：1LoRA：$\\alpha=32$，$r=32$学习率：5e-5线性衰减：2.5%warmup steps：32实验结果横轴：疑似是数据集纵轴：是不同方法的对比，包括 非结构化模型（仅针对文本） 非结构化模型（使用了OCR模块） 非结构化模型（使用了继承步骤）以上三种模型的检索器中使用的嵌入模型是BM25/BGE-M3。 对比学习视觉语言模型此类模型的检索器中使用的嵌入模型是Jina-CLIP/Nomic-vision/SigLIP（原生） 本文模型此类模型的检索器中使用的嵌入模型是SigLIP（原生）/BiSigLIP（微调）/BiPali（LLM）/ColPali（延迟交互）" }, { "title": "多模态rag", "url": "/posts/%E5%A4%9A%E6%A8%A1%E6%80%81RAG/", "categories": "", "tags": "", "date": "2024-12-20 00:00:00 +0800", "snippet": "多模态RAG调研零、为什么研究RAG🌱为什么要研究RAG模型？ Recently, there have been impressive advancements in large language models (LLMs) like ChatGPT (OpenAI 2022), LLaMA-2 (Touvron et al. 2023), and ChatGLM (THUDM 2023a). Although these models have shown remarkable general abilities (Bang et al. 2023; Guo et al. 2023), they still suffer severely from challenges including factual hallucination (Cao et al. 2020; Raunak, Menezes, and JunczysDowmunt 2021; Ji et al. 2023), knowledge out-dating (He, Zhang, and Roth 2022), and the lack of domain-specific expertise (Li et al. 2023c; Shen et al. 2023).ChatGPT、LLaMA-2、ChatGLM等大模型虽然有优秀的通用能力，但是存在一些问题，①factual hallucination事实幻觉；②knowledge out-dating知识过时；③domain-specific expertise特定领域的专业知识。 With the help of external knowledge, LLMs can generate more accurate and reliable responses. The most common method is to use a search engine as a retriever such as New Bing.LLMs+RAG的一个典型应用就是New Bing。 On the other hand, LLMs suffer from unreliable generation challenge. LLMs can be misled by incorrect information contained in the context (Bian et al. 2023) and also suffer from hallucination during the generation (Adlakha et al. 2023), resulting in generating content that goes beyond external information.RAG也会给LLMs带来负面影响，比如①互联网中存在虚假信息，②LLM可能会被上下文中包含的错误信息误导，③LLM在生成过程中存在幻觉，会生成超出外部信息的内容。 Unfortunately, currently there lacks of comprehensive understanding on how these factors can influence RAG, and how could each model survives from these drawbacks and improvement their performance via information retrieval.不幸的是，目前对这些因素如何影响RAG，以及每个模型如何从这些缺陷中幸存下来并通过信息检索提高其性能缺乏全面的了解。因此，迫切需要对LLM进行全面评估，评估其有效利用检索到的信息的能力，以及抵御信息检索中存在的各种缺点的能力。💡为了确保LLM的内部知识不会在评估结果中引入偏差，RGB选择聚合最新的新闻信息，并基于新闻信息构建查询。然后，基于这些查询，我们使用搜索API获取相关文档，并从内容中选择最相关的片段作为外部检索文档。最后，基于查询和文档集对的不同组成，我们扩展语料库，并将其划分为4个测试平台，根据RAG中常见的挑战来评估LLM的以下基本能力，如图1所示： 噪声鲁棒性Noise Robustness：“从嘈杂文档中提取有用信息”的能力。 本文中，我们将【嘈杂的文档】定义为“与问题相关的文档，但不包含任何答案信息。” 噪声鲁棒性平台： ​\t所有相关外部文档=噪声文档+包含答案信息的文档 ​\t噪声文档=噪声比*所有相关外部文档 负拒绝Negative Rejection：“拒绝回答无答案查询”的能力 无答案查询：所需知识不存在于任何检索到的文档中。此情况，LLM应给出“信息不足”或其他拒绝信号。 负拒绝平台： ​\t外部文档=噪声文档 信息整合Information Integration：“回答关联多个文档的复杂问题”的能力 信息整合平台： ​\t查询=只能用多个文档才能回答的实例 ​\t外部文档=多个包含答案信息的文档+噪声文档 反事实鲁棒性Counterfactual Robustness：“通过指令提示LLMs’警告：检索到的信息存在潜在风险‘时，能够识别检索到的文档中的已知事实错误”的能力 反事实稳健性平台： ​\tLLM已知的知识，即可以直接回答的query。 ​\t外部文档=存在事实错误的文档 请注意，我们只评估 LLM 通过指令对检索到的信息中潜在风险的警告的情况。 We found that even though RAG can improve the response accuracy of LLMs, they still suffer from the abovementioned challenges significantly. Specifically, we found that even though LLMs demonstrate some level of noise robustness, they tend to confuse similar information and frequently generate inaccurate answers when relevant information exists.尽管RAG可以提高llm的响应精度，但它们仍然受到上述挑战的显著影响。具体来说， 我们发现，尽管 LLM 展示了一定程度的噪声鲁棒性，但当相关信息存在时，它们往往会混淆相似的信息并经常生成不准确的答案。例如，当面对有关 2022 年诺贝尔文学奖的问题时，如果有关外部文件文献中 2021 年诺贝尔奖的嘈杂文档，LLM 可能会混淆并提供不准确的答案。 此外，当没有一个外部文档包含相关信息时，LLM 经常无法拒绝回答并生成不正确的答案。 此外，LLM 缺乏从多个文档中总结的能力，因此如果需要多个文档来回答问题，LLM 通常无法提供准确的答案。 最后，我们发现，即使llm包含所需的知识，并通过指令对检索到的信息中潜在风险的警告，它们仍然倾向于信任和优先考虑检索到的信息而不是他们自己的现有知识。一、什么是多模态RAG多模态大模型成功，让端到端（end-to-end）的算法已经成为了主流。原生多模态的RAG算法也成为了可能：既然多模态大模型有能力理解文本，那我们其实即无需再把图像转换为文本，而可以直接使用图像，提取embedding去做RAG。上图是我们之前研究的传统RAG模型。多模态RAG与传统RAG相比： 检索器分两个图像检索器&amp;文本检索器，返回的上下文也是既包含文本也包含图像。 ① 多模态索引构建：为文本、图像、音频等数据创建索引。文本可以使用词向量，图像可以使用卷积神经网络提取特征，音频可以用声学模型编码。 ② 检索策略：设计适合多模态数据的检索策略。例如，在图像检索中使用视觉特征比对，在文本检索中使用语义匹配。 ③ 结果融合：将多模态检索结果进行融合，形成一个综合的查询结果。 训练不同：传统RAG的LLM模型训练是先预训练后微调；多模态RAG用到的LLM模型需要对比学习训练，对比学习不同模态之间的关联性。 ① 多模态数据对齐：将不同模态的数据（如文本与图像）配对，以便模型学习它们之间的关联。 ② 编码器训练：训练编码器将不同模态的数据映射到共同的特征空间，便于相似性计算。 ③ 对比学习：通过对比损失函数，优化模型在不同模态之间的相似性学习。 微调方法不同：传统RAG是指令（文本）调优；多模态RAG是视觉指令调优（Visual Instruction Tuning）。 ① 图像描述生成：LLM解析图像内容生成文本描述。 ② 多模态问答：LLM根据输入的文本和图像或音频，回答相关问题。 整体流程不同： 传统RAG系统的实施流程通常为：检索相关文本上下文→结合LLM生成答案。 多模态RAG系统需要整合多模态数据，流程如下： 多模态数据检索：根据查询从不同模态数据源中检索相关信息。 多模态上下文融合：将检索到的多模态数据整合为统一的上下文。 答案生成：LLM基于整合后的多模态上下文生成答案。 例如，用户可以查询某产品的文本描述和图片，系统会检索相关的文本信息及图像数据，并将其融合到LLM生成的回答中。 二、行业应用前景传统RAG：文档管理、知识问答等多模态RAG： 流程图→代码 音视频流媒体→推荐/分析 流媒体内容管理：在音视频流媒体中，结合字幕文本和视频内容生成推荐或分析。 用户浏览过的产品图片→电商推荐 模态特征向量化：为每种模态数据生成特征向量。 相似性计算：在多个模态之间计算相似性。 推荐生成：根据多模态相似性生成推荐列表。 三、多模态RAG有哪些解决方案1. 多模态RAG图 (a) Multimodal RAG with Multimodal Embeddings and Separate Vector Stores 的分析 PDF Collection: 输入是PDF文档集合。 Image Retrieval和Text Retrieval: Extracted Images: 从PDF中提取图像。 Multimodal Embeddings: 将提取的图像转换为多模态嵌入，并存储在向量数据库中。 Extracted Texts: 从PDF中提取文本。 Text Embeddings: 将提取的文本转换为文本嵌入，并存储在向量数据库中。 Answer Synthesis: 用户提问后，通过多模态LLM（Language Model）合成答案。图 (b) Multimodal RAG with Image Summaries and Combined Vector Store 的架构 PDF Collection: 输入是PDF文档集合。 Image Retrieval和Text Retrieval: Extracted Images: 从PDF中提取图像。 Multimodal LLM: 使用多模态LLM生成图像摘要。 Image Summaries: 图像摘要被转换为文本嵌入。 Text Embeddings: 提取的文本直接转换为文本嵌入。 Multi-Vector Retriever: 使用一个联合的向量检索器来检索图像和文本。 Answer Synthesis: 用户提问后，通过多模态LLM合成答案。 https://mp.weixin.qq.com/s/n9LmqIOEtY6JX-fVDk20yg2. 延迟交互编码—ColBERT核心思想：延迟交互编码的核心思想是将查询和文档的编码过程分开，这样文档的编码可以离线完成，而查询的编码则在在线阶段进行。优点：1.在查询时仅对查询进行编码，从而大大提高了处理速度。2.由于文档的编码是预先完成的，可以将其存储在数据库中，这样就可以对更多的文档进行排序，从而提高查询的精度。具体做法：在延迟交互编码中，一个关键的计算是最大相似性（MaxSim）函数，它计算每个查询Token向量与所有文档Token向量之间的相似度，并跟踪每个查询Token的最大得分。查询和文档的总相似度分数是这些最大余弦相似度分数的总和。这种方法允许模型在保持较高排序质量的同时，也具备较高的性能。目前的发展前景：随着多模态大语言模型能力的增强，以它为基础的多模态RAG，也早已突破了传统的图像检索的应用方式，而是真正具备大规模非结构化多模态数据深度理解的能力，将会有更多toB的商业应用价值。3. ColPali该方法利用了Google团队的视觉语言模型 (VLM) PaliGemma，它将整个文档页面编码为嵌入向量，将页面布局和视觉元素视为检索过程的一部分。ColPali RAG使用受ColBERT（Column BERT）启发的后期交互机制，通过启用用户查询和文档patches之间的token-level匹配来增强检索。这种方法确保了高检索准确性，同时还保持了合理的索引和查询速度。它对于富含视觉效果的文档特别有益，例如信息图表、表格和复杂布局，而传统的基于文本的检索方法很难处理这些文档。上面是传统的建索引和检索过程，可以看出要进行一系列的pdf parse过程。最后得到文本chunking，然后送入文本向量模型中，得到向量；下面是ColPali的建索引和检索过程，直接输入整个页面的截图进入Vision LLM中，得到多个向量。核心思想：与 ColBERT 相比，ColPali 仍然使用文本作为查询，文档则是图像类型。在视觉encoder，也是利用多模态的视觉大模型来生成图片端的向量，但不仅仅只生成单个向量。而是利用VIT的patch embedding，来生成多个向量。直觉上确实是会有收益，因为一整页的pdf，只压缩在一个固定维度的向量中，那肯定有信息损失，而且以patch为单位生成embedding。优点：直接对文档用视觉模型解析。具体做法：ColPali选择PaliGemma-3B作为其视觉语言模型，为了生成轻量级的多向量表示，ColPali在PaliGemma-3B模型的基础上添加了一个投影层，将输出的语言建模嵌入映射到一个降低维度的向量空间中（D=128），（这与ColBERT论文中使用的向量空间大小相同）。ColPali采用了和ColBERT 类似的后期交互机制，通过这种方式，ColPali能够在检索时充分利用查询和文档之间的交互，同时保持了离线计算和快速查询匹配的优势。在ColPali模型中，对比损失用于优化检索任务，使得模型能够学习区分与查询相关的文档和不相关的文档。 PaliGemma-3B作为其视觉语言模型，这是一个相对较小的模型，具有多个针对不同图像分辨率和任务微调的检查点，并且在各种文档理解基准测试中表现出色。PaliGemma-3B的一个关键特性是其文本模型在前缀（指令文本和图像标记）上进行了全块注意力的微调。 在ColPali模型的训练过程中，每个批次包含多个查询-文档对。对于每对查询$q_k$和其对应的正样本文档$d_k$，模型会计算一个正样本分数$s_k^+$，这是通过后期交互操作$LI(q_k,d_k)$得到的。同时，模型还会计算一个负样本分数$s_k^-$，这是通过在批次中所有其他文档（即负样本）上执行晚期交互操作，并取最大值得到的。\\(L =\\frac{1}{b}\\sum^{b}_{k=1}softplus(s_k^--s_k^+)\\)其中： b是批次中查询-文档对的数量。 是查询$q_k$与其对应的正样本文档dk之间的正样本分数。 是查询$q_k$与所有负样本文档中的最大分数。 实验效果：横轴： ArxivQ: Arxiv问答任务的表现。 DocQ: 文档问答任务的表现。 InfoQ: 信息检索问答任务的表现。 TabF: 表格填充任务的表现。 TATQ: 图像文本问答任务的表现。 Shift: 数据集偏移适应能力。 AI: AI相关问题的回答能力。 Energy: 能源相关问题的回答能力。 Gov.: 政府相关问题的回答能力。 Health.: 健康相关问题的回答能力。 Avg.: 所有任务的平均表现。纵轴：是不同方法的对比，包括 非结构化模型（仅针对文本） 非结构化模型（使用了OCR模块） 非结构化模型（结合图像描述生成技术处理非结构化文本和图像信息。）以上三种模型的检索器中使用的嵌入模型是BM25/BGE-M3。对比学习视觉语言模型​\t此类模型的检索器中使用的嵌入模型是Jina-CLIP/Nomic-vision/SigLIP（原生）本文模型​\t此类模型的检索器中使用的嵌入模型是SigLIP（原生）/BiSigLIP（微调）/BiPali（LLM）/ColPali（延迟交互）实验结果分析 BM25 和 BGE-M3 在只使用文本信息时表现一般。 结合OCR和Captioning技术 后，模型在多个任务上的表现都有显著提升。 对比学习的视觉语言模型 在某些任务上表现不佳，但在特定任务上有一定优势。 提出的模型（如ColPali）在所有任务上的表现都优于基线模型，特别是在AI、能源、政府和健康相关问题的回答上表现尤为突出。Reference：​\tColPali: Efficient Document Retrieval with Vision Language Models​\thttps://github.com/illuin-tech/colpali?tab=readme-ov-file​\tColPali论文: https://arxiv.org/abs/2407.01449​\tColPali博客: https://huggingface.co/blog/manu/colpali​\tColPali实战: https://github.com/weaviate/recipes/blob/main/weaviate-features/named-vectors/NamedVectors-ColPali-POC.ipynb4. 混合ColPali RAG混合ColPali RAG 结合了图像嵌入和ColPali的后期交互机制的优势，进一步提高了检索性能。 系统首先使用图像嵌入（例如来自 JinaCLIP 等模型）执行粗略检索步骤，以检索前 k 个相关文档页面。 然后，在第二遍中，系统使用 ColPali 后期交互机制对这 k 个页面重新排序，以根据视觉和文本信息确定最终最相关的页面集。当文档包含复杂的视觉效果和详细的文本时，这种混合方法特别有用，允许系统利用这两种内容类型进行高度准确的文档检索。四、如何评估多模态RAG模型1.端到端评估—EvalScope将用户输入、检索上下文、模型输出和标准答案(可选)这四部分输入给法官大模型，模型将根据给定的指标评估整个RAG生成的效果。EvalScope支持RAG和多模态RAG的独立评估和端到端评估： 端到端评估：评估RAG模型对给定输入生成的输出内容，包括模型生成答案与输入查询的相关性和对齐程度。从内容生成目标视角来评估可以将评估划分为无参考答案和有参考答案：无参考答案评估指标包括上下文相关性(Context Relevance)、忠实度(Faithfulness) 等；而有参考答案评估指标包括正确性(Answer Correctness)、BLEU、ROUGE等。 EvalEcope基于Ragas忠实度衡量模型输出与检索上下文中的事实一致性。如果答案中所有的陈述都可以从检索上下文中推断出来，则认为生成的答案是忠实的。答案的得分范围为 (0,1)，得分越高表示忠实度越好。示例：用户输入: 特斯拉 Model X 怎么样？检索上下文:一张特斯拉 Model X 的图片高忠实度输出: 特斯拉 Model X 是一款由特斯拉制造的电动SUV。低忠实度输出: 猫很可爱。解释：”猫很可爱” 无法从特斯拉 Model X 的图片中推断出来，因此忠实度得分为 0。相关度标衡量模型输出与检索上下文以及用户输入的相关性。答案的得分范围为 (0,1)，得分越高表示相关性越好。示例：用户输入: 这幅画是谁画的？检索上下文: 一幅毕加索的画 高相关性输出: 这幅画是毕加索画的。低相关性输出: 这是一幅美丽的画。解释：”这是一幅美丽的画” 虽然描述了画的特性，但没有回答问题，因此相关性得分为 0。正确性评估涉及将模型输出与标准答案进行比对，以衡量其准确性。评分范围从0到1，分数越高，表示生成的回答与标准答案的匹配度越高，正确性更好。该指标不涉及图像模态。示例：用户输入: 爱因斯坦什么时候出生？标准答案：爱因斯坦于1879年在德国出生。高正确性回答：爱因斯坦于1879年在德国出生。低正确性回答：爱因斯坦于1879年在西班牙出生。解释：将标准答案和回答拆分成事实子句，并使用以下概念进行计算： TP（True Positive）：在标准答案和生成的回答中都存在的事实或陈述。 FP（False Positive）：存在于生成的回答中但不存在于标准答案中的事实或陈述。 FN（False Negative）：存在于标准答案中但不存在于生成的回答中的事实或陈述。在该例子中： TP: [爱因斯坦于1879年出生] FP: [爱因斯坦于西班牙出生] FN: [爱因斯坦于德国出生]计算公式：\\(F1 score = \\frac{|TP|}{(|TP|+{0.5}\\times{(|TP|+|FN|)})}\\)因此，低正确性回答的F1 score为0.5，而高正确性回答的F1为1。 更多常见评估指标可参考EvalScope使用指南:https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/ragas.html#id52. 独立评估—CLIP Benchmark也是EvalScope所支持的：https://evalscope.readthedocs.io/zh-cn/latest/user_guides/backend/rageval_backend/clip_benchmark.html#clip-benchmark，提供自定义图文检索评估支持。 独立评估：单独评估检索模块，评估指标包括指标包括 命中率(Hit Rate)、平均排名倒数(Mean Reciprocal Rank, MRR)、归一化折扣累积增益(Normalized Discounted Cumulative Gain, NDCG)、准确率(Precision) 等，这些指标用于测量系统在根据查询或任务排名项目方面的有效性。五、研究思路（v1.0）在已有的训练好的多模态RAG模型基础上使用微调技术得到新模型，然后使用EvalScope评测。Baseline：因此，首先需要先跑通ColPali的VLMs模型组合（即Qwen等Base模型作为LLM模型、ViTs模型作为视觉模型）在EvalScope上的实验。Experiment1：提出创新点，实现代码。" }, { "title": "深度学习 第一课", "url": "/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%AC%AC%E4%B8%80%E8%AF%BE/", "categories": "", "tags": "", "date": "2024-12-16 00:00:00 +0800", "snippet": "Course 1：Part 1：人工智能基础常识什么是监督学习？监督学习需要有明确的目标，很清楚自己想要什么结果。比如：按照“既定规则”来分类、预测某个具体的值…监督并不是指人站在机器旁边看机器做的对不对，而是下面的流程： 选择一个适合目标任务的数学模型 先把一部分已知的“问题和答案”（训练集）给机器去学习 机器总结出了自己的“方法论” 人类把”新的问题”（测试集）给机器，让他去解答 上面提到的问题和答案只是一个比喻，假如我们想要完成文章分类的任务，则是下面的方式： 选择一个合适的数学模型 把一堆已经分好类的文章和他们的分类给机器 机器学会了分类的“方法论” 机器学会后，再丢给他一些新的文章（不带分类），让机器预测这些文章的分类常见的监督学习算法分类算法是一类监督学习算法监督学习分为两大类任务​\t回归：预测连续的、具体的数值。​\t分类：对各种事物分门别类，用于离散型预测。 连续： 当我们说某个量是连续的，意味着它可以在一定范围内取任意值。 例如，人的身高、物体的重量或时间都是连续变量的例子，因为它们可以取无限多的值，并且两个值之间的差异可以非常小。 在数学上，如果一个变量可以在实数线上自由变动，那么这个变量就是连续的。 离散： 相对地，离散指的是那些只能取特定、分离值的数据类型。离散变量具有明确的、可数的不同状态或类别。 比如骰子的结果（1到6）、硬币投掷的结果（正面或反面），或者一个人的血型（A, B, AB, O）。什么是无监督学习？无监督学习是一种机器学习的训练方式，它本质上是一个统计手段，在没有标签的数据里可以发现潜在的一些结构的一种训练方式。它主要具备3个特点： 无监督学习没有明确的目的 无监督学习不需要给数据打标签 无监督学习无法量化效果聚类算法是一类无监督学习算法分类算法需要事先知道“类别”个数，如果“类别”个数无从得知，如何给一堆数据分类呢？聚类：简单说就是一种自动分类的方法，在监督学习中，你很清楚每一个分类是什么，但是聚类则不是，你并不清楚聚类后的几个分类每个代表什么意思。常见的无监督学习算法什么是深度学习？神经网络≈深度学习深度学习的概念源于人工神经网络的研究，但是并不完全等于传统神经网络。不过在叫法上，很多深度学习算法中都会包含”神经网络”这个词，比如：卷积神经网络、循环神经网络。模拟人的大脑，造出可以思考的机器。人为什么能够思考？原因在于人体的神经网络。 外部刺激通过神经末梢，转化为电信号，转导到神经细胞（又叫神经元）。 无数神经元构成神经中枢。 神经中枢综合各种信号，做出判断。 人体根据神经中枢的指令，对外部刺激做出反应。 既然思考的基础是神经元，如果能够”人造神经元”（artificial neuron），就能组成人工神经网络，模拟思考。上个世纪六十年代，提出了最早的”人造神经元”模型，叫做“感知器”（perceptron），直到今天还在用。上图的圆圈就代表一个感知器。它接受多个输入（x1，x2，x3…），产生一个输出（output），好比神经末梢感受各种外部环境的变化，最后产生电信号。举个例子：城里正在举办一年一度的游戏动漫展览，小明拿不定主意，周末要不要去参观。他决定考虑三个因素： 天气：周末是否晴天？ 同伴：能否找到人一起去？ 价格：门票是否可承受？ 这就构成一个感知器。上面三个因素就是外部输入，最后的决定就是感知器的输出。如果三个因素都是 Yes（使用1表示），输出就是1（去参观）；如果都是 No（使用0表示），输出就是0（不去参观）。如果某些因素成立，另一些因素不成立，输出是什么？比如，周末是好天气，门票也不贵，但是小明找不到同伴，他还要不要去参观呢？现实中，各种因素很少具有同等重要性：某些因素是决定性因素，另一些因素是次要因素。因此，可以给这些因素指定权重（weight），代表它们不同的重要性。 天气：权重为8 同伴：权重为4 价格：权重为4 上面的权重表示，天气是决定性因素，同伴和价格都是次要因素。如果三个因素都为1，它们乘以权重的总和就是 8 + 4 + 4 = 16。如果天气和价格因素为1，同伴因素为0，总和就变为 8 + 0 + 4 = 12。这时，还需要指定一个阈值（threshold）。如果总和大于阈值，感知器输出1，否则输出0。假定阈值为8，那么 12 &gt; 8，小明决定去参观。阈值的高低代表了意愿的强烈，阈值越低就表示越想去，越高就越不想去。上面的决策过程，使用数学表达如下。上面公式中，x表示各种外部因素，w表示对应的权重。真实世界中，实际的决策模型则要复杂得多，是由多个感知器组成的多层网络。Part 2：如何训练一个人工智能模型2.0 神经网络的运作过程一个神经网络的搭建，需要满足三个条件。 输入和输出 权重（w）和阈值（b） 多层感知器的结构 也就是说需要先构建上面那个图的结构，其中，最困难的部分就是确定权重（w）和阈值（b）。目前为止，这两个值都是主观给出的，但现实中很难估计它们的值，必需有一种方法，可以自动计算出它们。:star: 试错法：先随机初始化它们的值，控制变量法保持其他参数都不变，w（或b）的微小变动，记作Δw（或Δb），然后观察输出有什么变化。不断重复这个过程，直至得到对应最精确输出的那组w和b，就是我们要的值。这个过程称为模型的训练。神经网络的运作过程： 确定输入和输出 找到一种或多种算法，可以从输入得到输出 找到一组已知答案的数据集，用来训练模型，估算w和b 一旦新的数据产生，输入模型，就可以得到结果，同时对w和b进行校正数据集→训练阶段→测试阶段2.1 实例-手写数字识别 灰度图像：其每一个像素值的范围是0~255（由纯黑色到纯白色），表示其颜色强弱程度。 黑白图像：每个像素值要么是0（表示纯黑色），要么是255（表示纯白色）。 RGB图像：有三个通道，分别是红色、绿色、蓝色。每个通道的每个像素值的范围也是0~255，表示其每个像素的颜色强弱。 通常处理的基本都是灰度图像，因为比较好操作（值范围较小，颜色较单一），有些RGB图像在输入给神经网络之前也被转化为灰度图像，也是为了方便计算，否则三个通道的像素一起处理计算量非常大。第一层-输入层：将图像转换为其对应的由像素值构成的二维矩阵第二层-卷积层：用来提取图像的底层特征卷积（convolution）是CNN的核心操作，它是一种数学操作，用于将一张图像与另一张滤波器/卷积核（kernel）进行乘积运算，从而生成一张新的图像。滤波器是一种小尺寸的矩阵，通常用于提取图像中的特定特征，每个卷积核是独立学习的，它们可以捕捉到输入图像的不同特征。。卷积操作可以帮助提取图像中的有用信息，同时减少噪声和不必要的细节。 卷积核个数=卷积层生成的特征图个数 较小的内核（如3x3或5x5）通常能够捕捉更精细的特征，而较大的内核可能会捕捉到更广泛但更模糊的模式。model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(28, 28, 3) \t\t) ) #添加卷积层 padding='same': 这个参数控制了如何处理输入的边界。当你使用 'same' 时，它会在输入的边缘添加适当的零填充，以确保输出特征图的尺寸与输入尺寸相同（不考虑步幅的影响）。如果使用 'valid'，则不会进行填充，输出尺寸将会减小。 activation='relu': 激活函数应用于卷积操作后的输出。ReLU（Rectified Linear Unit）函数是深度学习中最常用的激活函数之一，它的公式是 f(x)=max⁡(0,x)f(x)=max(0,x)，它可以引入非线性，帮助模型学习复杂模式并缓解梯度消失问题。 input_shape=(28, 28, 3): 这个参数定义了输入到该层的数据的形状。对于二维卷积层来说，它应该是一个三元组 (height, width, channels)。在这个例子中，28, 28 是输入图像的高度和宽度，而 3 表示颜色通道的数量，即这是一个RGB图像（红、绿、蓝三个通道）。这是网络中第一个卷积层特有的参数，因为后续的卷积层可以从前一层自动推断输入形状。第三层-池化层：防止过拟合，将数据维度减小第四层-全连接层：汇总卷积层和池化层得到的图像的底层特征和信息将前一层的多为矩阵展开为一维的向量。第五层-输出层：根据全连接层的信息得到概率最大的结果通常也是一层全连接层。model = tf.keras.Sequential( ) # 创建sequential对象model model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(28, 28, 3) \t\t) ) #添加卷积层1 model.add(tf.keras.layers.BatchNormalization()) #添加BN层1 model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu' ) )#添加卷积层2 model.add(tf.keras.layers.BatchNormalization( )) #添加BN层2 model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu') ) #添加卷积层3 model.add(tf.keras.layers.BatchNormalization()) #添加BN层3 model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2) ) ) #添加最大池化层1 model.add(tf.keras.layers.Flatten()) #添加Flatten层 model.add(tf.keras.layers.Dense(256,activation='relu')) #添加全连接层1model.add(tf.keras.layers.Dropout(0.5))#添加Dropoutmodel.add(tf.keras.layers.Dense(50, activation='softmax')) #添加输出层2.2 神经网络的补充知识① BatchNormalization()层② Flatten层③ Dropout层④ 激活函数⑤ 优化器⑥ 损失函数⑦ 学习率2.3 图像分类任务的模型调参技巧2.4 数据增强方法2.4.1 图像数据 几何变换： 旋转（Rotation） 平移（Translation） 缩放（Scaling） 翻转（Flipping）：水平或垂直 剪切（Shearing） 色彩变换： 调整亮度（Brightness） 改变对比度（Contrast） 色调调整（Hue） 饱和度调整（Saturation） 噪声添加： 添加高斯噪声 盐椒噪声 模糊和锐化： 高斯模糊（Gaussian Blur） 中值模糊（Median Blur） 裁剪与填充（Cropping and Padding） 随机擦除（Random Erasing） 透视变换（Perspective Transform） 混合（Mixup）：将两个图像及其标签按一定比例线性组合成新的训练样本。 Cutmix：从一个图像中切割出一块区域，并用另一个随机选择的图像的相应块替换它。 Mosaic Augmentation：将四个不同的图像拼接成一个新的图像作为输入。2.4.2 文本数据 同义词替换（Synonym Replacement, SR）： 在句子中随机选择一些单词，并用它们的同义词替换。 随机插入（Random Insertion, RI）： 在句子中随机位置插入一个随机选择的单词的同义词。 随机交换（Random Swap, RS）： 随机交换句子中的两个单词的位置。 随机删除（Random Deletion, RD）： 随机删除句子中的某些单词。 翻译后回译（Back Translation）： 将文本翻译成另一种语言，再翻译回来以生成变体。 文本混洗（Text Shuffling）： 打乱文档或段落的顺序，对于不依赖严格顺序的任务有效。 模板应用（Template Application）： 使用预定义的句子结构模板生成新句子。 字符级扰动（Character-level Perturbation）： 包括字符替换、插入、删除或交换，模仿打字错误等。 使用上下文感知的语言模型（Context-aware Language Models）： 利用如BERT这样的预训练模型来生成相似但有差异的句子。 文本拼接（Concatenation）： 将多个文本片段合并为一个更长的文本。 " }, { "title": "RAG", "url": "/posts/RAG/", "categories": "NLP, 基础知识", "tags": "nlp, RAG, 技术综述", "date": "2024-02-29 00:00:00 +0800", "snippet": "RAG调研一、一些共识 External knowledge is the key to resolving the problems of LLMs such as hallucination and outdated knowledge, which can make LLMs generate more accurate and reliable responses through retrieval-augmented generation (RAG). However, LLMs cannot always response as expected with RAG. For one thing, there are numerous irrelevant documents and false information on the Internet. Incorporating these external documents into LLMs could have a detrimental effect. For anthoer, LLMs suffer from the unreliable generation challenge. The generation of LLMs is often unpredictable, and we cannot guarantee that they will utilize the useful information entailed in the external documents. Additionally, LLMs can easily be misled by incorrect information in the document. To this end, we build RetrievalAugmented Generation Benchmark (RGB) to evaluate the retrieval-augmented generation of LLMs, and we concern about 4 specific abilities:[^Benchmarking Large Language Models in Retrieval-Augmented Generation]外部知识是解决幻觉和过时知识等llm问题的关键，它可以通过检索增强生成(RAG)使llm产生更准确、更可靠的响应。然而，⭐LLM 并不总是像 RAG 预期的那样响应。⭐将这些不相关的或者虚假的外部文档合并到 LLM 中可能会产生不利影响。⭐llm的生成通常是不可预测的，我们不能保证它们将利用外部文档中包含的有用信息。⭐LLM 很容易被文档中不正确的信息误导。 In real-world scenarios, it is not possible to obtain perfect documents with all the necessary external knowledge. Therefore, evaluating these four abilities of the model becomes essential in order to measure the RAG of LLMs.[^Benchmarking Large Language Models in Retrieval-Augmented Generation]⭐在现实世界的场景中，不可能获得具有所有必要的外部知识的完美文档。因此需要能够评估模型的这四个能力。二、论文调研Benchmarking Large Language Models in Retrieval-Augmented Generation[^Benchmarking Large Language Models in Retrieval-Augmented Generation]---中科院计算所，23年9月4日摘要：检索增强生成 (RAG) 是一种减轻大型语言模型 (LLM) 幻觉的有前途的方法。然而，现有的研究缺乏对检索增强生成对不同大型语言模型的影响的严格评估，这使得识别RAG对不同llm能力的潜在瓶颈具有挑战性。在本文中，我们系统地调查了 Retrieval-Augmented Generation 对大型语言模型的影响。我们分析了不同大型语言模型在RAG所需的4个基本能力下的性能，包括噪声鲁棒性、负拒绝、信息集成和反事实鲁棒性。为此，我们建立了 Retrieval-Augmented Generation Benchmark (RGB)，这是一个用于英文和中文 RAG 评估的新语料库。RGB 根据解决案例所需的上述基本能力，将基准内的实例划分为 4 个单独的测试平台。然后我们在 RGB 上评估 6 个具有代表性的 LLM，以诊断应用 RAG 时当前 LLM 的挑战。评估表明，虽然 LLM 表现出一定程度的噪声鲁棒性，但它们在负拒绝、信息集成和处理虚假信息方面仍然会遇到重大困难。上述评估结果表明，在有效地将RAG应用于LLM之前，仍有相当大的旅程。Introduction：🌱为什么要研究RAG模型？ Recently, there have been impressive advancements in large language models (LLMs) like ChatGPT (OpenAI 2022), LLaMA-2 (Touvron et al. 2023), and ChatGLM (THUDM 2023a). Although these models have shown remarkable general abilities (Bang et al. 2023; Guo et al. 2023), they still suffer severely from challenges including factual hallucination (Cao et al. 2020; Raunak, Menezes, and JunczysDowmunt 2021; Ji et al. 2023), knowledge out-dating (He, Zhang, and Roth 2022), and the lack of domain-specific expertise (Li et al. 2023c; Shen et al. 2023).ChatGPT、LLaMA-2、ChatGLM等大模型虽然有优秀的通用能力，但是存在一些问题，①factual hallucination事实幻觉；②knowledge out-dating知识过时；③domain-specific expertise特定领域的专业知识。 With the help of external knowledge, LLMs can generate more accurate and reliable responses. The most common method is to use a search engine as a retriever such as New Bing.LLMs+RAG的一个典型应用就是New Bing。 On the other hand, LLMs suffer from unreliable generation challenge. LLMs can be misled by incorrect information contained in the context (Bian et al. 2023) and also suffer from hallucination during the generation (Adlakha et al. 2023), resulting in generating content that goes beyond external information.RAG也会给LLMs带来负面影响，比如①互联网中存在虚假信息，②LLM可能会被上下文中包含的错误信息误导，③LLM在生成过程中存在幻觉，会生成超出外部信息的内容。 Unfortunately, currently there lacks of comprehensive understanding on how these factors can influence RAG, and how could each model survives from these drawbacks and improvement their performance via information retrieval.不幸的是，目前对这些因素如何影响RAG，以及每个模型如何从这些缺陷中幸存下来并通过信息检索提高其性能缺乏全面的了解。因此，迫切需要对LLM进行全面评估，评估其有效利用检索到的信息的能力，以及抵御信息检索中存在的各种缺点的能力。💡为了确保LLM的内部知识不会在评估结果中引入偏差，RGB选择聚合最新的新闻信息，并基于新闻信息构建查询。然后，基于这些查询，我们使用搜索API获取相关文档，并从内容中选择最相关的片段作为外部检索文档。最后，基于查询和文档集对的不同组成，我们扩展语料库，并将其划分为4个测试平台，根据RAG中常见的挑战来评估LLM的以下基本能力，如图1所示： 噪声鲁棒性Noise Robustness：“从嘈杂文档中提取有用信息”的能力。 本文中，我们将【嘈杂的文档】定义为“与问题相关的文档，但不包含任何答案信息。” 噪声鲁棒性平台： 所有相关外部文档=噪声文档+包含答案信息的文档\t噪声文档=噪声比*所有相关外部文档 负拒绝Negative Rejection：“拒绝回答无答案查询”的能力 无答案查询：所需知识不存在于任何检索到的文档中。此情况，LLM应给出“信息不足”或其他拒绝信号。 负拒绝平台： 外部文档=噪声文档 信息整合Information Integration：“回答关联多个文档的复杂问题”的能力 信息整合平台： 查询=只能用多个文档才能回答的实例\t外部文档=多个包含答案信息的文档+噪声文档 反事实鲁棒性Counterfactual Robustness：“通过指令提示LLMs’警告：检索到的信息存在潜在风险‘时，能够识别检索到的文档中的已知事实错误”的能力 反事实稳健性平台： LLM已知的知识，即可以直接回答的query。\t外部文档=存在事实错误的文档 请注意，我们只评估 LLM 通过指令对检索到的信息中潜在风险的警告的情况。 测评模型： ChatGPT ChatGLM-6B ChatGLM2-6B Vicuna-7b Qwen-7B-Chat BELLE-7B结论： We found that even though RAG can improve the response accuracy of LLMs, they still suffer from the abovementioned challenges significantly. Specifically, we found that even though LLMs demonstrate some level of noise robustness, they tend to confuse similar information and frequently generate inaccurate answers when relevant information exists.尽管RAG可以提高llm的响应精度，但它们仍然受到上述挑战的显著影响。具体来说， 我们发现，尽管 LLM 展示了一定程度的噪声鲁棒性，但当相关信息存在时，它们往往会混淆相似的信息并经常生成不准确的答案。例如，当面对有关 2022 年诺贝尔文学奖的问题时，如果有关外部文件文献中 2021 年诺贝尔奖的嘈杂文档，LLM 可能会混淆并提供不准确的答案。 此外，当没有一个外部文档包含相关信息时，LLM 经常无法拒绝回答并生成不正确的答案。 此外，LLM 缺乏从多个文档中总结的能力，因此如果需要多个文档来回答问题，LLM 通常无法提供准确的答案。 最后，我们发现，即使llm包含所需的知识，并通过指令对检索到的信息中潜在风险的警告，它们仍然倾向于信任和优先考虑检索到的信息而不是他们自己的现有知识。Retrival-Augmented Generation Benchmark数据集构建过程：News Collection–真实的Step 0：收集最新的新闻文章Step 1：QA实例生成（QA instances generation）：使用prompt让ChatGPT为每篇文章生成事件（Related event）、问题（Question）和答案（Key information）。这一步能够顺便初步过滤出不包含任何事件的新闻文章。Step 2：人工检查调整、过滤难以通过搜索引擎检索的数据。Step 3：使用搜索引擎API检索（Retrieve using search engine）：对于每个Query使用谷歌 API获取10个相关网页，并提取相应的文本片段。Step 4：阅读这些网页，并将其文本内容转换为最大长度为300个token的文本块。使用现有的密集检索模型选择与查询最有效匹配的前30个文本块。这些检索到的文本块，以及搜索API提供的片段，将用作我们的外部文档。Step 5：扩展语料库，为4种能力构建Testbeds（试验台）。对于噪声鲁棒性，根据所需的噪声比率对不同数量的负面文档进行采样。对于负样本拒绝，所有的外部分档都是从负文档中采样的。对于信息整合，基于前面生成的问题构造复杂问题。这涉及到扩展或重写这些问题，使它们的答案包含多个方面。例如，“谁获得了2023年超级碗的MVP？”这个问题可以改写为“谁赢得了2022年和2023年的超级碗MVP。因此，回答这样的问题需要利用来自各种文件的信息。对于反事实稳健性，反事实稳健性数据完全基于模型的内部知识构建，也就是说让模型自动生成已知的问题和答案，例如，基于“谁获得了 2022 年诺贝尔生理学和医学奖？”的问题，该模型将生成已知问题“谁获得了 2021 年诺贝尔文学奖？”并回答“Abdulrazak Gurnah”。然后人工验证生成的答案，并检索相关文档，为了使文档包含事实错误，我们人工修改答案并替换文档中的相应部分。 密集检索模型: for English：https://huggingface.co/sentence-transformers/all-mpnet-basev2 for Chinese：https://huggingface.co/moka-ai/m3e-baseRGB统计数据：共600个基本问题，200个信息整合能力的附加问题，200个反事实稳健性能力的附加问题。所有问题，一半中文，一半英文。评价指标：该基准的核心是评估 LLM 是否可以利用提供的外部文档来获取知识并生成合理的答案。 Accuracy：for 噪声鲁棒性、信息整合 精确匹配-如果生成的文本包含与答案的精确匹配，则视为回答正确。 Rejection rate：for 负样本拒绝 当只提供嘈杂的文档时，LLM应输出具体内容—— \"I can not answer the question because of the insufficient information in documents.\"（“由于文档中的信息不足，我无法回答问题。”）如果模型生成此内容，则表示成功拒绝。 \tPS.使用说明提示模型。 Error detection rate：for 反事实鲁棒性（衡量模型是否能检测出文档中的事实错误） 当提供的文档包含事实错误时，模型应该输出特定的内容-\"There are factual errors in the provided documents.\"（“提供的文档中存在事实错误”。）如果模型生成该内容，则表明模型在文档中检测到错误信息。 \tPS.使用说明提示模型。 Error correction rate：for 反事实鲁棒性（衡量模型是否能在识别到事实错误之后仍能提供正确的答案） 如果该模型生成正确的答案，则表明模型能够 修正 文档中的事实错误。 \\[ACC=\\frac{\\#tt}{\\#nums}\\] #tt：正确的response数量 #nums：所有待评估的❓实例数量 考虑到模型可能无法完全遵守指令，对于拒绝率和错误检测率，我们还使用ChatGPT对答案进行额外评估。具体而言，❓我们通过使用指导（instructions）和演示（demonstrations）来评估模型的响应，以确定它们是否能够反映文件中没有的信息或识别任何事实错误。ExperimentsSettings由于上下文限制，我们为每个问题提供5个外部文档。在我们关于噪声鲁棒性的实验中，我们评估了噪声比在0到0.8之间的场景。为了全面评估整体能力，我们对每种语言都采用了统一的指导（instructions），如图3所示。实验是使用NVIDIA GeForce RTX 3090进行的。include a system instruction followed by a user input instructionResults on Noise Robustness针对 噪声鲁棒性 实验：结论： RAG能有效改善LLMs的响应。 即使在存在噪声的情况下，llm也表现出了很强的性能，这表明RAG是llm产生准确可靠响应的一种有希望的方法。 噪声率的不断提高对llm中的RAG提出了挑战。 具体来说，当噪声比超过80%时，模型的精度明显下降。例如，ChatGPT的性能从96.33%下降到76.00%，而ChatGLM2-6B的性能从91.33%下降到57.33%。 深入分析这个实验中的模型答错的答案，发现错误通常源于3个原因：给出了噪声鲁棒性的误差案例，并且只给出了一个正面文档和一个负面文档。响应由ChatGLM2-6B生成。蓝色文本表示文档与问题或答案之间匹配的部分，而红色文本突出显示不匹配的部分。 长距离信息 当外部文档中“与问题相关的信息”与“与答案相关的信息”相距甚远时，模型难以识别正确答案。这种情况中，大部分是“与问题相关的信息”首先出现在文档的开头，在下文中使用代词指代它。这种情况出现时，模型会去依赖其他文档的信息，产生错误的印象（幻觉）。 如上图中卡塔尔公开赛只在开头出现了一次，与答案文本Anett Kontaveit人名相距甚远。 证据的不确定性 在备受关注的事件发生前，人们倾向于预测、猜测、预言它。尽管这样的预测文档明确指出这是不确定或推测性的内容，但它们仍然会影响LLM的检索增强生成。 如上图中，错误文档中的内容都是关于苹果新耳机的预测（Apple Reality Pro），并且存在有正确答案的文档（Vision Pro），模型仍然被误导了。 概念混淆 外部文档中的概念可能与问题中的概念相似，但又不同。这会让LLM混淆，并生成不正确的答案。 如上图中，模型对问题的理解集中在”汽车收入“的概念上（特斯拉，收入-&gt;通常提到特斯拉都是谈论特斯拉汽车-&gt;特斯拉，汽车收入），但问题问的是特斯拉这个公司一季度全部“收入”。 通过上述分析，LLM需要进一步详细的增强，比如长文档建模、精确的概念理解Results on Negative Rejection testbed针对 负拒绝 实验：（只提供负样本，看模型的拒绝率）采用精确匹配评估拒绝率（Rej），利用ChatGPT来确定LLM的response是否包含拒绝信息（Rej*）。结论： 负拒绝对LLM中的RAG提出了挑战。 中文/英文LLM的最高拒绝率最高43.33%/45%。 通过比较Rej和Rej*，发现LLM不能严格遵循指令，且经常产生不可预测的response，因此LLM很难直接用来识别负样本并判定拒绝。 2个原因： 证据不确定性 虽然文件中只提到与 “亚当-麦凯 “的联系，并没有明确指出他是这部电影的导演，但模型仍然断定他担任了这一角色。 概念混淆 答案中提供的信息与问题中提到的“2022 年冬季奥运会”而不是“2022 年奥运会”。与直接回答相比，检索增强生成对负拒绝提出了更大的挑战，因为它提供了可能误导 LLM 并导致错误响应的相关文档。 Results on Information Integration testbed针对 信息整合 实验：不同噪声率下，评估模型准确率。结论： 信息整合 对于LLMs中的RAG来说是一个大挑战（LLM难以有效地整合信息，不太适合直接回答复杂的问题。） 即使噪声率=0，英文/中文中最高也只有60%/67%的准确率 添加噪声文档后，英文/中文下降到43%/55% 对于文档嘈杂的RAG来说，复杂的问题更具挑战性。 对比 噪声鲁棒性 实验（简单问题，添加噪声文档），如下图。简单问题时，噪声比达到0.8时，性能才显著下降，而复杂问题时，噪声比=0.4时，性能猛降。 这表明复杂的问题更容易受到噪声的干扰。我们推测，这是因为解决复杂问题需要整合来自多个文档的信息，而这些信息可以被视为彼此的噪声，使模型更难从文档中提取相关信息。 4个原因：（这里分析的是噪声率-0的ChatGLM2-6B的错误数据） 38%的错误实例属于 噪声稳健性 实验中发现的3个错误原因 合并错误（占比28%）： 模型试图合并两个子问题的答案，错误地使用一个问题的答案直接回答两个子问题，然后忽略另一个子问题相关的所有文档。 例如：模型回答D 组是法国队和德国队的世界杯小组，而实际上德国队被分到了 E 组。 忽视错误（28%） 模型只回答一个问题，直接忽略另一个问题。原因是模型对问题缺乏全面的理解，没有认识到问题由多个子问题组成。 例如：模型只提供了 2022 年超级碗 MVP 的答案，而没有考虑 2023 年。 错位误差（6%） 模型把子问题1的文档误识别为子问题2的问答，导致答案错位。 例如：模型仅提到了 2023（95）学院奖的最佳图片，完全忽略了 2022 年奖项。此外，它错误地指出，“CODA”是 2023 的最佳图片，当它实际上被授予 2022 年的最佳图片时。 上述错误主要是由于对复杂问题的理解有限，这阻碍了有效利用来自不同子问题的信息的能力。关键是提高模型的推理能力。一种可能的解决方案是使用思维链方法来分解复杂问题（Zhou et al. 2023a; Xu et al. 2023b; Drozdov et al. 2023）。然而，这些方法会减慢推理速度，无法提供及时的响应。Results on Counterfactual Robustness testbed针对 反事实鲁棒性 实验：（考察模型对于已知事实知识的问题，包含事实错误的噪声文档能否干扰他的思考。因此只考虑准确率超过 70% 的 LLM。）4个指标：①不包含任何文档的准确率、②包含反事实文档的准确率、③错误识别率、④错误纠正率采用精确匹配评估错误识别率（ED），利用ChatGPT来识别LLM的response是否包含反事实错误信息（ED*）。 检索增强生成不是为自动解决给定上下文中的事实错误而设计的，因为这与模型缺乏知识并且依赖于检索到的文档以获取附加信息的基本假设相矛盾。 然而，由于互联网上假新闻泛滥，这个问题在实际应用中至关重要。现有的 LLM 不具备处理因错误信息造成的不准确回复的保障措施。事实上，它们在很大程度上依赖于检索到的信息。即使 LLM 包含有关问题的内部知识，它们也经常相信检索到的虚假信息。这对未来在 LLMs 中发展 RAG 提出了重大挑战。&lt;font color=#008B8B&gt;🤹个人总结&lt;/font&gt;" }, { "title": "RAG", "url": "/posts/RAG%E6%A3%80%E7%B4%A2%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA/", "categories": "NLP, 基础知识", "tags": "nlp, RAG, 技术综述", "date": "2024-02-21 00:00:00 +0800", "snippet": "RAG检索知识增强 不完全手册本文从k个角度对RAG技术进行综述，第一个角度是“范式演变”，即原始的“朴素RAG”-“进阶RAG”-“模块化RAG”，这个角度个人认为是工程的角度、宏观的角度；第二个角度是“关键问题和相关研究”，大类上可分两类“以优化生成模型的表征空间为目的”和“以优化知识密集型问答任务为目的”。通常“以优化生成模型的表征空间为目的”的这些模型有几个特点：（1）检索对象是token/chunk，即取input中的部分或全部token作为query触发检索器，（2）检索的时机是每生成1个token都要增强一下，这样才能达到优化整个表征空间的目的，（3）使用检索内容的方式既可以在输入层注入、在生成模型内部注入也可以在输出层注入。通常“以优化知识密集型问答任务为目的”的这些模型有几个特点：（1）检索对象下至token-level上至段落-level，（2）检索的时机多为隔几个token或者遇见特定的token才会触发检索，因为它的目的不是优化表征空间，而是优化问答任务，(3）使用检索内容的方式往往不采用在生成模型内部注入的方法，因为这种模型往往不希望训练。[1] 专补大模型短板的RAG有哪些新进展？这篇综述讲明白了https://mp.weixin.qq.com/s/yZo-HcGuWFQE8B63hZkqVQ [2] 前沿重器[41] 综述-面向大模型的检索增强生成（RAG）https://mp.weixin.qq.com/s/Ebc-DEpfEBNP1nrQHkQv4g [3] OpenAI Sora、Prompt工程、RAG、表格处理、长文本、embedding等大模型进展总结分享回顾https://mp.weixin.qq.com/s/5nUKrTYSGrJhPMIXw6_35A[4] 大模型如何可解释？帝国理工最新《大型语言模型的解释性》最新综述https://mp.weixin.qq.com/s/kW5qpJQQ74wo8LfiiGgeCA[5] 大模型增量预训练新技巧-解决灾难性遗忘https://mp.weixin.qq.com/s/IoCxatkaDeYhfWVuCQKnJw[6] 说说我对RAG技术的理解https://mp.weixin.qq.com/s/7FH7sqk3pc2hjH-bkGtHxg[7] Transformer的无限之路：位置编码视角下的长度外推综述https://mp.weixin.qq.com/s/9SK8YuUzHr4ZOnFkQNQb6g[8] TRL 正式推出，来训练你的首个 RLHF 模型吧！https://mp.weixin.qq.com/s/WSUs0ipdb2gKkNdQ60isRw [9] 分享   OpenAI 如何不通过 fine-tuning 将 RAG 的准确率由 45% 提升至 98%？https://mp.weixin.qq.com/s/B7VaDU02LIC5x_ww3zlb2A [10] RAG的究极进化：知识图谱和向量检索结合https://mp.weixin.qq.com/s/Q-Dtq-sFgsJQI4r7C4lr4Q[11] 简单提升RAG的10种方法https://mp.weixin.qq.com/s/bNyMMDkjPWOq_V1AqHqCBg[12]【图文长文】探索RAG技术：从知识检索到答案生成的神奇过程解析https://mp.weixin.qq.com/s/qaEEMUxstAjVKml7VbfqFA[13] RAG行业交流中发现的一些问题和改进方法https://mp.weixin.qq.com/s/BXP3g8El3jUF8VFjh3Ufzg[14] ChatGPT应用：如何征服市场眼中的“万能RAG”https://mp.weixin.qq.com/s/4kwwBnpGvXbwNBatLqcghg[15] Graph RAG: 知识图谱结合 LLM 的检索增强 - 知乎https://zhuanlan.zhihu.com/p/654008500[16] 基于RAG构建生成式AI应用最佳实践与“避坑指南”–亚马逊https://github.com/lizhe2004/Awesome-LLM-RAG-Application/blob/main/resource[17] 爱可可 AI 前沿推介(8.10)https://mp.weixin.qq.com/s/VqjlUeji1PAsF9PEKy4oYQ一、范式演变[1]【RAG系列探索之旅·第四弹】RAG研究范式：从初级到高级的演变https://mp.weixin.qq.com/s/VHHmn2DDZ_ykSxdFNsK9vg[2]【RAG系列探索之旅·第六弹】模块化RAG：重塑信息检索的未来https://mp.weixin.qq.com/s/FX4zs3GbIFIhtG4O3b6dNg [3] 技术动态 模块化（Modular）RAG 和 RAG Flowhttps://mp.weixin.qq.com/s/Pdx8J_tCyYJ_bgFknc4UoA [4] 也看大模型RAG长文本任务中的上下文精简与构造方式：兼看最近一周的大模型开源工作https://mp.weixin.qq.com/s/6RrFxfcj0w7jhPXk5mqEuw1.1 Naive RAG局限性： 检索——不够准确 精度低：检索块错位 召回率低：无法检索所有相关块 信息过时：产生不准确的检索结果 生成——幻觉、可控性差 生成的答案不基于检索到的文档 生成的答案与问题上下文不相关 生成的答案存在潜在反事实错误或偏见 增强——如何更好的融合外部文档 多个检索段包含相似信息时，生成的响应中会出现重复内容 多个检索段的重要性和相关性，对答案生成的价值需要平衡好 模型过度依赖检索段的信息，导致输出的只是重复检索的内容，而不提供新的价值或合成信息 1.2 Advanced RAG1.2.1 Pre-Retrieval Process优化检索器的输出质量： 数据清洗与更新：删除不相关的信息，消除实体和术语中的歧义，确认事实的准确性，维护上下文，以及更新过时的文件。 优化文档结构：调整块的大小以捕获相关上下文，跨多个索引路径进行查询，以及通过利用图数据索引中节点之间的关系来合并来自图结构的信息以捕获相关的上下文。 添加元数据信息：将引用的元数据（如日期和目的）集成到块中以进行过滤，并合并引用的章节和小节等元数据以提高检索效率。 对齐优化：对问题进行重写、路由和扩充。通过在文档中引入“假设问题”[Li et al.，2023d]来纠正对齐问题和差异，从而解决对齐问题和文档之间的差异。 1.2.2 Post-Retrieval Process从数据库中检索到有价值的上下文后，必须将其与查询合并，作为LLM的输入，同时解决上下文窗口限制带来的挑战。简单地将所有相关文档一次性呈现给LLM可能会超出上下文窗口限制，引入噪声，并阻碍对关键信息的关注。为了解决这些问题，需要对检索到的内容进行额外的处理。 重排序：对检索到的信息重新排序以将最相关的内容重新定位到提示的边缘是一个关键策略，避免 “Lost in the Middle ” 现象的发生。 Prompt 压缩：检索到的文档中的噪声会对RAG性能产生不利影响。在后处理中，重点在于压缩不相关的上下文，突出关键段落，减少整体上下文长度。1.3 Modular RAG部分模块介绍： 记忆池模块 在检索增强生成（RAG）中，记忆池和检索到的外部知识文档的区别。 记忆池： ​\t1.定义：记忆池是一个存储在模型内部的数据集合，用于引导模型的生成过程。 ​\t2.数据来源：记忆池中的数据通常来自于模型训练时使用的数据集，或者是与任务相关的其他文本片段。 ​\t3.特点：这些数据是模型已经学到的知识，存储在模型的权重中（参数记忆）。 ​\t4.更新：记忆池的数据通常不会频繁更新，因为它们是模型训练期间获取的。 检索到的外部知识文档： ​\t1.定义：这些文档是从外部知识源（例如向量数据库、专业领域的文章等）中检索到的数据。 ​\t2.数据来源：外部知识文档可能包含最新的、专有的、或特定领域的信息。 ​\t3.特点：这些文档不是模型训练期间学到的，而是从外部获取的，因此可能更准确、更具体。 ​\t4.更新：外部知识文档可以随时更新，而不会产生重大成本。 总之，记忆池是模型已知的内部数据，而检索到的外部知识文档是从外部获取的数据。RAG通过结合这两者，使模型能够更准确地生成答案，并且可以利用最新的、来自外部的信息。 融合模块 通过多查询的方式扩展用户查询，从不同角度获取信息，确保搜索结果与用户的显性和隐性意图密切相关，从而实现更深入、更准确的信息发现。 RAG-Fusion：这个方法通过多查询的方式扩展用户查询，从不同角度获取信息，利用语言模型（LLM）来揭示更深层次、变革性的知识。具体而言，它包括以下步骤： 扩展查询：将用户查询扩展为多个不同的视角，以获取更全面的信息。 并行向量搜索：同时对原始查询和扩展查询进行向量搜索，以获取相关文档。 智能重新排序：对搜索结果进行智能排序，以优化答案的质量。 新查询匹配：将最佳结果与新查询匹配，以满足用户的显性和隐性意图。 优势与挑战： 优势： 实时更新：RAG能够访问最新的外部信息，保持知识的时效性。 减少幻觉：通过使用外部验证信息，RAG有助于减少语言模型产生的错误或虚构信息。 高透明度：生成答案时引用外部信息源，增加可信度和可追溯性。 挑战： 外部依赖：RAG的性能高度依赖于外部知识库的质量和覆盖范围。 生成延迟：检索过程可能增加回答生成的时间延迟。 路由模块 负责决定对用户查询的后续操作。 数据源处理决策：RAG系统的检索过程利用了不同领域、不同语言和不同格式的多样化数据源。路由模块决定对这些数据源如何操作比如交替或合并。 查询处理决策： 摘要生成：查询路由器决定是否提供相关信息的简洁摘要。 数据库搜索：路由模块决定连接到哪个特定数据库，以获取详细的信息。 路径合并：路由模块决定是否将不同路径的信息合并为单个响应。 数据存储方式决策：查询路由器还会选择适当的数据存储方式，这可能包括各种来源，如向量存储、图数据库、关系数据库或索引层次结构，例如用于多文档存储的摘要索引和文档块向量索引。查询路由器的决策是预定义的，并通过语言模型（LLMs）调用来执行，从而将查询定向到选择的索引。 预测模块 不直接从数据源检索内容，而是利用语言模型（LLM）生成必要的上下文。这个上下文包含了与用户查询相关的信息。这个上下文被当作query执行检索。 好处：​\t1. 减少冗余：由LLM生成的内容更有可能包含相关信息，相比直接从数据源检索获得的内容更具相关性。因此，它可以减少重复的信息。​ \t2. 降低噪音：通过使用LLM生成的上下文，预测模块可以过滤掉不相关或低质量的信息**，从而减少了检索结果中的噪音。 适配器模块 两个例子： UPRISE：这个模块自动从预先构建的数据池中检索用于零样本任务输入的提示。通过这种方式，它增强了任务和模型之间的通用性。UPRISE使得RAG能够适应不同任务的需求，而无需重新训练模型。 PROMPTAGATOR：这个模块利用语言模型（LLM）作为少样本查询生成器。基于生成的数据，它创建了特定任务的检索器。通过充分利用LLMs的泛化能力，PROMPTAGATOR可以使用极少的示例开发特定任务的端到端检索器。 二、RAG的3大关键问题**解决关键问题的方法其实是在优化RAG中的1或多个模块的方法。**[1] Advanced RAG — Improving retrieval using Hypothetical Document Embeddings(HyDE):https://medium.aiplanet.com/advanced-rag-improving-retrieval-using-hypothetical-document-embeddings-hyde-1421a8ec075a[2] 一文纵览LLM+RAG 的方法实现https://mp.weixin.qq.com/s/ifp2i71Psn86ZCEzffsF0Q[3] 大模型检索增强生成（RAG）系统进化指南https://mp.weixin.qq.com/s/4Ttd3hi13B4VdoPGauwMzA [4] ACL23 基于检索的大语言模型-陈丹琦报告阅读https://mp.weixin.qq.com/s/_rajDkg-T1a6R0MKlLSEXA [5] RAG Survey - 三大组件https://mp.weixin.qq.com/s/W1E_qFajK6XJYRNzlriS6Q[6] ACL 2023 Tutorial: Retrieval-based LMs and Applicationshttps://acl2023-retrieval-lm.github.io/4.1 优化生成模型表征空间每隔l个token触发1次检索，第t次生成是根据当前片段$window_t$以及其对应的最相似文档$R(window_t)$生成的。① RETRORETRO模型是每隔l个token触发一次检索，检索器的输入q是当前的上下文片段，然后根据q检索出k个最相似的文档片段，作为检索器的输出retriever(q)。然后，第t次生成的输出yt是由语言模型LM根据q和retriever(q)的联合表示来预测的。也就是说，yt=RETRO-block(qt,retriever(qt)]是RETRO模型的生成公式。公式② IC-RALM算是一种适应性检索。 Since common Transformer-based LM implementations support limited length input sequences, when the concatenation of the document and the input sequence exceed this limit we remove tokens from the beginning of x until the overall input length equals that allowed by the model. Because our retrieved documents are passages of limited length, we always have enough context left from x.IC-RALM是一种检索增强语言模型，它使用一个可训练的检索器来动态地调整检索到的文档的权重，以适应不同的任务1。它的主要思想是将检索到的文档作为上下文，与当前的生成状态一起输入到语言模型中，从而提高生成的质量和多样性。它的主要优点是可以根据任务的需求，自动选择最相关的文档，并且可以处理多个文档的组合。它的主要缺点是需要对检索器和语言模型进行联合训练，这会增加计算成本和复杂性。IC-RALM的工作流程如下： 首先，给定一个输入序列，例如一个问题或一个部分生成的文本，检索器会从一个大型的文档集合中检索出k个最相关的文档。检索器可以是任何基于向量的检索方法，例如BM25、DPR或ColBERT。 然后，检索器会为每个检索到的文档分配一个权重，表示它对当前输入的重要性。权重是通过一个可训练的神经网络计算的，该网络接收检索到的文档和输入序列的嵌入作为输入，并输出一个标量值。权重可以被视为一个注意力机制，用于选择最相关的文档。 接下来，检索到的文档和输入序列被拼接在一起，形成一个新的输入序列，用于语言模型的生成。语言模型可以是任何基于Transformer的自回归模型，例如GPT-2或GPT-3。语言模型会根据检索到的文档和输入序列生成下一个标记，从而实现文本生成或问答等任务。 最后，检索器和语言模型会通过反向传播进行联合训练，以最小化生成标记的交叉熵损失。这样，检索器可以学习如何为不同的任务选择最合适的文档，而语言模型可以学习如何利用检索到的文档进行更好的生成。③ KNNLMKNN-LM在生成模型根据上文生成下一个token的时候，将上文的向量表征和训练集中的所有上文的表征进行knn计算，得到最相近的k个上文各自的下一个token，归一化，和原来的下一个token计算，得到一个更优的下一个token。每个token检索1次。④ REPLUG [1] 论文分享 arXiv-23 REPLUG：检索增强黑盒语言模型https://mp.weixin.qq.com/s/cS3eTDSO1oDs8hSxMkTjvw [2] 一文详解检索增强语言模型新范式REPLUGhttps://mp.weixin.qq.com/s/kULpBme-NSYmOw4RLboyGQ[3] REPLUG: Retrieval-Augmented Black-Box Language Modelshttps://readpaper.com/pdf-annotate/note?pdfId=4762336033991819265[4] 大模型RAG中embedding的两个新进展BGE-M3及MRL：兼看几个有趣的综述及水印工作https://mp.weixin.qq.com/s/NJNnyfNaeVNjO1GnJ46T1g[5] 微软放大招：基于RAG与Fine-Tuning的数据整合策略探索https://mp.weixin.qq.com/s/oeQnMGmGMT0R8x3h35weqg4.2 优化知识密集型问答任务每个句子触发1次检索，用前1个句子作为$q$，$q_{t-1}$和$R(q_{t-1})$生成$y_t$。① REALM② IRCoT[1] 「think step by step」还不够，让模型「think more steps」更有用https://mp.weixin.qq.com/s/UgglOk-u6Qv3IrY6aWcarQ[2] In-Context Learning玩法大全https://mp.weixin.qq.com/s/NLWCuzcCdwljQfzu-Jd9lQ 问题分解方法是手动注释特定任务的示例，以指导LMs在生成输出的同时生成分解的子问题。③ Self-Ask实验结果：GPT3：Davinci-002上表显示了Bamb&amp;2wikimultihop&amp;musique三个数据集的Accuracy指标，Bamb是作者人工判断的，剩下两个是“完全匹配”。上表显示了和Least-to-Most模型的对比，toks指生成的回答的长度。④ Least-to-Most第一阶段：先“问”模型回答问题首先需要知道什么第二阶段：引导模型先生成子问题的回答，然后将子问题及其回答作为instruction补充在input中，引导模型生成最后的回答[1] 如何使用快速压缩将RAG的Prompt成本削减80%https://mp.weixin.qq.com/s/7j1eTqD3DnRH-o2ZjREuPg⑤ FLARE[1] Active RAG – FLARE 详解https://mp.weixin.qq.com/s/YTxOjwjNvyhrUtVtmQ-rRQ[2] logprobs-OpenAI fine tune数据集准备最佳实践https://mp.weixin.qq.com/s/S9t7gw55MIaCGY60iaso7Q⑤ Self-RAG[1] 也看引入自我反思的大模型RAG检索增强生成框架：SELF-RAG的数据构造及基本实现思路https://mp.weixin.qq.com/s/VyrkSnYb4Uss8cfZp1yrvA4.4 RAG的评估方法A.评估方式① NDCG（Normalized Discounted Cumulative Gain，归一化折损累计增益）是一种用于评估排序结果的指标。在搜索和推荐任务中，系统常常返回一个项目列表。NDCG的目的是衡量这个返回的列表的质量。原理如下： Gain（增益）：表示一个列表中所有项目的相关性分数。rel(i) 表示第 i 个项目的相关性得分。 Cumulative Gain（累积增益）：对前 k 个项目的 Gain 进行累加。$CG_k = \\sum_{i=1}^{k} rel(i)$ Discounted Cumulative Gain（折损累计增益）：考虑了排序顺序的因素，使得排名靠前的项目增益更高，对排名靠后的项目进行折损。DCG 的计算公式为：$DCG_k = \\sum_{i=1}^{k} \\frac{rel(i)}{\\log_2(i+1)}$。其中，i 表示项目的排名，rel(i) 是项目的相关性得分。 Ideal DCG（理想的 DCG）：根据相关性降序排列的最佳状态，即排列到最好的状态。IDCG 是 DCG 的理想值，它是一个参考点。 Normalized DCG（归一化折损累计增益）：用 DCG 除以 IDCG 来表示，这样可以将 DCG 归一化到 [0, 1] 范围内。NDCG 的计算公式为：$NDCG = \\frac{DCG}{IDCG}$。通过 NDCG，我们可以在不受不同查询结果数量的影响的前提下相对地评估不同查询或推荐结果的质量。② MRR，该指标越大越好（即预测排名越靠前，倒数就越大，求和结果越大越好） $\\text{MRR} = \\frac{1}{Q} \\sum_{i=1}^{ Q } \\frac{1}{\\text{rank}_i}$ RAG 的评估方法多样，主要包括三个质量评分：上下文相关性、答案忠实性和答案相关性。此外，评估还涉及四个关键能力：噪声鲁棒性、拒答能力、信息整合和反事实鲁棒性。这些评估维度结合了传统量化指标和针对 RAG 特性的专门评估标准，尽管这些标准尚未统一。 忠实度：衡量生成的答案与给定上下文的事实一致性。$\\text{忠实度} = \\frac{\\text{句子中的基本事实可以从上下文中推断出的句子数量}}{\\text{生成回答的句子总数}}$ 答案相关性：评估生成的答案与用户问题之间的相关程度。​ 思想：如果生成的答案准确地解决了最初的问题，LLM应该能够从答案中生成与原始问题相符的问题。​ 做法：为了计算这个分数，LLM会被提示多次为生成的答案生成适当的问题，并测量这些生成的问题与原始问题之间的平均余弦相似度。$score_{AR} = \\frac{\\sum_{i=0}^{k-1} \\text{cos_sim} (Q, q’_{i})}{k} $$q’=LLM(answer)$$Answer Relevance = \\sum_{i=0}^{k-1} \\text{cos_sim} (Question, LLM(answer)_{i})$ 上下文相关性：评估检索到的上下文的相关性。 准确率：检索到的context中与query相关的句子数量/检索到的context的句子总数 ​ 理想情况下，检索到的Context应只包含解答question的信息。 我们首先通过识别检索到的Context中与回答question相关的句子数量来估计 S 的值。 最终分数由以下公式确定： $\\text{上下文相关性} = \\frac{\\text{检索到的上下文中与真实答案相关的句子数量}}{\\text{真实答案中的句子总数}}$在评估框架方面，存在如 RGB 和 RECALL 这样的基准测试，以及 RAGAS、ARES 和 TruLens 等自动化评估工具，它们有助于全面衡量 RAG 模型的表现。表中汇总了如何将传统量化指标应用于 RAG 评估以及各种 RAG 评估框架的评估内容，包括评估的对象、维度和指标，为深入理解 RAG 模型的性能和潜在应用提供了宝贵信息。$score_{EM}$搜索推荐评价指标Precision@k、Recall@k、F1@k、NDCG@k_precision@5-CSDN博客https://blog.csdn.net/guolindonggld/article/details/121114309NDCG 归一化折损累计增益的动机、讲解、公式、实现 - 知乎https://zhuanlan.zhihu.com/p/474423793 Advanced RAG Techniques: an Illustrated Overview by IVAN ILIN Towards AIhttps://pub.towardsai.net/advanced-rag-techniques-an-illustrated-overview-04d193d8fec6 【RAG系列探索之旅·第十弹】RAG评估全攻略：方法、指标与框架https://mp.weixin.qq.com/s/EJteC2assR-mGoEv-n5Q_w05细说RAG评估指标https://mp.weixin.qq.com/s/jVs2S4O8Sd_AtzEWdFRjoQ再看大模型RAG检索增强如何评估：RAGAS开源自动化评估框架https://mp.weixin.qq.com/s/TrXWXkQIYTVsS1o4IZjs9w大模型RAG检索增强问答如何评估：噪声、拒答、反事实、信息整合四大能力评测任务探索https://mp.weixin.qq.com/s/YFji1s2yT8MTrO3z9_aI_w如何评估 RAG 应用的质量？最典型的方法论和评估工具都在这里了https://mp.weixin.qq.com/s/OnfSxBJx_lVYV_MtyViUMw LLM之RAG实战（二十七） 如何评估RAG系统https://mp.weixin.qq.com/s/4EcYxBpPLMrbACy3aSyOyQ 科普贴：一文说透RAG的方方面面https://mp.weixin.qq.com/s/mrnU3DLbGsumJ7mOjosh8gRAG 评估框架 – RAGAShttps://mp.weixin.qq.com/s/MPUPbyjuIVhw1wM3l0P1aANDCG - 知乎https://zhuanlan.zhihu.com/p/371432647高级RAG(四)：Ragas评估 - - 派神 -的文章 - 知乎https://zhuanlan.zhihu.com/p/675777378学习检索增强生成(RAG)技术，看这篇就够了——热门RAG文章摘译(9篇) - 吕阿华的文章 - 知乎https://zhuanlan.zhihu.com/p/673392898 🚀 Get Started Ragashttps://docs.ragas.io/en/latest/getstarted/index.html RAG 评估框架 – ARES - 知乎https://zhuanlan.zhihu.com/p/677767672 Evaluating Naive RAG and Advanced RAG pipeline using langchain v.0.1.0 and RAGAS by Plaban Nayak Feb, 2024 AI Planethttps://medium.aiplanet.com/evaluating-naive-rag-and-advanced-rag-pipeline-using-langchain-v-0-1-0-and-ragas-17d24e74e5cf openai-cookbook/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb at main · openai/openai-cookbookhttps://github.com/openai/openai-cookbook/blob/main/examples/evaluation/Evaluate_RAG_with_LlamaIndex.ipynb也看大模型的选择题评估方式是否鲁棒：兼看RAG知识增强问答中的知识冲突和评测基准https://mp.weixin.qq.com/s/LYJcqvWsqnTkQcFnPd2WKghttps://arxiv.org/pdf/2309.15217.pdfRAG的未来展望 Gemini 1.5 Pro 快速工程指南https://www.promptingguide.ai/models/gemini-pro 谷歌10M上下文窗口正在杀死RAG？被Sora夺走风头的Gemini被低估了？https://mp.weixin.qq.com/s/t3fsKksf7DWwVJY5vldPNwLLM长上下文时代，RAG的机遇与挑战https://mp.weixin.qq.com/s/LhwWE6t_muigcLbXAAKQ_w负拒绝问题：首先，什么问题需要LLM拒绝回答？1. 本身不具备的知识拒绝回答，2. 上下文中提供的信息仍然不包含回答问题必要的信息时拒绝回答，3. 上下文中提供的信息包含反事实错误时识别出来并拒绝回答。针对第一种问题可以采用置信度的策略，针对模型的直接回答中提取置信度低的token的个数，当数量超过总token的30%（此阈值可调参）视为LLM无法回答问题，给出拒绝回答。 使用拒绝采样（Rejection Sampling）技术，让LLM在生成答案之后，对答案进行评估，如果评估得分低于某个阈值，就拒绝输出该答案，或者重新生成一个新的答案45。 使用自我评估（Self-Evaluation）技术，让LLM在生成答案之后，对答案进行自我检查，如果发现答案与问题不匹配，或者包含错误或不确定的信息，就拒绝输出该答案，或者给出一个带有免责声明的答案67" }, { "title": "Papernote_llm可解释性综述", "url": "/posts/PaperNote_LLM%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7%E7%BB%BC%E8%BF%B0/", "categories": "", "tags": "", "date": "2024-01-27 00:00:00 +0800", "snippet": "LLM可解释性综述" }, { "title": "Llm的幻觉问题综述", "url": "/posts/LLM%E7%9A%84%E5%B9%BB%E8%A7%89%E9%97%AE%E9%A2%98%E7%BB%BC%E8%BF%B0/", "categories": "", "tags": "", "date": "2023-12-13 00:00:00 +0800", "snippet": "LLM的幻觉问题综述《A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions》" }, { "title": "调研mteb massive text embedding benchmark", "url": "/posts/%E8%B0%83%E7%A0%94MTEB-Massive-Text-Embedding-Benchmark/", "categories": "", "tags": "", "date": "2023-12-08 00:00:00 +0800", "snippet": "调研 MTEB &amp; BEIR 问题：是否要做所有的任务，是否先做检索任务(需要参考BEIR) 目前先检索任务+分类任务实验设置模型：33个语言：112种指标：速度、内存占用访问方式：调用、API调用数据集：58个，8类嵌入任务Bitext 挖掘、分类、聚类、对分类、重新排序、检索、STS 和摘要① 任务划分 聚类：给定一组句子或段落，目标是将它们分组到有意义的集群中。批量大小为 32 和 k 的小批量 k-means 模型等于不同标签的数量 (Pedregosa et al., 2011) 在嵌入文本上进行训练。该模型使用 v-measure 进行评分（Rosenberg 和 Hirschberg，2007）。Vmeasure 不依赖于集群标签，因此标签的排列不会影响分数。 RedditClustering：199个子版块的标题聚类。25个分段的聚类，每个分段有10-50个类，每个类有100-1000个句子 RedditClusteringP2P：使用Reddit帖子中的可用数据为MTEB创建数据集1。该任务包括根据标题+帖子的subreddit进行聚类拼接。它包含10个拆分，每个拆分有10到100个集群和1,000到100,000个帖子。 StackExchangeClustering：来自121个堆栈交换的标题聚类。聚类25个片段，每个片段有10-50个类，每个类有100-1000个句子。 StackExchangeClusteringP2P：使用StackExchange posts2中的可用数据为MTEB创建数据集。该任务包括根据标题和帖子的子reddit进行聚类拼接。它包含10个拆分，每个拆分有10到100个集群和5,000到10,000个帖子。 TwentyNewsgroupsClustering3：对给定文章标题的20个新闻组数据集进行聚类，目标是找到新闻组(总共20个)。包含10个拆分，每个拆分包含20个类，每个拆分包含1,000到10,000个标题。 分类： A 训练集和测试集嵌入了提供的模型。训练集嵌入用于训练具有 100 个最大迭代次数的逻辑回归分类器，该分类器在测试集上进行评分。主要指标是平均精度的准确度，另外提供了 f1。 AmazonCounterfactual：一组为反事实检测 对分类? 注释的亚马逊客户评论。对于每个评论，标签是“反事实”或“非反事实”。这是一个具有 4 种可用语言的多语言数据集。 AmazonPolarity：一组为极性分类注释的亚马逊客户评论。对于每个评论，标签要么是“正面”，要么是“负面”。 AmazonReviews：一组亚马逊评论，旨在帮助多语言文本分类的研究。对于每个评论，标签是评论在 0 到 4（1-5 星）之间的分数。这是一个具有 6 种可用语言的多语言数据集。 Banking77：由带有相应意图注释的在线银行查询组成的数据集。对于每个用户查询，标签是 77 个意图中的一个意图，例如“activ_my_card”、“apple_pay”、“bank_transfer”等。 Emotion：具有六种基本情绪的英语 Twitter 消息数据集：愤怒、恐惧、喜悦、爱、悲伤和惊讶。 Imdb：带有正面或负面标签的大型电影评论数据集。 MassiveIntent：一组带有相关意图注释的 Amazon Alexa 虚拟助手话语。对于每个用户话语，标签是 60 个意图之一，例如“play_music”、“alarm_set”等。这是一个具有 51 种可用语言的多语言数据集。 MassiveScenario：一组带有相关意图注释的 Amazon Alexa 虚拟助手话语。对于每个用户话语，标签是 60 个场景中的一个主题，例如“音乐”、“天气”等。这是一个具有 51 种可用语言的多语言数据集。 MTOPDomain / TOPIntent：MTOP (Li et al., 2020) 基准的多语言句子数据集。有关详细信息，请参阅他们的论文。 ToxicConversations：数据集来自Kaggle competition4。收集来自民间评论平台的评论，以及评论是否有毒的注释。 TweetSentimentExtraction：数据集来自Kaggle competition5。将tweet的情绪分类为中性、积极或消极。 对分类：提供了一对文本输入，并且需要分配标签。标签通常是表示重复或释义对的二元变量。嵌入了这两个文本，它们的距离是通过各种指标计算的（余弦相似度、点积、欧几里得距离、曼哈顿距离）。使用最佳二进制阈值精度，计算平均精度 f1，精度和召回率。基于余弦相似度的平均精度分数是主要的指标。 SprintDuplicateQuestions：来自Sprint社区的问题集合。目标是将一对句子分类为重复或不重复。 TwitterSemEval2015：来自SemEval 2015研讨会的对推文的释义。目标是将一对推文分类为释义或非释义。 TwitterURLCorpus：推文的释义对。目标是将一对推文分类为释义/非释义。 Bitext 挖掘：输入是来自两个不同语言的两组句子。对于第一组中的每个句子，需要找到第二组中的最佳匹配。匹配通常是翻译。提供的模型用于嵌入每个句子，最接近的对是通过余弦相似度找到的。F1 作为双文本挖掘的主要指标。还计算了准确性、精度和召回率。 BUCC：BUCC 为英语、法语、俄语、德语和中文提供了大量句子（每个约 10-70k），以及相关的对注释。这里的注释对对应于一对翻译的句子，即一个句子及其在另一种语言的翻译。 Tatoeba：Tatoeba 为 112 种语言提供了一组句子（每个句子 1000 个句子），其中包含带有注释的相关对。每对都是一个句子及其在另一种语言中的翻译。 重新排序：输入是查询和相关和不相关参考文本列表。目的是根据结果与查询的相关性对结果进行排名。该模型用于嵌入参考，然后使用余弦相似度与查询进行比较。每个查询对生成的排名进行评分，并在所有查询中取平均值。指标是平均 MRR@k 和 MAP，后者是主要指标。 AskUbuntuDupQuestions：来自 AskUbuntu 的问题，带有手动注释，将成对的问题标记为相似或不相似。 MindSmall：用于新闻推荐研究的大规模英语数据集。给定新闻文章标题对新闻文章标题进行排名。这个想法是从您正在阅读的新闻中推荐其他新闻。 SciDocsRR：根据其标题对相关科学论文进行排名。 StackOverflowDupQuestions：针对带有Java, JavaScript和Python标签的问题的任务，将问题列为重复或不重复。 检索：每个数据集由一个语料库、查询和每个查询到语料库中的相关文档的映射组成。目标是找到这些相关文档。提供的模型用于嵌入所有查询，并使用余弦相似度计算所有语料库文档和相似度分数。根据分数对每个查询的语料库文档进行排名后，计算 nDCG@k、MRR@k、MAP@k、precision@k 和 recall@k 的几个 k 值。nDCG@10 作为主要指标。MTEB 重用来自 BEIR 的数据集和评估（Thakur 等人，2021 年）。下图来自论文BEIR。 ArguAna, ClimateFEVER, CQADupstack, DBPedia, FEVER, FiQA2018, HotpotQA, MSMARCO, NFCorpus, NQ, Quora, SCIDOCS, SciFact, Touche2020, TRECCOVID：参考BEIR 语义文本相似度 (STS) ：给定一个句子对，目的是确定它们的相似度。标签是连续分数，数字更高，表明句子更相似。提供的模型用于嵌入句子，并使用各种距离度量计算它们的相似度。使用 Pearson 和 Spearman 相关性以基本事实相似性对距离进行基准测试。基于余弦相似度的 Spearman 相关性作为主要指标（Reimers 等人，2016 年）。分值为0-5之间的连续值。 STS12, STS13, STS14, STS15, STS16, STS17, STS22, STSBenchmark：原始STS基准，分数从0到5。选择的句子包括来自图片说明、新闻标题和用户论坛的文本。它们总共包含1,000到20,000个句子。STS12 - STS16 &amp; STSBenchmark是单英语的，STS17和STS22包含跨语言的句子对，目的是评估不同语言中两个句子的相似度。STS17有11个语言对(韩语、阿拉伯语、英语、法语、德语、土耳其语、西班牙语、意大利语和荷兰语)，STS22有18个语言对(阿拉伯语、英语、法语、德语、土耳其语、西班牙语、波兰语、意大利语、俄语和汉语)。 BIOSSES：包含100个来自生物医学领域的句子对。 SICK-R：涉及构成知识的句子(SICK)包含大量的句子对(10万个)，这些句子对在词汇、句法和语义上都很丰富。 摘要：提供了一组人工编写的和机器生成的摘要。目的是对机器摘要进行评分。提供的模型首先用于嵌入所有摘要。对于每个机器摘要嵌入，计算所有人类摘要嵌入的距离。保留最接近的分数（例如最高余弦相似度），并用作模型对单个机器生成的摘要的分数。计算了 Pearson 和 Spearman 相关性与机器生成的摘要的基本事实人工评估。与 STS 一样，基于余弦相似度的 Spearman 相关性作为主要指标（Reimers 等人，2016 年）。 SummEval：由CNN或DailyMail上训练的最新摘要模型生成的摘要以及人工注释。 相关工作2.1 基准构建相关工作 【文本嵌入首选Benchmark】SemEval datasets (Agirre et al., 2012, 2013, 2014, 2015, 2016) (Super)GLUE (Wang et al., 2018, 2019) Big-BENCH (Srivastava et al., 2022) evaluation frameworks (Gao et al., 2021a) SentEval (Conneau and Kiela, 2018) USEB (Wang et al., 2021) BEIR 基准 (Thakur et al., 2021) MTEBSemEval：要求嵌入向量是“几何接近”的。并且单个SemEval数据集的表达能力有限。SentEval：聚合了多个STS数据集。专注于在嵌入之上微调分类器。它缺乏检索或聚类等任务，其中嵌入是在没有额外分类器的情况下直接比较的。2018年提出的不适用于基于Transformer的文本嵌入。USEB：主要由重新排序任务组成。它不涵盖检索或分类等任务。BEIR 基准：评估零样本信息检索嵌入的标准。MTEB ：将来自不同嵌入任务的数据集统一到一个通用、可访问的评估框架中。$MTEB =SemEval (STS11 - STS22) + BEIR +来自各种任务的各种其他数据集$2.2 Embedding ModelsGlove (Pennington et al., 2014) 等文本嵌入模型缺乏上下文感知，因此通常被标记为词嵌入模型。简单地说，就是Glove这样的模型只能生成每个词的固定的向量表示，而不考虑词在不同的句子中的含义和用法。例如，词“bank”在“river bank”和“bank account”中的意思是不同的，但Glove会给它们相同的向量。而且，Glove这样的模型不能直接生成句子的向量表示，而是需要对句子中的所有词的向量进行平均，这样会损失很多信息。例如，句子“I love you”和“You love me”的平均向量是一样的，但它们的语义是不同的。​\t\t词嵌入模型：$1hidden layer+1pooler layer$​\t\tpooler layer是为了固定输出向量的长度Transformers (Vaswani et al., 2017) 通过 self-attention 将上下文感知注入到语言模型中。BERT (Devlin et al., 2018) 使用 Transformer 架构并执行大规模的自我监督预训练。BERT产生的模型可以直接用于通过平均操作生成文本嵌入，就像Glove一样，是指在一些任务中，可以不用对BERT进行微调，而是直接使用预训练的模型，对输入的文本进行编码，然后对每个词的向量进行平均，得到一个文本级别的向量表示。BERT后面可以接一个池化层生成文本嵌入，但这里说的平均操作并不是指池化层，而是指对每个词的向量进行简单的算术平均，得到一个文本级别的向量。在InferSent（Conneau等人，2017）的基础上，SBERT（Reimers和Gurevych，2019）证明了对转换器进行额外的微调以获得有竞争力的嵌入性能是有益的。最近的微调嵌入模型使用对比损失目标对正文本对和负文本对进行监督微调（Gao等人，2021b；王等人，2021；Ni等人，2021b；Muennighoff，2022）。由于有大量可用的预训练转换器（Wolf等人，2020），至少有同样多的潜在文本嵌入模型有待探索。这导致了对哪种模型为从业者的嵌入用例提供了最佳性能的困惑。我们在 MTEB 上对词嵌入和Transformer-base模型进行了基准测试，量化了通常要慢得多的上下文感知模型所带来的收益。结论1：没有单一最好方案，不同模型适配不同任务。不同模型的适用任务/不适用任务。语义-文本相似性（STS）任务，该任务要求模型嵌入具有几何紧密嵌入的相似句子。SentEval（不支持Transformers）专注于在嵌入之上微调分类器。它缺乏像检索或聚类这样的任务，在这些任务中，嵌入可以在没有额外分类器的情况下直接进行比较。由于STS基准测试的不足，引入了USEB（Wang et al.，2021），主要由重新排序任务组成。因此，它不包括检索或分类等任务。BEIR基准（Thakur et al.，2021）已成为零样本信息检索嵌入评估的标准。需求： 多样性：MTEB 旨在提供对嵌入模型在各种用例中的可用性的理解。该基准包含 8 个不同的任务，每个任务最多 15 个数据集。在 MTEB 的 58 个总数据集中，10 个是多语言的，涵盖 112 种不同的语言。包含句子级和段落级数据集，以对比短文本和长文本的性能。 简单性：MTEB 提供了一个简单的 API，用于插入任何给定文本列表的模型可以为具有一致形状的每个列表项生成一个向量。这使得可以对一组不同的模型进行基准测试成为可能。 可扩展性：现有任务的新数据集可以通过指定任务的单个文件和上传数据的 Hugging Face 数据集名称在 MTEB 中进行基准测试（Lhoest 等人，2021 年）。新任务需要实施任务界面来加载数据和评估器进行基准测试。我们通过拉取请求来欢迎来自社区的数据集、任务或度量贡献，以继续开发 MTEB。 再现性：通过在数据集和软件级别进行版本化，我们的目标是使在 MTEB 中重现结果变得容易。与 MTEB 基准一起提供了与本文中提供的所有结果相对应的 JSON 文件数据集长度上分类：Sentence to sentence (S2S)：包括 文本 语义相似度 (STS)Paragraph to paragraph (P2P)：包括 聚类任务（构建为了S2S和P2P两个版本）MTEB对输入长度没有限制，必要时由模型截断。几个集群任务被框定为S2S和P2P任务。前者只比较标题，后者包括标题和内容。例如，对于ArxivClustering，摘要被连接到P2P设置中的标题。Sentence to paragraph (S2P)：包括 检索任务 这里的查询是一个句子，而文档是由多个句子组成的长段落。工作2：每个数据集取100个样本，用每个模型生成embedding，计算句表征之间的余弦相似度，作为数据集和数据集之间相似度的表征。图 2 展示了 56 个 MTEB 数据集的相似性。有几个数据集依赖于相同的语料库，如 ClimateFEVER 和 FEVER，结果得分为 1。同一数据集的 S2S 和 P2P 变体也趋于相似。科学数据集，如 SciDocsRR、SciFact 和 ArxivClustering，即使来自不同的任务（本例中为重新排序、检索和聚类），相互之间也显示出很高的相似性。一些结论除了MSMARCO数据集，其他的都是在test集上进行评估，dev集的使用方式参考Thakur et al. (2021)。按照自监督和监督方法对模型进行分类。图3：MTEB 性能随型号大小而变化。最小的 SGPT 变体性能低于类似大小的 GTR 和 ST5 变体。这可能是由于 SGPT 采用了只对偏置进行微调的方法，只有当模型规模增大、偏置参数数量增加时，这种方法才能赶上完全微调（Muennighoff，2022 年）。 自监督方法：（a）：基于Transformer的 BERT（Devlin 等人，2018 年）使用自监督掩码和句子预测任务进行训练。通过取序列长度的平均值（平均池化），该模型可直接用于生成文本嵌入。SimCSE-Unsup（Gao 等人，2021b）使用 BERT 作为基础，并执行额外的自我监督训练： Komninos（Komninos 和 Manandhar，2016 年）和 Glove（Pennington 等人，2014 年）是两个直接将单词映射到向量的单词嵌入模型。因此，它们的嵌入缺乏上下文意识，但却能显著提高速度。 监督方法：原始Transformer模型（Vaswani 等人，2017 年）由编码器和解码器网络组成。后续的Transformer器通常只训练编码器，如 BERT（Devlin 等人，2018 年）或解码器，如 GPT（Radford 等人，2019 年）。（a） Transformer-编码器方法：GTR（Ni et al.，2021b）和ST5（Ni et al，2021a）基于T5模型的编码器部分（Raffel et al.，2020），仅在微调数据集上有所不同。在额外的自我监督训练后，ST5对NLI进行对比微调以适应STS任务。同时，GTR对MSMARCO进行了微调，并专注于检索任务。MPNet和MiniLM对应于预训练的MPNet和MiniLM模型的微调嵌入模型，它们使用不同的数据集来针对任何嵌入用例。（b）Transformer-解码器方法：SGPT BiEncoders使用加权均值池对小于 0.1% 的预训练参数进行对比微调。与 ST5 和 GTR 相似，SGPT-nli 模型面向 STS，而 SGPT-msmarco 模型面向检索。SGPT-msmarco 模型用不同的特殊标记嵌入检索查询和文档，以帮助模型区分它们的作用。对于非检索任务，我们使用其查询表示法。我们对基于 GPT-NeoX、GPTJ和 BLOOM的公开 SGPT 模型进行了基准测试。另外，cpt-text将预先训练好的 GPT 解码器通过一个两阶段的过程，使用最后一个标记池来提供解码器的嵌入。我们通过 OpenAI Embeddings API 对它们的模型进行了基准测试。（c）非Transformer ：LASER是我们唯一的上下文感知非变换器模型，它依赖于 LSTM。与 LaBSE 类似，该模型也是在并行数据上进行训练，并侧重于 bitxt 挖掘应用。 自监督和监督模型之间存在巨大差距。自监督的大语言模型可以缩小这个差距，但仍然需要有监督的微调才能获得有竞争力的嵌入。 嵌入的性能与模型大小密切相关，如图3。 大多数MTEB任务中表现好的模型是数十亿参数模型。① 结论-分类任务 ST5模型综合最优，ST5-XXL的平均性能最高，高于非ST5模型中最佳模型Ada Similarity 3%。② 结论-聚类任务 MPNet最优，与ST5-XXL相当。（其实MPNet比ST5-XXL小50倍，可能是因为MPNet已经在各种数据集上微调过了） 聚类需要大量的嵌入之间的coherent distances。 像 SimCSE-sup 或 SGPTnli 这样的模型只在单一数据集 NLI 上进行过微调，当遇到微调过程中未见过的主题时，可能会产生不一致的嵌入。 SGPTmsmarco 和 Ada Search 端点的查询嵌入分别与 SGPT-nli 和 Ada Similarity 端点具有竞争力。 虽然 OpenAI 文档建议在聚类用例中使用相似性嵌入6，但在某些情况下，检索查询嵌入可能是更好的选择。③ 结论-对分类 GTR-XL和GTR-XXL具有最强的性能。配对分类在其框架上最接近STS，但模型在两个任务上的排名明显不同。④ 结论-重排序 MPNet和MiniLM模型在重新排序任务中表现出色。（在SciDocsRR（Cohan et al.，2020a）上，它们的表现远好于更大的模型，这可能是因为SciDocsRR的部分内容包含在它们的训练数据中。） 我们的实验规模和模型预训练规模使得控制数据污染具有挑战性。因此，我们在MTEB得分中忽略了MTEB数据集与模型训练数据集的重叠。只要对足够多的数据集进行平均，我们认为这些影响是微不足道的。⑤ 结论-检索 SGPT-5.8B-msmarco是MTEB中BEIR子集以及完整BEIR基准上的最佳嵌入模型 使用BLOOM的更大的7.1B SGPT模型（Scao et al.，2022）表现明显较弱，这可能是由于BLOOM的多语言性。 面向STS的模型（SimCSE、ST5、SGPTnli）在检索任务中表现不佳。 检索任务的独特之处在于有两种不同类型的文本：查询和文档（“不对称”），而其他任务只有一种类型的文本（“对称”）。⑥ 结论-STS&amp;摘要 检索模型（GTR、SGPT-msmarco）在STS上的性能较差，而ST5-XXL的性能最高。这突出了该领域在检索（不对称）和相似性（对称）用例中分为独立嵌入模型的分歧（Muennighoff，2022）。⑦ 速度和性能最佳平衡者-MPNet不同嵌入模型的性能、速度和生成嵌入结果的大小（圆圈大小）。每个示例的嵌入大小从 1.2 kB（Glove / Komninos）到 16.4 kB（SGPT-5.8B）不等。在 STS15 上使用 1x Nvidia A100 80GB 和 CUDA 11.6 对速度进行了基准测试。我们在图 4 中研究了模型的延迟-性能权衡。该图允许在模型选择过程中大量删除候选模型。它将模型选择减少到三个群组： 最高速度 最高性能 速度和性能⑧ 中文上，SGPT最优，MPNet最具性价比（分类、STS任务）图5：MTEB 多语言性能。LaBSE 在 Bitext 挖掘方面占主导地位，而分类和 STS 结果则参差不齐。SGPT-BLOOM-7B1-msmarco 在对 BLOOM 进行预训练的语言（如中文、法语和葡萄牙语）上往往表现出色。MTEB 包含 10 个多语言数据集，涉及咬文挖掘、分类和 STS 任务。我们在图 5 中研究了这些数据集的性能。表格结果见表 12、表 13 和表 14。⑨图6：模型和（8大）任务之间的皮尔逊相关性。左图：同一结构的尺寸变体显示出高度相关性。右图 聚类和重排的性能相关性最强，而摘要和分类与其他任务的相关性较弱。图6展示了不同的文本嵌入模型在不同的嵌入任务上的皮尔逊相关性，即模型的性能与任务的难度之间的线性关系1。图6的皮尔逊相关性是这样计算的：首先，对于每个嵌入任务，计算所有模型的平均性能，作为任务的难度指标1。然后，对于每个模型，计算它在所有嵌入任务上的性能，作为模型的能力指标1。最后，对于每个模型，计算它的能力指标与任务的难度指标之间的皮尔逊相关系数，作为模型和任务的皮尔逊相关性1。图6的目的是展示不同的文本嵌入模型在不同的嵌入任务上的一致性，即模型是否能够在不同难度的任务上保持稳定的性能1。图6的结果表明，没有哪一种文本嵌入方法在所有任务上都占据优势，这意味着文本嵌入领域还没有找到一种通用的方法，能够在所有嵌入任务上提供最佳的结果12。本Benchmark局限性： 缺乏文档级数据：MTEB 涵盖多个文本长度（S2S、P2P、S2P），但仍然缺少非常长的文档。MTEB 中最长的数据集有几百个单词，较长的文本大小可能与检索等用例相关。 任务不平衡：MTEB 中的任务有不同数量的数据集，摘要仅包含单个数据集。这意味着在所有数据集上计算的 MTEB 平均分数偏向于具有许多数据集的任务，特别是检索、分类和聚类。随着 MTEB 的增长，我们希望为当前代表性不足的任务添加更多数据集，例如摘要或配对分类。 多语言：MTEB包含多语言分类、STS和文本挖掘数据集。然而，检索和聚类只支持英语。SGPT-BLOOM-7B1-msmarco面向多语言检索数据集，由于缺乏这些数据集，因此无法在MTEB中进行全面的基准测试。 额外的方法：文本嵌入通常用作下游模型的输入特征，例如在我们的分类任务中。这可能涉及其他形式，特别是图像内容。我们只专注于自然语言应用，并将文本嵌入的广泛基准测试作为未来工作的其他模式的输入。工作1：先找到当前最优模型 (https://huggingface.co/datasets/sentence-transformers/reddit-title-body) &#8617; (https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_title_body_jsonl) &#8617; (https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) &#8617; (https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification) &#8617; (https://www.kaggle.com/competitions/tweet-sentiment-extraction) &#8617; " }, { "title": "Mamba", "url": "/posts/Mamba/", "categories": "NLP, 模型研究", "tags": "nlp, Mamba, 模型研究", "date": "2023-12-05 00:00:00 +0800", "snippet": "Mamba：Transformer的挑战者Mamba: Linear-Time Sequence Modeling with Selective State SpacesMamba：具有选择性状态空间的线性时间序列建模摘要：目前深度学习的应用都是基于 Transformer架构（及其注意力模块）。Transformer的一个致命问题是不善于长序列计算。有许多subquadratic-time architectures（二次时间架构），例如：线性注意力、门控卷积、循环模型、结构化状态空间模型（SSM）。这些模型在语言等重要模态上的表现都不如注意力机制。我们发现这类模型的一个关键弱点是它们无法执行基于内容的推理，并针对这个弱点进行了改进。具体，首先简单地让SSM参数是func(input)，就可以用离散模态来解决它们地弱点，允许模型根据当前token沿着序列长度维度选择性地传播/忘记信息。其次，尽管这种变化阻止了高效卷积的使用，但我们在递归模式下设计了一种硬件感知的并行算法。我们将这些选择性SSM集成到一个简化的端到端神经网络架构中，而无需注意力，甚至无需MLP块（Mamba）。（以下为表现）Mamba具有快速推理（比Transformers高5倍的吞吐量）和序列长度的线性缩放，其性能在高达百万长度的真实数据序列上得到了提高。作为通用序列模型的主干，Mamba在语言、音频和基因组学等多种模式中实现了最先进的性能。在语言建模方面，我们的Mamba-3B模型在预训练和下游评估方面都优于相同大小的Transformers，并与两倍于其大小的Transformer相匹配。" }, { "title": "RAG", "url": "/posts/RAG%E6%A3%80%E7%B4%A2%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA/", "categories": "NLP, 基础知识", "tags": "nlp, RAG, 技术综述", "date": "2023-12-04 00:00:00 +0800", "snippet": "RAG调研一、一些共识 External knowledge is the key to resolving the problems of LLMs such as hallucination and outdated knowledge, which can make LLMs generate more accurate and reliable responses through retrieval-augmented generation (RAG). However, LLMs cannot always response as expected with RAG. For one thing, there are numerous irrelevant documents and false information on the Internet. Incorporating these external documents into LLMs could have a detrimental effect. For anthoer, LLMs suffer from the unreliable generation challenge. The generation of LLMs is often unpredictable, and we cannot guarantee that they will utilize the useful information entailed in the external documents. Additionally, LLMs can easily be misled by incorrect information in the document. To this end, we build RetrievalAugmented Generation Benchmark (RGB) to evaluate the retrieval-augmented generation of LLMs, and we concern about 4 specific abilities:[^Benchmarking Large Language Models in Retrieval-Augmented Generation]外部知识是解决幻觉和过时知识等llm问题的关键，它可以通过检索增强生成(RAG)使llm产生更准确、更可靠的响应。然而，⭐LLM 并不总是像 RAG 预期的那样响应。⭐将这些不相关的或者虚假的外部文档合并到 LLM 中可能会产生不利影响。⭐llm的生成通常是不可预测的，我们不能保证它们将利用外部文档中包含的有用信息。⭐LLM 很容易被文档中不正确的信息误导。 In real-world scenarios, it is not possible to obtain perfect documents with all the necessary external knowledge. Therefore, evaluating these four abilities of the model becomes essential in order to measure the RAG of LLMs.[^Benchmarking Large Language Models in Retrieval-Augmented Generation]⭐在现实世界的场景中，不可能获得具有所有必要的外部知识的完美文档。因此需要能够评估模型的这四个能力。二、论文调研1. Benchmarking Large Language Models in Retrieval-Augmented Generation[^Benchmarking Large Language Models in Retrieval-Augmented Generation]---中科院计算所，23年9月4日摘要：检索增强生成 (RAG) 是一种减轻大型语言模型 (LLM) 幻觉的有前途的方法。然而，现有的研究缺乏对检索增强生成对不同大型语言模型的影响的严格评估，这使得识别RAG对不同llm能力的潜在瓶颈具有挑战性。在本文中，我们系统地调查了 Retrieval-Augmented Generation 对大型语言模型的影响。我们分析了不同大型语言模型在RAG所需的4个基本能力下的性能，包括噪声鲁棒性、负拒绝、信息集成和反事实鲁棒性。为此，我们建立了 Retrieval-Augmented Generation Benchmark (RGB)，这是一个用于英文和中文 RAG 评估的新语料库。RGB 根据解决案例所需的上述基本能力，将基准内的实例划分为 4 个单独的测试平台。然后我们在 RGB 上评估 6 个具有代表性的 LLM，以诊断应用 RAG 时当前 LLM 的挑战。评估表明，虽然 LLM 表现出一定程度的噪声鲁棒性，但它们在负拒绝、信息集成和处理虚假信息方面仍然会遇到重大困难。上述评估结果表明，在有效地将RAG应用于LLM之前，仍有相当大的旅程。Introduction：🌱为什么要研究RAG模型？ Recently, there have been impressive advancements in large language models (LLMs) like ChatGPT (OpenAI 2022), LLaMA-2 (Touvron et al. 2023), and ChatGLM (THUDM 2023a). Although these models have shown remarkable general abilities (Bang et al. 2023; Guo et al. 2023), they still suffer severely from challenges including factual hallucination (Cao et al. 2020; Raunak, Menezes, and JunczysDowmunt 2021; Ji et al. 2023), knowledge out-dating (He, Zhang, and Roth 2022), and the lack of domain-specific expertise (Li et al. 2023c; Shen et al. 2023).ChatGPT、LLaMA-2、ChatGLM等大模型虽然有优秀的通用能力，但是存在一些问题，①factual hallucination事实幻觉；②knowledge out-dating知识过时；③domain-specific expertise特定领域的专业知识。 With the help of external knowledge, LLMs can generate more accurate and reliable responses. The most common method is to use a search engine as a retriever such as New Bing.LLMs+RAG的一个典型应用就是New Bing。 On the other hand, LLMs suffer from unreliable generation challenge. LLMs can be misled by incorrect information contained in the context (Bian et al. 2023) and also suffer from hallucination during the generation (Adlakha et al. 2023), resulting in generating content that goes beyond external information.RAG也会给LLMs带来负面影响，比如①互联网中存在虚假信息，②LLM可能会被上下文中包含的错误信息误导，③LLM在生成过程中存在幻觉，会生成超出外部信息的内容。 Unfortunately, currently there lacks of comprehensive understanding on how these factors can influence RAG, and how could each model survives from these drawbacks and improvement their performance via information retrieval.不幸的是，目前对这些因素如何影响RAG，以及每个模型如何从这些缺陷中幸存下来并通过信息检索提高其性能缺乏全面的了解。因此，迫切需要对LLM进行全面评估，评估其有效利用检索到的信息的能力，以及抵御信息检索中存在的各种缺点的能力。💡为了确保LLM的内部知识不会在评估结果中引入偏差，RGB选择聚合最新的新闻信息，并基于新闻信息构建查询。然后，基于这些查询，我们使用搜索API获取相关文档，并从内容中选择最相关的片段作为外部检索文档。最后，基于查询和文档集对的不同组成，我们扩展语料库，并将其划分为4个测试平台，根据RAG中常见的挑战来评估LLM的以下基本能力，如图1所示： 噪声鲁棒性Noise Robustness：“从嘈杂文档中提取有用信息”的能力。 本文中，我们将【嘈杂的文档】定义为“与问题相关的文档，但不包含任何答案信息。” 噪声鲁棒性平台： ​\t所有相关外部文档=噪声文档+包含答案信息的文档 ​\t噪声文档=噪声比*所有相关外部文档 负拒绝Negative Rejection：“拒绝回答无答案查询”的能力 无答案查询：所需知识不存在于任何检索到的文档中。此情况，LLM应给出“信息不足”或其他拒绝信号。 负拒绝平台： ​\t外部文档=噪声文档 信息整合Information Integration：“回答关联多个文档的复杂问题”的能力 信息整合平台： ​\t查询=只能用多个文档才能回答的实例 ​\t外部文档=多个包含答案信息的文档+噪声文档 反事实鲁棒性Counterfactual Robustness：“通过指令提示LLMs’警告：检索到的信息存在潜在风险‘时，能够识别检索到的文档中的已知事实错误”的能力 反事实稳健性平台： ​\tLLM已知的知识，即可以直接回答的query。 ​\t外部文档=存在事实错误的文档 请注意，我们只评估 LLM 通过指令对检索到的信息中潜在风险的警告的情况。 测评模型： ChatGPT ChatGLM-6B ChatGLM2-6B Vicuna-7b Qwen-7B-Chat BELLE-7B结论： We found that even though RAG can improve the response accuracy of LLMs, they still suffer from the abovementioned challenges significantly. Specifically, we found that even though LLMs demonstrate some level of noise robustness, they tend to confuse similar information and frequently generate inaccurate answers when relevant information exists.尽管RAG可以提高llm的响应精度，但它们仍然受到上述挑战的显著影响。具体来说， 我们发现，尽管 LLM 展示了一定程度的噪声鲁棒性，但当相关信息存在时，它们往往会混淆相似的信息并经常生成不准确的答案。例如，当面对有关 2022 年诺贝尔文学奖的问题时，如果有关外部文件文献中 2021 年诺贝尔奖的嘈杂文档，LLM 可能会混淆并提供不准确的答案。 此外，当没有一个外部文档包含相关信息时，LLM 经常无法拒绝回答并生成不正确的答案。 此外，LLM 缺乏从多个文档中总结的能力，因此如果需要多个文档来回答问题，LLM 通常无法提供准确的答案。 最后，我们发现，即使llm包含所需的知识，并通过指令对检索到的信息中潜在风险的警告，它们仍然倾向于信任和优先考虑检索到的信息而不是他们自己的现有知识。Retrival-Augmented Generation Benchmark数据集构建过程：News Collection–真实的Step 0：收集最新的新闻文章Step 1：QA实例生成（QA instances generation）：使用prompt让ChatGPT为每篇文章生成事件（Related event）、问题（Question）和答案（Key information）。这一步能够顺便初步过滤出不包含任何事件的新闻文章。Step 2：人工检查调整、过滤难以通过搜索引擎检索的数据。Step 3：使用搜索引擎API检索（Retrieve using search engine）：对于每个Query使用谷歌 API获取10个相关网页，并提取相应的文本片段。Step 4：阅读这些网页，并将其文本内容转换为最大长度为300个token的文本块。使用现有的密集检索模型选择与查询最有效匹配的前30个文本块。这些检索到的文本块，以及搜索API提供的片段，将用作我们的外部文档。Step 5：扩展语料库，为4种能力构建Testbeds（试验台）。​\t对于噪声鲁棒性，根据所需的噪声比率对不同数量的负面文档进行采样。​\t对于负样本拒绝，所有的外部分档都是从负文档中采样的。​\t对于信息整合，基于前面生成的问题构造复杂问题。这涉及到扩展或重写这些问题，使它们的答案包含多个方面。例如，“谁获得了2023年超级碗的MVP？”这个问题可以改写为“谁赢得了2022年和2023年的超级碗MVP。因此，回答这样的问题需要利用来自各种文件的信息。​\t对于反事实稳健性，反事实稳健性数据完全基于模型的内部知识构建，也就是说让模型自动生成已知的问题和答案，例如，基于“谁获得了 2022 年诺贝尔生理学和医学奖？”的问题，该模型将生成已知问题“谁获得了 2021 年诺贝尔文学奖？”并回答“Abdulrazak Gurnah”。然后人工验证生成的答案，并检索相关文档，为了使文档包含事实错误，我们人工修改答案并替换文档中的相应部分。 密集检索模型: for English：https://huggingface.co/sentence-transformers/all-mpnet-basev2 for Chinese：https://huggingface.co/moka-ai/m3e-baseRGB统计数据：共600个基本问题，200个信息整合能力的附加问题，200个反事实稳健性能力的附加问题。所有问题，一半中文，一半英文。评价指标：该基准的核心是评估 LLM 是否可以利用提供的外部文档来获取知识并生成合理的答案。 Accuracy：for 噪声鲁棒性、信息整合 ​\t精确匹配-如果生成的文本包含与答案的精确匹配，则视为回答正确。 Rejection rate：for 负样本拒绝 ​\t当只提供嘈杂的文档时，LLM应输出具体内容—— “I can not answer the question because of the insufficient information in documents.”（“由于文档中的信息不足，我无法回答问题。”）如果模型生成此内容，则表示成功拒绝。 ​\tPS.使用说明提示模型。 Error detection rate：for 反事实鲁棒性（衡量模型是否能检测出文档中的事实错误） ​\t当提供的文档包含事实错误时，模型应该输出特定的内容-“There are factual errors in the provided documents.”（“提供的文档中存在事实错误”。）如果模型生成该内容，则表明模型在文档中检测到错误信息。 ​\tPS.使用说明提示模型。 Error correction rate：for 反事实鲁棒性（衡量模型是否能在识别到事实错误之后仍能提供正确的答案） ​\t如果该模型生成正确的答案，则表明模型能够 修正 文档中的事实错误。\\[ACC=\\frac{\\#tt}{\\#nums}\\] #tt：正确的response数量 #nums：所有待评估的❓实例数量 考虑到模型可能无法完全遵守指令，对于拒绝率和错误检测率，我们还使用ChatGPT对答案进行额外评估。具体而言，❓我们通过使用指导（instructions）和演示（demonstrations）来评估模型的响应，以确定它们是否能够反映文件中没有的信息或识别任何事实错误。ExperimentsSettings由于上下文限制，我们为每个问题提供5个外部文档。在我们关于噪声鲁棒性的实验中，我们评估了噪声比在0到0.8之间的场景。为了全面评估整体能力，我们对每种语言都采用了统一的指导（instructions），如图3所示。实验是使用NVIDIA GeForce RTX 3090进行的。include a system instruction followed by a user input instructionResults on Noise Robustness针对 噪声鲁棒性 实验：结论： RAG能有效改善LLMs的响应。 即使在存在噪声的情况下，llm也表现出了很强的性能，这表明RAG是llm产生准确可靠响应的一种有希望的方法。 噪声率的不断提高对llm中的RAG提出了挑战。 具体来说，当噪声比超过80%时，模型的精度明显下降。例如，ChatGPT的性能从96.33%下降到76.00%，而ChatGLM2-6B的性能从91.33%下降到57.33%。 深入分析这个实验中的模型答错的答案，发现错误通常源于3个原因：给出了噪声鲁棒性的误差案例，并且只给出了一个正面文档和一个负面文档。响应由ChatGLM2-6B生成。蓝色文本表示文档与问题或答案之间匹配的部分，而红色文本突出显示不匹配的部分。 长距离信息 当外部文档中“与问题相关的信息”与“与答案相关的信息”相距甚远时，模型难以识别正确答案。这种情况中，大部分是“与问题相关的信息”首先出现在文档的开头，在下文中使用代词指代它。这种情况出现时，模型会去依赖其他文档的信息，产生错误的印象（幻觉）。 如上图中卡塔尔公开赛只在开头出现了一次，与答案文本Anett Kontaveit人名相距甚远。 证据的不确定性 在备受关注的事件发生前，人们倾向于预测、猜测、预言它。尽管这样的预测文档明确指出这是不确定或推测性的内容，但它们仍然会影响LLM的检索增强生成。 如上图中，错误文档中的内容都是关于苹果新耳机的预测（Apple Reality Pro），并且存在有正确答案的文档（Vision Pro），模型仍然被误导了。 概念混淆 外部文档中的概念可能与问题中的概念相似，但又不同。这会让LLM混淆，并生成不正确的答案。 如上图中，模型对问题的理解集中在”汽车收入“的概念上（特斯拉，收入-&gt;通常提到特斯拉都是谈论特斯拉汽车-&gt;特斯拉，汽车收入），但问题问的是特斯拉这个公司一季度全部“收入”。 通过上述分析，LLM需要进一步详细的增强，比如长文档建模、精确的概念理解Results on Negative Rejection testbed针对 负拒绝 实验：（只提供负样本，看模型的拒绝率）采用精确匹配评估拒绝率（Rej），利用ChatGPT来确定LLM的response是否包含拒绝信息（Rej*）。结论： 负拒绝对LLM中的RAG提出了挑战。 中文/英文LLM的最高拒绝率最高43.33%/45%。 通过比较Rej和Rej*，发现LLM不能严格遵循指令，且经常产生不可预测的response，因此LLM很难直接用来识别负样本并判定拒绝。 2个原因： 证据不确定性 虽然文件中只提到与 “亚当-麦凯 “的联系，并没有明确指出他是这部电影的导演，但模型仍然断定他担任了这一角色。 概念混淆 答案中提供的信息与问题中提到的“2022 年冬季奥运会”而不是“2022 年奥运会”。与直接回答相比，检索增强生成对负拒绝提出了更大的挑战，因为它提供了可能误导 LLM 并导致错误响应的相关文档。 Results on Information Integration testbed针对 信息整合 实验：不同噪声率下，评估模型准确率。结论： 信息整合 对于LLMs中的RAG来说是一个大挑战（LLM难以有效地整合信息，不太适合直接回答复杂的问题。） 即使噪声率=0，英文/中文中最高也只有60%/67%的准确率 添加噪声文档后，英文/中文下降到43%/55% 对于文档嘈杂的RAG来说，复杂的问题更具挑战性。 对比 噪声鲁棒性 实验（简单问题，添加噪声文档），如下图。简单问题时，噪声比达到0.8时，性能才显著下降，而复杂问题时，噪声比=0.4时，性能猛降。 这表明复杂的问题更容易受到噪声的干扰。我们推测，这是因为解决复杂问题需要整合来自多个文档的信息，而这些信息可以被视为彼此的噪声，使模型更难从文档中提取相关信息。 4个原因：（这里分析的是噪声率-0的ChatGLM2-6B的错误数据） 38%的错误实例属于 噪声稳健性 实验中发现的3个错误原因 合并错误（占比28%）： 模型试图合并两个子问题的答案，错误地使用一个问题的答案直接回答两个子问题，然后忽略另一个子问题相关的所有文档。 例如：模型回答D 组是法国队和德国队的世界杯小组，而实际上德国队被分到了 E 组。 忽视错误（28%） 模型只回答一个问题，直接忽略另一个问题。原因是模型对问题缺乏全面的理解，没有认识到问题由多个子问题组成。 例如：模型只提供了 2022 年超级碗 MVP 的答案，而没有考虑 2023 年。 错位误差（6%） 模型把子问题1的文档误识别为子问题2的问答，导致答案错位。 例如：模型仅提到了 2023（95）学院奖的最佳图片，完全忽略了 2022 年奖项。此外，它错误地指出，“CODA”是 2023 的最佳图片，当它实际上被授予 2022 年的最佳图片时。 上述错误主要是由于对复杂问题的理解有限，这阻碍了有效利用来自不同子问题的信息的能力。关键是提高模型的推理能力。一种可能的解决方案是使用思维链方法来分解复杂问题（Zhou et al. 2023a; Xu et al. 2023b; Drozdov et al. 2023）。然而，这些方法会减慢推理速度，无法提供及时的响应。Results on Counterfactual Robustness testbed针对 反事实鲁棒性 实验：（考察模型对于已知事实知识的问题，包含事实错误的噪声文档能否干扰他的思考。因此只考虑准确率超过 70% 的 LLM。）4个指标：①不包含任何文档的准确率、②包含反事实文档的准确率、③错误识别率、④错误纠正率采用精确匹配评估错误识别率（ED），利用ChatGPT来识别LLM的response是否包含反事实错误信息（ED*）。 ChatGPT-en在错误纠正率（CR）方面表现最好，达到了57.14%，远高于其他两种模型。 所有的模型在有反事实文档时的准确率（ACCdoc）都比没有外部文档时的准确率（Acc）下降了很多，说明反事实文档对LLMs的生成能力有很大的干扰。 Qwen-7B-Chat-zh在错误检测率（ED）方面表现优于ChatGPT-zh，但在错误纠正率（CR）方面不如ChatGPT-zh，说明它能够发现错误，但不能有效地修正错误。原因分析： LLMs过于依赖检索到的文档，而不是利用自身的知识。这导致LLMs容易被含有错误事实的文档所误导，从而产生不准确的回答。 检索增强生成不是为自动解决给定上下文中的事实错误而设计的，因为这与模型缺乏知识并且依赖于检索到的文档以获取附加信息的基本假设相矛盾。 检索增强生成（RAG）是一种让大型语言模型（LLMs）在生成内容时，可以从外部知识源获取额外信息的技术。 RAG的一个基本假设是，LLMs缺乏知识，需要依靠检索到的文档来补充信息。也就是说，RAG认为检索到的文档是可靠的，可以帮助LLMs生成更准确的答案。 但是，如果检索到的文档中包含了错误的事实，也就是与真实情况相悖的信息，那么RAG就会出现问题。因为RAG没有设计自动纠正文档中的错误事实的能力，它会盲目地信任检索到的文档，从而生成错误的答案。 这就是为什么RAG并不适合解决反事实鲁棒性的问题。反事实鲁棒性是指LLMs在处理与事实相悖的外部文档时，能否正确地检测和纠正错误的能力。这是一种重要的信息整合能力，对于实际应用中的可靠性和安全性至关重要。 然而，由于互联网上假新闻泛滥，这个问题在实际应用中至关重要。现有的 LLM 不具备处理因错误信息造成的不准确回复的保障措施。事实上，它们在很大程度上依赖于检索到的信息。即使 LLM 包含有关问题的内部知识，它们也经常相信检索到的虚假信息。这对未来在 LLMs 中发展 RAG 提出了重大挑战。&lt;font color=#008B8B&gt;🤹个人总结&lt;/font&gt;2. Active Retrieval Augmented Generation[^Active Retrieval Augmented Generation]---EMNLP 2023，卡耐基梅隆大学，23年8月摘要： Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution.尽管大型语言模型 (LM) 具有理解和生成语言的显着能力，但它们倾向于产生幻觉并创建事实不准确的输出。通过从外部知识资源中检索信息来增强 LM 是一种很有前途的解决方案。 Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential.大多数现有的检索增强LMs采用检索-生成设置，仅根据输入检索一次信息。然而，在涉及生成长文本的更一般的场景中，这是有局限的，在生成过程中不断收集信息是必不可少的。 In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We proposeForward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.在这项工作中，我们提供了主动检索增强生成的广义视图，这些方法在生成过程中主动决定何时检索和检索什么。我们提出了前瞻性主动检索增强生成(FLARE)，这是一种通用方法，它迭代地使用对即将到来的句子的预测来预测未来的内容，然后将其用作检索相关文档的查询，以便在包含低置信度令牌的情况下重新生成句子。实验是在4个长篇知识密集型生成任务上验证的效果。Introduction：LLM→幻觉 Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form outputLLM在更复杂的任务中保有优秀的产生长序输出的能力。这些复杂任务（long-form输出任务）有：long-form QA，open-domain summarization，chain-of-thought（CoT）reasoning。RAG+LM/LLM：检索+生成 → 文本长度受限/单次检索/被动注入 These single-time retrieval augmented LMs outperform purely parametric LMs, particularly for short-form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the information needs are clear in the user’s input, and it is sufficient to retrieve relevant knowledge once solely based on the input.对于short-form的知识密集型生成任务，比如事实性问答（factoid question answering）任务，这类任务是单次的，这种情况下，信息需求在用户的输入时就明确了，所以仅基于输入执行单次检索（相关知识）就可以支撑。short-form &amp; long-form In contrast to short-form generation, long-form generation presents complex information needs that are not always evident from the input alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowledge throughout the generation process.与short-form生成相比，long-form生成需要复杂的信息，而这些信息并不总是仅从输入中就能看出来。与人类在创作论文、散文或书籍等内容时逐步收集信息的方式类似，使用 LM 的long-form生成需要在整个生成过程中收集多种知识。FLARE的idea例如：要生成某个特定主题的摘要，基于主题名称的初始检索可能无法涵盖所有方面和细节。在生成过程中，如生成某个方面（如乔·拜登的教育史）或某个特定细节（例如乔·拜登的总统竞选公告日期）时，根据需要检索额外信息至关重要。具体来说，从用户输入 x 和初始检索结果 Dx 开始，FLARE 会反复生成一个临时的下一个句子（以灰色斜体显示），并检查其中是否包含低概率标记（以下划线表示）。如果是（步骤 2 和 3），系统将检索相关文档并重新生成句子。除了FLARE，还有哪些尝试？FLARE的关键思路对于这些尝试，作者提出以下这个问题： 能否创建一个简单通用的检索增强LM，在整个生成过程中主动决定何时检索以及检索什么，并适用于各种长形式的生成任务？问题1：何时检索假设-只有在缺乏所需知识的情况下才应该检索信息，以避免被动检索增强LMs中发生的不必要或不适当的检索 本文鉴于LLM通常具有良好的校准性，因此只有当出现低概率（低置信度）的时候表明LM缺乏知识，因此采用一种主动检索策略，该策略仅在LM生成低概率token时检索。问题2：检索什么假设-在考虑检索什么的时候，重要的是考虑LMs未来打算生成什么，因为主动检索的目标是优化未来的生成。 本文提出通过生成临时下一句来预测未来，将其用作检索相关文档的查询，然后根据检索到的文档重新生成下一句。FLARE的性能：&lt;font color=#008B8B&gt; FLARE无需训练：FLARE is applicable to any existing LMs at inference time without additional training.&lt;/font&gt; 考虑到GPT-3.5（Ouyang et al.，2022）在各种任务上取得的令人印象深刻的性能，我们在text-davinci-003上检验了我们的方法的有效性。模型：text-davinci-003方法：FLARE任务：* 多跳QA（2WikiMultihopQA）* 常识推理（StrategyQA）* long form QA（ASQA）* 开放域摘要（WikiAsp）结果显示：单次检索 ❌ → 多次检索 ❌ → FLARE ✅RAG的数学定义 正式定义单次检索增强生成 主动检索增强生成的定义 详解FLARE本研究的动机： LM应该只在必要的时候检索信息，避免不必要的或不适当的检索。 检索query应该能够反映后续生成的意图，即预测下一句话的内容，根据其检索相关文档。本文提出了2种前瞻性的主动检索增强生成（FLARE）方法： FLAREinstruct：在生成答案的时候，使用一些鼓励检索的指令，提示LM在必要时生成检索查询。 FLAREdirect：直接使用LM的生成作为检索查询，迭代地生成下一句话，如果出现不确定的词，就检索相关文档，重新生成下一句话。方法1：（FLAREinstruct）FLARE with Retrieval Instructions我们受Toolformer的启发，FLAREinstruct是一种直接表达检索信息需求的方法。 Toolformer：一种让语言模型（LMs）在生成文本的过程中，根据需要从外部知识资源中检索相关信息的方法。具体来说，在需要额外信息时，向query中添加”[Search(query)]”e.g.“The colors on the flag of Ghana have the following meanings. Red is for [Search(Ghana flag red meaning)] the blood of martyrs, …”当使用只提供API接口的模型（比如GPT3.5和GPT4）时，通过few-shot prompting结合这种toolformer的方法诱导模型检索必要的信息以生成回答。而FLAREinstruct：具体来说，对于下游任务，我们将与检索相关的指令和示例作为Skill 1放在开头，然后将下游任务的指令和示例作为Skill 2。当给出一个sample时，要求LM在执行任务时结合技能1和2生成检索query。提示的结构如下图：FLAREinstruct的流程如下图：如图2所示，当LM生成“[Search(query)]”(以灰色斜体显示)时，我们停止生成并使用查询条件来检索相关文档，这些文档在用户输入之前添加，以帮助将来生成，直到生成下一个搜索查询或到达结束。附加的实现细节包含在附录A中。方法2：（FLAREdirect）Direct FLARE Since we cannot fine-tune black-box LMs, we found queries generated by FLAREinstruct through retrieval instructions might not be reliable.由于不能对黑盒LM进行微调，所以用FLAREinstruct方法生成的检索查询可能不太可靠，也就是说，可能不能很好地反映用户的意图，或者不能找到最合适的文档。这就引出了FLAREdirect，作者概括这个方法是一种更直接的前瞻性主动检索方法，该方法可以使用the next sentence来决定何时以及如何检索。3.2.1 基于置信度的主动检索 每个step中（假设step t），首先生成1个临时的next sentence ，这个句子不考虑外部文档。 根据这个$\\hat{s}_t$决定是否检索、生成query $q_t$ 如果LM对于$\\hat{s}_t$很自信，那么不在额外检索。 否则，用$q_t$检索出相关文档，再重新生成the next sentence $s_t$ 也可以用段落作为迭代的basis，作者选句子，是因为它不长不短蛮合适。 Since LMs tend to be well-calibrated that low probability/confidence often indicates a lack of knowledge (Jiang et al., 2021; Kadavath et al., 2022; Varshney et al., 2022)多篇论文都发现，经过无监督预训练的大型LM，比如GPT-3，通常是比较好的校准，也就是说，它们的置信度和概率是比较一致的。* 什么是LM的置信度校准？指LM的置信度（confidence）能够反映出它的预测正确的概率（probability）。比如，如果LM对一个答案的置信度是80%，那么它的预测正确的概率也应该接近80%。如果LM的置信度和概率不一致，那么就说明LM是不校准的（miscalibrated），可能会导致一些问题，比如过于自信（overconfident）或过于谨慎（underconfident）。预训练的LM，通常 置信度≈准确率（概率）；微调后的LM / RLHF之后的LM，通常 置信度不约等于准确率（即置信度校准变差），往往是会变得“过于自信”，置信度&gt;准确率。那么，if LM的置信度低，就很可能代表它没有足够的数据或信息来支持它的responce/predict（缺乏知识）。**此时，就是LM需要外部知识的时候。**本文的设定是，if 任何1个token的概率（置信度）低于阈值$\\theta \\in [0,1]$ → 检索else →不检索如下：​\t\\(\\theta = 0:不检索\\)​\t\\(\\theta&lt;1:当句子中存在概率低于\\theta的token时，触发检索\\)​\t\\(\\theta = 1:每句话都检索\\)其中，$q_t$是基于$\\hat{s}_t$生成的。3.2.2 基于置信度的query公式 先检索后生成or先生成后纠正？方式1：使用$\\hat{s}_t$直接作为$q_t$，这种方式放在long-form生成中，就必须有主动信息访问（active information access）。 参考：使用LMs生成的假设标题或段落作为query/evidence的方法。(Gao et al., 2022; Sun et al., 2022; Yu et al., 2022; Mao et al., 2021)previous context指的是生成过程中已经生成的句子，也就是当前句子之前的内容。这篇论文的作者发现，如果只根据previous context来检索文档，那么检索到的文档可能与当前句子的主题不相关，或者与下一句的内容有冲突。因此，他们提出了使用next sentence来检索文档，也就是根据当前句子的生成概率分布，选择一个最可能的下一句作为查询。这样做的好处是可以提前预测未来的内容，从而检索到更相关的文档，同时也可以避免生成重复或无意义的句子。我们发现，与前一个上下文相比，使用下一个句子进行检索可以获得更好的结果，如后面的第6.2小节所示。弊端是，这种方式有很高的风险出现“错误传播”的问题。比如：如果LM生成句子”Joe Biden attended the University of Pennsylvania”，事实是他就读于the University of Delaware，使用这个包含事实错误的句子可能会导致后续的检索都被误导，本文提出了2个简单的方法来客服这个问题，如下图3：图中下划线是概率（置信度）较低的token。方法A：隐式query公式（Masked sentences as implicit queries.）低于阈值$\\beta$的token mask掉。这消除了句子中潜在的干扰，从而提高了检索的准确性。\\[mask(\\hat{s}_t)\\]方法B：显式query公式（Generated questions as explicit queries.）针对$\\hat{s}_t$中的低置信度的token生成明确的问题，例如，如果LM不确定“宾夕法尼亚大学”，那么像“乔·拜登上过哪所大学？”这样的问题可以帮助检索相关信息。这种方法参考Self-Ask。 Self-ask（Press et al.，2022）通过手动将后续问题插入下游任务示例来实现这一点，如稍后提示D.2所示，这需要特定任务的注释工作。本文的方法在其基础上升级，在没有额外注释下生成低置信区间的问题。Step 1：首先，提取概率低于$\\beta$的所有token组成的区间$z$，用prompt3.2这样的模板来使LM生成$q_{t,z}$，这个问题的答案刚好能落在区间$z$中:Step 2：使用每个$q_{t,z}$进行检索，对于返回的文档交错排列到1个带index的list中。\\[qgen(\\hat{s}_t)\\]总结方法A和方法B： [^ 3](https://www.microsoft.com/en-us/bing/apis/ bing-web-search-api)多次检索的Baselines目前常见的被动多次检索增强语言模型也可以采用FLAREdirect框架。针对When&amp;What去检索，本文梳理了3个baseline大类，这些baselines并不是相应论文的精确复制品，因为许多设计选择不同，因此无法进行直接比较。本文使用相同的设置来实现它们，仅控制When&amp;What to retrieve变化。Previous-window这种方法生成一些tokens就使用当前句子之前的 $l$ 个token作为query触发检索，其中 $l$ = 窗口大小。前一个窗口生成的token用于query：RETRO、IC-RALM、KNNLM都是属于这一类，前两个是每隔一些tokens触发检索，最后一个是每个token都触发检索。本文follow IC-RALM设置 $l=16$ 。 这个公式是从这篇论文的第三页的第一段引用的1。它是在介绍一种名为RETRO的方法，它是一种用于文本生成的方法，它可以根据输入和生成过程中的信息来动态地检索外部知识资源，从而提高生成质量。这个公式的意思是，将生成的文本分成长度为l的窗口，每个窗口包含l个词。yt表示第t个窗口，它是一个向量，包含从第(t−1)l+1个词到第tl个词的所有词。例如，如果l=3，那么y1=[w1,w2,w3]，y2=[w4,w5,w6]，以此类推。这样做的目的是为了方便地使用当前句子之前的l个词作为查询，来检索与未来内容相关的文档。 假设设定窗口=3，那么是不是就一定是在3的倍数的时候触发检索？ 不一定是这样的。RETRO的检索方法是每生成几个词，就使用当前句子之前的l个词作为查询，来检索与未来内容相关的文档。这里的“几个词”是一个可调节的参数，它可以根据不同的任务和数据集来设置。如果这个参数等于l，那么就相当于每生成一个窗口，就触发一次检索。如果这个参数小于l，那么就相当于每生成几个词，就触发一次检索。如果这个参数大于l，那么就相当于每生成几个窗口，就触发一次检索。这篇论文中的实验结果表明，这个参数的选择会影响生成的质量和效率，因此需要根据具体的情况来确定。Previous-sentence这种方法每个句子都触发检索，并且使用前一个句子作为query。IRCoT属于这类。Question decomposition这种方法针对于特定任务的样本，人工标注以引导语言模型生成分解的子问题、同时生成输出。比如，Self-ask属于这类，它在样本中人工插入子问题，使用的Prompt如下图。对于测试样本，每当模型生成子问题时，都会动态触发检索。这三类方法都是在生成过程中检索外部知识。他们的缺点有： 使用前面生成的结果中的token作为query可能无法反映LMs将来打算生成什么 以固定间隔的token作为query检索信息的效率可能很低，因为它很可能选择的不是合适的token 问题分解方法要求特定任务的prompt工程，限制了在新任务中的泛化性实验设置（FLARE=FLAREdirect）策略：few-shot incontext learning评估任务：4个知识密集型任务，每个数据集抽样500个样本。如表7follow的工作：Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.超参数：根据dev集选择，如表9Multihop QA：通过信息检索和推理来回答复杂问题。2WikiMultihopQA数据集：来自维基百科文章的2-hop复杂问题，回答这些问题需要模型具备“组合”、“比较”、“推理”的能力。本文中，采用Self consistency improves chain of thought reasoning in language models.中提出的方法生成思维链和最终答案。具体设置参考表7。 “Why did the founder of Versus die?”(“范思哲创始人为何去世？”) 目标输出：（”The founder of Versus was Gianni Versace. Gianni Versace was shot and killed on the steps of his Miami Beach mansion on July 15, 1997. So the answer is shot.”）”范思哲的创始人是詹尼-范思哲。詹尼-范思哲于 1997 年 7 月 15 日在迈阿密海滩豪宅的台阶上被枪杀。所以答案是枪杀” 使用检索器为BM25，检索预料是维基百科文章。prompt为：其中这个8个示例是examples 与Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.这篇文章类似，本文也发现了将examples的检索结果纳入其中可以提高性能，使用每个example的输入x作为query来检索一些文档，然后用下面这个prompt的格式添加这些文档。 我们发现，检索文档数量越多能提高性能，所以本文使用了text-davinci-003 输入长度限制内可容纳的最大文档数，即2WikiMultihopQA 的文档数为 2Commonsense reasoning：需要世界知识和常识知识来回答的问题。StrategyQA：这是一个众包的是/否问题的集合本文中，采用Chain of thought prompting elicits reasoning in large language models.中提出的方法生成思维链和最终答案。具体设置参考表7。我们提取最终答案，并使用精确匹配将其与黄金答案进行匹配。 prompt是6个example和3个文档： Long-form QA：必须回答出复杂问题的全面答案。ASQA作为测试平台，其中输入是具有多种解释的模糊问题，输出应该涵盖所有这些问题。 例如，“费城老鹰队在哪里进行主场比赛？”可以询问城市、体育综合体或体育场。我们发现，在许多情况下，甚至对人类来说，识别问题的哪个方面是模糊的也是一项挑战。本文创建了另一个设置（ASQA-hint），在这里我们提供了一个简短的hint，以指导LMs在生成答案时保持正轨。上述例子的hint是“这个问题在具体地点或地点方面不明确。”指标：follow ASQA: Factoid Questions Meet Long-Form Answers这篇文章的指标，包括EM、基于RoBERTa的QA评分（DisambigF1）、ROUGE（Lin，2004），以及Disambig-F1和ROUGE的综合评分（DR）。prompt：手动注释了8个example，和3个文档。Open-domain summarization：通过从开放网络收集信息来生成关于主题的全面摘要。WikiAsp：从维基百科的20个领域中生成关于实体的aspect-based 的摘要。例如，”生成关于回声学校（俄勒冈州）的摘要，包括以下方面：学术、历史”。指标：包括ROUGE，基于命名实体的F1，以及衡量事实一致性的UniEval（Zhong et al.，2022）。实验结果 整体结果 在所有任务/数据集上，FLARE 的性能都优于所有基线，这表明 FLARE 是一种通用方法，可以在整个生成过程中有效地检索更多信息。 在各种任务中，多跳 QA 的改进最为显著。这主要是由于该任务定义明确，目标明确，即通过 2 跳推理过程生成最终答案，这使得 LM 更容易生成主题输出。 相比之下，ASQA 和 WikiAsp 更具开放性，增加了生成和评估的难度。 ASQA-hint 的改进幅度大于 ASQA，这是因为在很多情况下，识别模棱两可的方面即使对人类来说也是一项挑战，而提供通用提示则有助于 LM 紧扣主题。 FLARE和baselines在2WikiMultihopQA上的整体结果。 大多数情况下，多次检索增强都优于单次检索，但各有优势。 the previous sentence进行检索的改进相对较小，我们认为这主要是因为在 2WikiMultihopQA 中，上一句话描述的实体或关系往往与下一句话中的实体或关系不同。 the previous-window可能会利用句子的前半部分来检索可能有助于生成后半句的信息。 在所有基线中，问题分解方法（Press 等人，2022 年）的性能最好，这并不奇怪，因为用分解的子问题（提示 D.2）手动注释的非上下文示例可以引导 LM 生成与后代主题/意图一致的子问题。 FLARE 的表现优于这一基线，这表明手动注释示例对于有效的未来感知检索并非必要。 FLAREinstruct 与问题分解之间的差距很大，这表明使用一个固定的检索指令和示例来生成搜索查询具有挑战性。比如语言模型可能无法理解检索指令的含义，或者无法根据示例生成合适的查询，或者生成的查询过于简单或重复，导致检索出的文档质量不高 在 ASQA 数据集上，使用the previous window 进行检索的性能低于单次检索，我们假设这是因为前一个窗口不能准确反映未来的意图。由于我们专注于评估事实性，强调事实内容的指标（例如 EM、Disambig-F1、UniEval）比在所有标记上计算的指标更可靠（ROUGE-L）。 消融实验前瞻性检索和基于过去上下文的检索哪个更有效？在 2WikiMultihopQA 和 ASQA-hint 上进行了消融实验，比较了使用上一句和下一句进行检索的效果。具体来说，这两种方法都检索每个句子，并直接使用完整的上一句/下一句作为查询。如表 3 所示，使用下一句进行检索的效果明显优于使用上一句，这证实了我们的假设。我们还使用不同数量的过去标记作为查询，运行了前一窗口方法。如表 4 所示，使用过多的过去标记（大于 32 个）会降低性能，这进一步证实了我们的假设，即过去的上下文可能与后代的意图无关。主动检索的重要性策略是调整主动检索的阈值，从不检索到逐句检索，$\\theta$从0-1。然后计算激活检索的steps/sentences的比例，基于此呈现性能。如图5所示：在2WikiMultihopQA上，当检索百分比超过60%时，性能趋于平稳，这表明在lm是自信的情况下检索是不必要的。在StrategyQA上，当检索百分比超过50%时，性能下降，表明不必要的检索会引入噪声，阻碍原始生成过程。我们发现对40%-80%的句子触发检索通常会导致跨任务/数据集的良好性能。不同query公式的影响在表 5 中，我们比较了 FLARE 在不同屏蔽阈值 β 下的性能。 直接检索完整句子（β = 0）比屏蔽低概率标记更差，这证实了我们的假设，即低置信度的错误标记会分散检索者的注意力。 我们在表 6 中比较了隐式和显式查询表述方法。 两种方法的性能相似，表明这两种方法都能有效反映信息需求。局限性FLARE在Wizard of Wikipedia (Dinan et al.， 2019)和ELI5上没有提高性能。 Wizard of Wikipedia是一个知识密集型对话生成数据集，其输出相对较短(平均约20个令牌)，因此可能不需要检索多个不同的信息片段。 在ELI5数据集上，生成的答案内容比较长，所以需要检索多个相关的信息源，但是这也带来了一些困难，比如如何将检索到的信息与生成的文本进行有效的对齐，以及如何评估生成文本的质量和事实性4。这些困难导致了单次检索和FLARE的方法都没有比不使用检索的方法有显著的提升。 从工程角度看，交错生成和检索的天真实施会增加生成的开销和成本。LMs 需要激活多次（每次检索一次），而且无缓存实现还需要在每次检索后重新计算之前的激活。如果采用特殊的架构设计，对检索文档 Dqt 和输入/生成 (x/y&lt;t) 进行独立编码，就有可能缓解这一问题。 3. MAKING RETRIEVAL-AUGMENTED LANGUAGE MODELS ROBUST TO IRRELEVANT CONTEXThttps://github.com/oriyor/ret-robust针对的问题：RALM 的一个重要要求是检索到的信息在相关时有助于模型性能，并且在不会损害性能。这在多跳推理场景中尤其重要，因为滥用不相关的证据会导致级联错误。本文的工作： 对5个开放域问答基准进行了彻底的分析，描述了检索何时降低准确性的情况。 提出了2种缓解方法： 方法1：（Baseline）根据小型自然语言推理 (NLI) 模型过滤掉不包含问答对的检索到的段落。这可以有效地防止性能下降，但代价是也丢弃相关段落。 方法2：一种自动生成数据的方法来微调语言模型（训练llm何时使用检索），具体，在训练时使用相关和不相关上下文的混合。 实验表明，即使是 1,000 个示例也足以对模型进行训练，使其对无关上下文具有鲁棒性，同时在具有相关上下文的示例中保持较高的性能。 工作1：对5个开放域问答基准进行了彻底的分析，描述了检索何时降低准确性的情况。工作1.1 对5个开放域问答基准进行了彻底的分析lama-2- 13b在五个QA任务中提示的准确性，三种设置下:(a)没有检索，(b)从强大的搜索引擎检索top-1，以及(c)随机检索的通道。发现：检索增强可以提高性能，但即使是强大的检索也会损害StrategyQA和Fermi上的性能，并且随机上下文会显著降低性能。工作1.2 将evidence/文档纳入RALM的常用方法概述RALMs和LMs的定义\\[p_{LM}=\\prod_{i=1}^{n}p_\\theta(x_i|x_{&lt;i})\\]\\[p_{RALM}=\\prod_{i=1}^{n}p_\\theta(x_i|R_C(x_{&lt;i});x_{&lt;i})\\]其中$R_C$是检索操作。本文follow Self-Ask和IR-CoT，关注ODQA（开放域问答）的多跳问答交错检索。 多跳问答交错检索： 对每个中间问题进行检索，并为每个问题准备上下文。 单跳VS多跳 在单跳设置中，模型必须在“给定问题”和“检索到的上下文”的情况下生成答案。 在多跳设置中，模型必须生成中间问题和答案，直到得到最终答案，并且在每个中间问题之后调用原始问题的检索器。工作2：2种缓解方法工作2.1：探讨使用NLI模型识别不相关上下文的潜力NLI的一个用法是判断一个假设（hypothesis）是否由一个前提（premise）推出，或者是否与前提相矛盾，或者是否与前提无关1。例如：前提：有一只猫在沙发上睡觉。 假设：有一只动物在沙发上睡觉。 结果：蕴含（entailment），因为前提可以推出假设。前提：有一只猫在沙发上睡觉。 假设：有一只狗在沙发上睡觉。 结果：矛盾（contradiction），因为前提与假设不一致。前提：有一只猫在沙发上睡觉。 假设：今天是星期三。 结果：中立（neutral），因为前提与假设无关。本文提出首先利用一个检索模型（retrieval model）从外部文档（external document）中找到与问题相关的上下文（context），然后利用一个生成模型（generation model）根据上下文生成答案。生成模型有两种，一种是LM，另一种是RALM。论文中提出了一种简单的回退策略（back-off strategy），就是先用LM生成一次答案，然后用RALM生成一次答案，最后用一个NLI模型判断哪个答案更符合上下文的逻辑关系，如果RALM生成的答案被NLI模型判断为蕴含，就选择RALM的答案，否则就选择LM的答案。工作2.2：对模型进行微调以使其对无关上下文具有鲁棒性的过程由于仅引入了检索增强的LM（RALM）在他原本的训练时没有引入外部文档检索，所以比posthoc filtering更有效的解决方案可能是训练RALM忽略无关的上下文。本文关注在相对较小的数据集（几百个）上训练是否足够单跳：$R$ 检索到的文档（context）扩充问题即可。relevant context：创建训练样本（使用的是$R_C$返回的top1 context）irrelevant context：要么是用$R_C$返回的低排序的结果，要么是另一个问题的随机context。多跳（本文研究目标）：主要挑战是生成训练示例，具体目标是“自动生成检索增强分解步骤”。基模型：Llama-2-13B训练数据：3 ODQA benchmarks=NQ的1000个训练样本（单跳）+2WIKIQA的500个训练问题1539个样本（显示）+StrategyQA的414个问题和1584个样本（隐式） 首先，它用 GPT-3作为一个大型语言模型，用 SA-NR提示来生成复杂任务的分解方案。这些分解方案是一系列的简单子任务，每个子任务都有一个输入和一个输出。 然后，它用不同的方法来验证生成的分解方案的质量。对于有中间答案的数据集，它用中间答案来过滤掉不正确的分解方案。对于没有中间答案的数据集，它用自洽性来检查分解方案是否能够得到正确的最终答案。 接着，它用 Llama-2-13B作为一个微调过的 LLM，用 SA-RetRobust提示来根据分解方案和检索到的证据来解决复杂任务。它在训练过程中用相同的概率来选择最相关的、低排名的或随机的证据，以提高模型的鲁棒性。 首先，它需要用一个已经生成了分解方案的 LLM，比如 GPT-31，来提供每个子任务的输入和输出。这些分解方案是一系列的简单问题和答案，比如“谁是美国总统？”和“乔·拜登”。 然后，它需要用一个检索模型，比如 DPR2，来根据每个子任务的输入，从一个大规模的文档集合中检索出最相关的证据。它只用最高排名的证据，也就是 SA-R@1 提示的意思。这些证据是一些包含相关信息的文本片段，比如“乔·拜登于 2021 年 1 月 20 日宣誓就职，成为美国第 46 任总统”。 接着，它需要用一个微调过的 LLM，比如 Llama-2-13B3，来根据分解方案和检索到的证据来解决复杂任务。它用 SA-R@1 提示来指导 LLM 的输入和输出的格式，比如“输入：谁是美国总统？证据：乔·拜登于 2021 年 1 月 20 日宣誓就职，成为美国第 46 任总统。输出：乔·拜登”。 最后，它需要用一个优化方法，比如最大似然估计（MLE）4，来调整 LLM 的参数，使其能够生成正确的子任务和最终任务的输出。它用一些已知答案的训练数据来计算 LLM 的输出的概率，然后找到使这个概率最大的参数值。 最后，它在三个开放域问答（ODQA）的基准数据集上评估了它的方法的效果，分别是单跳的 NQ，显式的 2WIKIMQA 和隐式的 STRATEGYQA。它还与其他的对比方法进行了消融实验，分析了不同的分解方案和证据的影响。实验结果： 即使不检索外部文档，那用分解问题数据集训练之后在 5 个数据集上表现都提升了，这说明分解问题数据集的训练的确提高了模型的解决复杂问题的能力。 在 prompt 中增加外部文档的 RALM 能够提升 LM 在单跳问题和显式问题上的表现，但隐式数据上的表现反而下降了。这可能是因为隐式问题需要更多的推理和创造力，而不是简单的检索和匹配。 当使用 NLI 模型来辅助区分外部文档是否相关后，检索就不会再伤害 LM 的性能了。这是因为 NLI 模型可以帮助过滤掉那些与问题无关或有误导性的证据，让 LM 只关注有用的信息。但是，这样做也有一个代价，就是当检索能够提高模型的性能时，这种提升会减少。这是因为 NLI 模型可能也会过滤掉一些有助于解决问题的证据，导致模型缺少一些重要的信息。 本文的模型性能最优。这是因为它综合了分解方案、检索证据和 NLI 模型的优势，使得 LM 能够更好地解决复杂任务，尤其是那些需要多跳推理和检索的任务。 当引入的外部文档是low-rank排名低的内容的时候，本文的模型能够提高NQ和2WIKIMQA的性能，保持STRATEGYQA的性能。 当引入的外部文档是随机内容的时候，只有本文的模型能够保持住性能，也就是说能够避免收到随机内容的影响。4. SELF-RAG: LEARNING TO RETRIEVE, GENERATE, ANDCRITIQUE THROUGH SELF-REFLECTION---Arxiv，华盛顿大学 IBM研究院，23年8月尽管具有非凡的能力，大型语言模型(llm)经常产生包含事实不准确的响应，因为它们只依赖于它们封装的参数化知识。RAG改善了这个问题，然而，不加选择地检索和合并固定数量的检索段落，无论检索是否必要，或者段落是否相关，都会降低LM的通用性，或者可能导致无益的响应生成。我们引入了一个名为 “自我反思检索-增强生成（SELF-RAG）”的新框架，通过检索和自我反思来提高 LM 的质量和事实性。Self-RAG的做法：训练一个任意的LM，它可以自适应地按需检索段落，并使用特殊的token(称为[反射token])生成和反映检索到的段落及其自己的生成结果。生成[反射token]使LM在推理阶段可以控制，使其能够根据不同的任务需求调整其行为。实验表明，Self-RAG (7B和13B参数)在不同的任务集上显著优于最先进的llm和检索增强模型。具体来说，Self-RAG在开放域QA、推理和事实验证任务上优于ChatGPT和检索增强的Llama2-chat，并且相对于这些模型，它在提高长格式代的事实性和引用准确性方面显示出显著的进步。Introduction如果需要，我们的端到端训练可以让LM $M$根据检索到的段落生成文本，并通过学习生成特殊标记来批评输出。这些反射token（表1）表示需要检索或确认输出的相关性、支持性或完整性。具体来说：给定input（或还有前几轮生成）Self-RAG首先确定 是否有必要触发检索​\t\tif yes：​\t\t\t1. 输出检索token retrieve ，然后（如上图中step1：Retrieve on demand）调用检索器。​\t\t\t2. Self-RAG同时评估检索到的这些外部文档的相关性​\t\t\t3. 给每个文档构造1个prompt并传给LM生成response（如上图中step2：Generate segment in parallel）​\t\t\t4. 生成批评tokenCritique评价responses，然后根据真实性和整体质量选择最佳输出（如上图中step3：Critique outputs and select best segmentSELF-RAG① SELF-RAG Inference② Self-RAG Training$R$-检索器模型：得到相关段落，交错排序。$C$-评论家模型：需要训练$C$能生成IsRel IsSup IsUse$M$-生成器LM：在包含了相关文档和反射tokens的精选语料库中训练$M$，沿用传统的LM目标训练。3.2.1 Training The Critic Model for评论家模型的数据收集 人工标注反射token昂贵，pass。用GPT-4来生成这样的反馈（feedback），但是依赖这种专有的LMs会增加API成本并降低再现性。本文通过prompt GPT4生成反射token创建可复用的监督数据，训练一个评论家模型$C$来学习这些监督数据的知识。 由于不同的反射令牌组有自己的定义和输入如表1，对于每组反射token，都分别从原始训练数据中给他们采样一批样本，并且分别设计不同的prompt指令。 比如retrievetoken，我们用”Given an instruction, make a judgment on whether finding some external documents from the web helps to generate a better response.”这个few-shot prompt来引导GPT-4生成反射token $r$。这个过程形式化表示为$p(r I,x,y)$，$I=$few shot examples。 人工评估了GPT-4生成的结果和人类标注的结果高度一致。 最终给每组token形成了4k-20k的监督数据，详见Section D和附录A.1。 下面展示了使用GPT4生成数据阶段的所有instruction和demonstrations。 表8：生成初始检索token[Retrieval]，7个example。 表9：生成[Continue to Use Evidence] [No Retrieval] [Retrieval]。 (如果输出句子只能用证据进行验证 如果句子不需要任何事实验证（例如，关于常识的主观句子或句子） 如果需要额外的信息来验证输出句子并提供解释) 表10：生成[Relevant] [Irrelevant] 表11：生成[Fully supported] [Partially supported] [No support / Contradictory] 表12：生成[IsUse]token，这个token为数值形式，是有用程度。 output几乎没有主题或完全不相关 output解决了主要请求，但与instruction/query不完整或不相关 output是可以接受的，但需要一些关键信息的添加和改进 output大部分内容满足query的需求，需要细微的改进，比如更详细的信息、更好的响应结构、语言流畅性 output提供了对查询的完整、高度详细和信息丰富的响应，完全满足信息需求 评论家模型训练过程 采集了训练数据$D_{critic}$之后，本文用Llama 2-7B来初始化$C$(跟生成器LM $M$ 保持一致)， 1月底之前复现一个论文，follow他的包括数据集、实验设置等。5. Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning" }, { "title": "RAG", "url": "/posts/RAG%E6%A3%80%E7%B4%A2%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA-(2)/", "categories": "NLP, 基础知识", "tags": "nlp, RAG, 技术综述", "date": "2023-12-04 00:00:00 +0800", "snippet": "RAG调研一、一些共识 External knowledge is the key to resolving the problems of LLMs such as hallucination and outdated knowledge, which can make LLMs generate more accurate and reliable responses through retrieval-augmented generation (RAG). However, LLMs cannot always response as expected with RAG. For one thing, there are numerous irrelevant documents and false information on the Internet. Incorporating these external documents into LLMs could have a detrimental effect. For anthoer, LLMs suffer from the unreliable generation challenge. The generation of LLMs is often unpredictable, and we cannot guarantee that they will utilize the useful information entailed in the external documents. Additionally, LLMs can easily be misled by incorrect information in the document. To this end, we build RetrievalAugmented Generation Benchmark (RGB) to evaluate the retrieval-augmented generation of LLMs, and we concern about 4 specific abilities:[^Benchmarking Large Language Models in Retrieval-Augmented Generation]外部知识是解决幻觉和过时知识等llm问题的关键，它可以通过检索增强生成(RAG)使llm产生更准确、更可靠的响应。然而，⭐LLM 并不总是像 RAG 预期的那样响应。⭐将这些不相关的或者虚假的外部文档合并到 LLM 中可能会产生不利影响。⭐llm的生成通常是不可预测的，我们不能保证它们将利用外部文档中包含的有用信息。⭐LLM 很容易被文档中不正确的信息误导。 In real-world scenarios, it is not possible to obtain perfect documents with all the necessary external knowledge. Therefore, evaluating these four abilities of the model becomes essential in order to measure the RAG of LLMs.[^Benchmarking Large Language Models in Retrieval-Augmented Generation]⭐在现实世界的场景中，不可能获得具有所有必要的外部知识的完美文档。因此需要能够评估模型的这四个能力。二、论文调研1. Benchmarking Large Language Models in Retrieval-Augmented Generation[^Benchmarking Large Language Models in Retrieval-Augmented Generation]---中科院计算所，23年9月4日摘要：检索增强生成 (RAG) 是一种减轻大型语言模型 (LLM) 幻觉的有前途的方法。然而，现有的研究缺乏对检索增强生成对不同大型语言模型的影响的严格评估，这使得识别RAG对不同llm能力的潜在瓶颈具有挑战性。在本文中，我们系统地调查了 Retrieval-Augmented Generation 对大型语言模型的影响。我们分析了不同大型语言模型在RAG所需的4个基本能力下的性能，包括噪声鲁棒性、负拒绝、信息集成和反事实鲁棒性。为此，我们建立了 Retrieval-Augmented Generation Benchmark (RGB)，这是一个用于英文和中文 RAG 评估的新语料库。RGB 根据解决案例所需的上述基本能力，将基准内的实例划分为 4 个单独的测试平台。然后我们在 RGB 上评估 6 个具有代表性的 LLM，以诊断应用 RAG 时当前 LLM 的挑战。评估表明，虽然 LLM 表现出一定程度的噪声鲁棒性，但它们在负拒绝、信息集成和处理虚假信息方面仍然会遇到重大困难。上述评估结果表明，在有效地将RAG应用于LLM之前，仍有相当大的旅程。Introduction：🌱为什么要研究RAG模型？ Recently, there have been impressive advancements in large language models (LLMs) like ChatGPT (OpenAI 2022), LLaMA-2 (Touvron et al. 2023), and ChatGLM (THUDM 2023a). Although these models have shown remarkable general abilities (Bang et al. 2023; Guo et al. 2023), they still suffer severely from challenges including factual hallucination (Cao et al. 2020; Raunak, Menezes, and JunczysDowmunt 2021; Ji et al. 2023), knowledge out-dating (He, Zhang, and Roth 2022), and the lack of domain-specific expertise (Li et al. 2023c; Shen et al. 2023).ChatGPT、LLaMA-2、ChatGLM等大模型虽然有优秀的通用能力，但是存在一些问题，①factual hallucination事实幻觉；②knowledge out-dating知识过时；③domain-specific expertise特定领域的专业知识。 With the help of external knowledge, LLMs can generate more accurate and reliable responses. The most common method is to use a search engine as a retriever such as New Bing.LLMs+RAG的一个典型应用就是New Bing。 On the other hand, LLMs suffer from unreliable generation challenge. LLMs can be misled by incorrect information contained in the context (Bian et al. 2023) and also suffer from hallucination during the generation (Adlakha et al. 2023), resulting in generating content that goes beyond external information.RAG也会给LLMs带来负面影响，比如①互联网中存在虚假信息，②LLM可能会被上下文中包含的错误信息误导，③LLM在生成过程中存在幻觉，会生成超出外部信息的内容。 Unfortunately, currently there lacks of comprehensive understanding on how these factors can influence RAG, and how could each model survives from these drawbacks and improvement their performance via information retrieval.不幸的是，目前对这些因素如何影响RAG，以及每个模型如何从这些缺陷中幸存下来并通过信息检索提高其性能缺乏全面的了解。因此，迫切需要对LLM进行全面评估，评估其有效利用检索到的信息的能力，以及抵御信息检索中存在的各种缺点的能力。💡为了确保LLM的内部知识不会在评估结果中引入偏差，RGB选择聚合最新的新闻信息，并基于新闻信息构建查询。然后，基于这些查询，我们使用搜索API获取相关文档，并从内容中选择最相关的片段作为外部检索文档。最后，基于查询和文档集对的不同组成，我们扩展语料库，并将其划分为4个测试平台，根据RAG中常见的挑战来评估LLM的以下基本能力，如图1所示： 噪声鲁棒性Noise Robustness：“从嘈杂文档中提取有用信息”的能力。 本文中，我们将【嘈杂的文档】定义为“与问题相关的文档，但不包含任何答案信息。” 噪声鲁棒性平台： ​\t所有相关外部文档=噪声文档+包含答案信息的文档 ​\t噪声文档=噪声比*所有相关外部文档 负拒绝Negative Rejection：“拒绝回答无答案查询”的能力 无答案查询：所需知识不存在于任何检索到的文档中。此情况，LLM应给出“信息不足”或其他拒绝信号。 负拒绝平台： ​\t外部文档=噪声文档 信息整合Information Integration：“回答关联多个文档的复杂问题”的能力 信息整合平台： ​\t查询=只能用多个文档才能回答的实例 ​\t外部文档=多个包含答案信息的文档+噪声文档 反事实鲁棒性Counterfactual Robustness：“通过指令提示LLMs’警告：检索到的信息存在潜在风险‘时，能够识别检索到的文档中的已知事实错误”的能力 反事实稳健性平台： ​\tLLM已知的知识，即可以直接回答的query。 ​\t外部文档=存在事实错误的文档 请注意，我们只评估 LLM 通过指令对检索到的信息中潜在风险的警告的情况。 测评模型： ChatGPT ChatGLM-6B ChatGLM2-6B Vicuna-7b Qwen-7B-Chat BELLE-7B结论： We found that even though RAG can improve the response accuracy of LLMs, they still suffer from the abovementioned challenges significantly. Specifically, we found that even though LLMs demonstrate some level of noise robustness, they tend to confuse similar information and frequently generate inaccurate answers when relevant information exists.尽管RAG可以提高llm的响应精度，但它们仍然受到上述挑战的显著影响。具体来说， 我们发现，尽管 LLM 展示了一定程度的噪声鲁棒性，但当相关信息存在时，它们往往会混淆相似的信息并经常生成不准确的答案。例如，当面对有关 2022 年诺贝尔文学奖的问题时，如果有关外部文件文献中 2021 年诺贝尔奖的嘈杂文档，LLM 可能会混淆并提供不准确的答案。 此外，当没有一个外部文档包含相关信息时，LLM 经常无法拒绝回答并生成不正确的答案。 此外，LLM 缺乏从多个文档中总结的能力，因此如果需要多个文档来回答问题，LLM 通常无法提供准确的答案。 最后，我们发现，即使llm包含所需的知识，并通过指令对检索到的信息中潜在风险的警告，它们仍然倾向于信任和优先考虑检索到的信息而不是他们自己的现有知识。Retrival-Augmented Generation Benchmark数据集构建过程：News Collection–真实的Step 0：收集最新的新闻文章Step 1：QA实例生成（QA instances generation）：使用prompt让ChatGPT为每篇文章生成事件（Related event）、问题（Question）和答案（Key information）。这一步能够顺便初步过滤出不包含任何事件的新闻文章。Step 2：人工检查调整、过滤难以通过搜索引擎检索的数据。Step 3：使用搜索引擎API检索（Retrieve using search engine）：对于每个Query使用谷歌 API获取10个相关网页，并提取相应的文本片段。Step 4：阅读这些网页，并将其文本内容转换为最大长度为300个token的文本块。使用现有的密集检索模型选择与查询最有效匹配的前30个文本块。这些检索到的文本块，以及搜索API提供的片段，将用作我们的外部文档。Step 5：扩展语料库，为4种能力构建Testbeds（试验台）。​\t对于噪声鲁棒性，根据所需的噪声比率对不同数量的负面文档进行采样。​\t对于负样本拒绝，所有的外部分档都是从负文档中采样的。​\t对于信息整合，基于前面生成的问题构造复杂问题。这涉及到扩展或重写这些问题，使它们的答案包含多个方面。例如，“谁获得了2023年超级碗的MVP？”这个问题可以改写为“谁赢得了2022年和2023年的超级碗MVP。因此，回答这样的问题需要利用来自各种文件的信息。​\t对于反事实稳健性，反事实稳健性数据完全基于模型的内部知识构建，也就是说让模型自动生成已知的问题和答案，例如，基于“谁获得了 2022 年诺贝尔生理学和医学奖？”的问题，该模型将生成已知问题“谁获得了 2021 年诺贝尔文学奖？”并回答“Abdulrazak Gurnah”。然后人工验证生成的答案，并检索相关文档，为了使文档包含事实错误，我们人工修改答案并替换文档中的相应部分。 密集检索模型: for English：https://huggingface.co/sentence-transformers/all-mpnet-basev2 for Chinese：https://huggingface.co/moka-ai/m3e-baseRGB统计数据：共600个基本问题，200个信息整合能力的附加问题，200个反事实稳健性能力的附加问题。所有问题，一半中文，一半英文。评价指标：该基准的核心是评估 LLM 是否可以利用提供的外部文档来获取知识并生成合理的答案。 Accuracy：for 噪声鲁棒性、信息整合 ​\t精确匹配-如果生成的文本包含与答案的精确匹配，则视为回答正确。 Rejection rate：for 负样本拒绝 ​\t当只提供嘈杂的文档时，LLM应输出具体内容—— “I can not answer the question because of the insufficient information in documents.”（“由于文档中的信息不足，我无法回答问题。”）如果模型生成此内容，则表示成功拒绝。 ​\tPS.使用说明提示模型。 Error detection rate：for 反事实鲁棒性（衡量模型是否能检测出文档中的事实错误） ​\t当提供的文档包含事实错误时，模型应该输出特定的内容-“There are factual errors in the provided documents.”（“提供的文档中存在事实错误”。）如果模型生成该内容，则表明模型在文档中检测到错误信息。 ​\tPS.使用说明提示模型。 Error correction rate：for 反事实鲁棒性（衡量模型是否能在识别到事实错误之后仍能提供正确的答案） ​\t如果该模型生成正确的答案，则表明模型能够 修正 文档中的事实错误。\\[ACC=\\frac{\\#tt}{\\#nums}\\] #tt：正确的response数量 #nums：所有待评估的❓实例数量 考虑到模型可能无法完全遵守指令，对于拒绝率和错误检测率，我们还使用ChatGPT对答案进行额外评估。具体而言，❓我们通过使用指导（instructions）和演示（demonstrations）来评估模型的响应，以确定它们是否能够反映文件中没有的信息或识别任何事实错误。ExperimentsSettings由于上下文限制，我们为每个问题提供5个外部文档。在我们关于噪声鲁棒性的实验中，我们评估了噪声比在0到0.8之间的场景。为了全面评估整体能力，我们对每种语言都采用了统一的指导（instructions），如图3所示。实验是使用NVIDIA GeForce RTX 3090进行的。include a system instruction followed by a user input instructionResults on Noise Robustness针对 噪声鲁棒性 实验：结论： RAG能有效改善LLMs的响应。 即使在存在噪声的情况下，llm也表现出了很强的性能，这表明RAG是llm产生准确可靠响应的一种有希望的方法。 噪声率的不断提高对llm中的RAG提出了挑战。 具体来说，当噪声比超过80%时，模型的精度明显下降。例如，ChatGPT的性能从96.33%下降到76.00%，而ChatGLM2-6B的性能从91.33%下降到57.33%。 深入分析这个实验中的模型答错的答案，发现错误通常源于3个原因：给出了噪声鲁棒性的误差案例，并且只给出了一个正面文档和一个负面文档。响应由ChatGLM2-6B生成。蓝色文本表示文档与问题或答案之间匹配的部分，而红色文本突出显示不匹配的部分。 长距离信息 当外部文档中“与问题相关的信息”与“与答案相关的信息”相距甚远时，模型难以识别正确答案。这种情况中，大部分是“与问题相关的信息”首先出现在文档的开头，在下文中使用代词指代它。这种情况出现时，模型会去依赖其他文档的信息，产生错误的印象（幻觉）。 如上图中卡塔尔公开赛只在开头出现了一次，与答案文本Anett Kontaveit人名相距甚远。 证据的不确定性 在备受关注的事件发生前，人们倾向于预测、猜测、预言它。尽管这样的预测文档明确指出这是不确定或推测性的内容，但它们仍然会影响LLM的检索增强生成。 如上图中，错误文档中的内容都是关于苹果新耳机的预测（Apple Reality Pro），并且存在有正确答案的文档（Vision Pro），模型仍然被误导了。 概念混淆 外部文档中的概念可能与问题中的概念相似，但又不同。这会让LLM混淆，并生成不正确的答案。 如上图中，模型对问题的理解集中在”汽车收入“的概念上（特斯拉，收入-&gt;通常提到特斯拉都是谈论特斯拉汽车-&gt;特斯拉，汽车收入），但问题问的是特斯拉这个公司一季度全部“收入”。 通过上述分析，LLM需要进一步详细的增强，比如长文档建模、精确的概念理解Results on Negative Rejection testbed针对 负拒绝 实验：（只提供负样本，看模型的拒绝率）采用精确匹配评估拒绝率（Rej），利用ChatGPT来确定LLM的response是否包含拒绝信息（Rej*）。结论： 负拒绝对LLM中的RAG提出了挑战。 中文/英文LLM的最高拒绝率最高43.33%/45%。 通过比较Rej和Rej*，发现LLM不能严格遵循指令，且经常产生不可预测的response，因此LLM很难直接用来识别负样本并判定拒绝。 2个原因： 证据不确定性 虽然文件中只提到与 “亚当-麦凯 “的联系，并没有明确指出他是这部电影的导演，但模型仍然断定他担任了这一角色。 概念混淆 答案中提供的信息与问题中提到的“2022 年冬季奥运会”而不是“2022 年奥运会”。与直接回答相比，检索增强生成对负拒绝提出了更大的挑战，因为它提供了可能误导 LLM 并导致错误响应的相关文档。 Results on Information Integration testbed针对 信息整合 实验：不同噪声率下，评估模型准确率。结论： 信息整合 对于LLMs中的RAG来说是一个大挑战（LLM难以有效地整合信息，不太适合直接回答复杂的问题。） 即使噪声率=0，英文/中文中最高也只有60%/67%的准确率 添加噪声文档后，英文/中文下降到43%/55% 对于文档嘈杂的RAG来说，复杂的问题更具挑战性。 对比 噪声鲁棒性 实验（简单问题，添加噪声文档），如下图。简单问题时，噪声比达到0.8时，性能才显著下降，而复杂问题时，噪声比=0.4时，性能猛降。 这表明复杂的问题更容易受到噪声的干扰。我们推测，这是因为解决复杂问题需要整合来自多个文档的信息，而这些信息可以被视为彼此的噪声，使模型更难从文档中提取相关信息。 4个原因：（这里分析的是噪声率-0的ChatGLM2-6B的错误数据） 38%的错误实例属于 噪声稳健性 实验中发现的3个错误原因 合并错误（占比28%）： 模型试图合并两个子问题的答案，错误地使用一个问题的答案直接回答两个子问题，然后忽略另一个子问题相关的所有文档。 例如：模型回答D 组是法国队和德国队的世界杯小组，而实际上德国队被分到了 E 组。 忽视错误（28%） 模型只回答一个问题，直接忽略另一个问题。原因是模型对问题缺乏全面的理解，没有认识到问题由多个子问题组成。 例如：模型只提供了 2022 年超级碗 MVP 的答案，而没有考虑 2023 年。 错位误差（6%） 模型把子问题1的文档误识别为子问题2的问答，导致答案错位。 例如：模型仅提到了 2023（95）学院奖的最佳图片，完全忽略了 2022 年奖项。此外，它错误地指出，“CODA”是 2023 的最佳图片，当它实际上被授予 2022 年的最佳图片时。 上述错误主要是由于对复杂问题的理解有限，这阻碍了有效利用来自不同子问题的信息的能力。关键是提高模型的推理能力。一种可能的解决方案是使用思维链方法来分解复杂问题（Zhou et al. 2023a; Xu et al. 2023b; Drozdov et al. 2023）。然而，这些方法会减慢推理速度，无法提供及时的响应。Results on Counterfactual Robustness testbed针对 反事实鲁棒性 实验：（考察模型对于已知事实知识的问题，包含事实错误的噪声文档能否干扰他的思考。因此只考虑准确率超过 70% 的 LLM。）4个指标：①不包含任何文档的准确率、②包含反事实文档的准确率、③错误识别率、④错误纠正率采用精确匹配评估错误识别率（ED），利用ChatGPT来识别LLM的response是否包含反事实错误信息（ED*）。 ChatGPT-en在错误纠正率（CR）方面表现最好，达到了57.14%，远高于其他两种模型。 所有的模型在有反事实文档时的准确率（ACCdoc）都比没有外部文档时的准确率（Acc）下降了很多，说明反事实文档对LLMs的生成能力有很大的干扰。 Qwen-7B-Chat-zh在错误检测率（ED）方面表现优于ChatGPT-zh，但在错误纠正率（CR）方面不如ChatGPT-zh，说明它能够发现错误，但不能有效地修正错误。原因分析： LLMs过于依赖检索到的文档，而不是利用自身的知识。这导致LLMs容易被含有错误事实的文档所误导，从而产生不准确的回答。 检索增强生成不是为自动解决给定上下文中的事实错误而设计的，因为这与模型缺乏知识并且依赖于检索到的文档以获取附加信息的基本假设相矛盾。 检索增强生成（RAG）是一种让大型语言模型（LLMs）在生成内容时，可以从外部知识源获取额外信息的技术。 RAG的一个基本假设是，LLMs缺乏知识，需要依靠检索到的文档来补充信息。也就是说，RAG认为检索到的文档是可靠的，可以帮助LLMs生成更准确的答案。 但是，如果检索到的文档中包含了错误的事实，也就是与真实情况相悖的信息，那么RAG就会出现问题。因为RAG没有设计自动纠正文档中的错误事实的能力，它会盲目地信任检索到的文档，从而生成错误的答案。 这就是为什么RAG并不适合解决反事实鲁棒性的问题。反事实鲁棒性是指LLMs在处理与事实相悖的外部文档时，能否正确地检测和纠正错误的能力。这是一种重要的信息整合能力，对于实际应用中的可靠性和安全性至关重要。 然而，由于互联网上假新闻泛滥，这个问题在实际应用中至关重要。现有的 LLM 不具备处理因错误信息造成的不准确回复的保障措施。事实上，它们在很大程度上依赖于检索到的信息。即使 LLM 包含有关问题的内部知识，它们也经常相信检索到的虚假信息。这对未来在 LLMs 中发展 RAG 提出了重大挑战。&lt;font color=#008B8B&gt;🤹个人总结&lt;/font&gt;2. Active Retrieval Augmented Generation[^Active Retrieval Augmented Generation]---EMNLP 2023，卡耐基梅隆大学，23年8月摘要： Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution.尽管大型语言模型 (LM) 具有理解和生成语言的显着能力，但它们倾向于产生幻觉并创建事实不准确的输出。通过从外部知识资源中检索信息来增强 LM 是一种很有前途的解决方案。 Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential.大多数现有的检索增强LMs采用检索-生成设置，仅根据输入检索一次信息。然而，在涉及生成长文本的更一般的场景中，这是有局限的，在生成过程中不断收集信息是必不可少的。 In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We proposeForward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.在这项工作中，我们提供了主动检索增强生成的广义视图，这些方法在生成过程中主动决定何时检索和检索什么。我们提出了前瞻性主动检索增强生成(FLARE)，这是一种通用方法，它迭代地使用对即将到来的句子的预测来预测未来的内容，然后将其用作检索相关文档的查询，以便在包含低置信度令牌的情况下重新生成句子。实验是在4个长篇知识密集型生成任务上验证的效果。Introduction：LLM→幻觉 Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form outputLLM在更复杂的任务中保有优秀的产生长序输出的能力。这些复杂任务（long-form输出任务）有：long-form QA，open-domain summarization，chain-of-thought（CoT）reasoning。RAG+LM/LLM：检索+生成 → 文本长度受限/单次检索/被动注入 These single-time retrieval augmented LMs outperform purely parametric LMs, particularly for short-form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the information needs are clear in the user’s input, and it is sufficient to retrieve relevant knowledge once solely based on the input.对于short-form的知识密集型生成任务，比如事实性问答（factoid question answering）任务，这类任务是单次的，这种情况下，信息需求在用户的输入时就明确了，所以仅基于输入执行单次检索（相关知识）就可以支撑。short-form &amp; long-form In contrast to short-form generation, long-form generation presents complex information needs that are not always evident from the input alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowledge throughout the generation process.与short-form生成相比，long-form生成需要复杂的信息，而这些信息并不总是仅从输入中就能看出来。与人类在创作论文、散文或书籍等内容时逐步收集信息的方式类似，使用 LM 的long-form生成需要在整个生成过程中收集多种知识。FLARE的idea例如：要生成某个特定主题的摘要，基于主题名称的初始检索可能无法涵盖所有方面和细节。在生成过程中，如生成某个方面（如乔·拜登的教育史）或某个特定细节（例如乔·拜登的总统竞选公告日期）时，根据需要检索额外信息至关重要。具体来说，从用户输入 x 和初始检索结果 Dx 开始，FLARE 会反复生成一个临时的下一个句子（以灰色斜体显示），并检查其中是否包含低概率标记（以下划线表示）。如果是（步骤 2 和 3），系统将检索相关文档并重新生成句子。除了FLARE，还有哪些尝试？FLARE的关键思路对于这些尝试，作者提出以下这个问题： 能否创建一个简单通用的检索增强LM，在整个生成过程中主动决定何时检索以及检索什么，并适用于各种长形式的生成任务？问题1：何时检索假设-只有在缺乏所需知识的情况下才应该检索信息，以避免被动检索增强LMs中发生的不必要或不适当的检索 本文鉴于LLM通常具有良好的校准性，因此只有当出现低概率（低置信度）的时候表明LM缺乏知识，因此采用一种主动检索策略，该策略仅在LM生成低概率token时检索。问题2：检索什么假设-在考虑检索什么的时候，重要的是考虑LMs未来打算生成什么，因为主动检索的目标是优化未来的生成。 本文提出通过生成临时下一句来预测未来，将其用作检索相关文档的查询，然后根据检索到的文档重新生成下一句。FLARE的性能：&lt;font color=#008B8B&gt; FLARE无需训练：FLARE is applicable to any existing LMs at inference time without additional training.&lt;/font&gt; 考虑到GPT-3.5（Ouyang et al.，2022）在各种任务上取得的令人印象深刻的性能，我们在text-davinci-003上检验了我们的方法的有效性。模型：text-davinci-003方法：FLARE任务：* 多跳QA（2WikiMultihopQA）* 常识推理（StrategyQA）* long form QA（ASQA）* 开放域摘要（WikiAsp）结果显示：单次检索 ❌ → 多次检索 ❌ → FLARE ✅RAG的数学定义 正式定义单次检索增强生成 主动检索增强生成的定义 详解FLARE本研究的动机： LM应该只在必要的时候检索信息，避免不必要的或不适当的检索。 检索query应该能够反映后续生成的意图，即预测下一句话的内容，根据其检索相关文档。本文提出了2种前瞻性的主动检索增强生成（FLARE）方法： FLAREinstruct：在生成答案的时候，使用一些鼓励检索的指令，提示LM在必要时生成检索查询。 FLAREdirect：直接使用LM的生成作为检索查询，迭代地生成下一句话，如果出现不确定的词，就检索相关文档，重新生成下一句话。方法1：（FLAREinstruct）FLARE with Retrieval Instructions我们受Toolformer的启发，FLAREinstruct是一种直接表达检索信息需求的方法。 Toolformer：一种让语言模型（LMs）在生成文本的过程中，根据需要从外部知识资源中检索相关信息的方法。具体来说，在需要额外信息时，向query中添加”[Search(query)]”e.g.“The colors on the flag of Ghana have the following meanings. Red is for [Search(Ghana flag red meaning)] the blood of martyrs, …”当使用只提供API接口的模型（比如GPT3.5和GPT4）时，通过few-shot prompting结合这种toolformer的方法诱导模型检索必要的信息以生成回答。而FLAREinstruct：具体来说，对于下游任务，我们将与检索相关的指令和示例作为Skill 1放在开头，然后将下游任务的指令和示例作为Skill 2。当给出一个sample时，要求LM在执行任务时结合技能1和2生成检索query。提示的结构如下图：FLAREinstruct的流程如下图：如图2所示，当LM生成“[Search(query)]”(以灰色斜体显示)时，我们停止生成并使用查询条件来检索相关文档，这些文档在用户输入之前添加，以帮助将来生成，直到生成下一个搜索查询或到达结束。附加的实现细节包含在附录A中。方法2：（FLAREdirect）Direct FLARE Since we cannot fine-tune black-box LMs, we found queries generated by FLAREinstruct through retrieval instructions might not be reliable.由于不能对黑盒LM进行微调，所以用FLAREinstruct方法生成的检索查询可能不太可靠，也就是说，可能不能很好地反映用户的意图，或者不能找到最合适的文档。这就引出了FLAREdirect，作者概括这个方法是一种更直接的前瞻性主动检索方法，该方法可以使用the next sentence来决定何时以及如何检索。3.2.1 基于置信度的主动检索 每个step中（假设step t），首先生成1个临时的next sentence ，这个句子不考虑外部文档。 根据这个$\\hat{s}_t$决定是否检索、生成query $q_t$ 如果LM对于$\\hat{s}_t$很自信，那么不在额外检索。 否则，用$q_t$检索出相关文档，再重新生成the next sentence $s_t$ 也可以用段落作为迭代的basis，作者选句子，是因为它不长不短蛮合适。 Since LMs tend to be well-calibrated that low probability/confidence often indicates a lack of knowledge (Jiang et al., 2021; Kadavath et al., 2022; Varshney et al., 2022)多篇论文都发现，经过无监督预训练的大型LM，比如GPT-3，通常是比较好的校准，也就是说，它们的置信度和概率是比较一致的。* 什么是LM的置信度校准？指LM的置信度（confidence）能够反映出它的预测正确的概率（probability）。比如，如果LM对一个答案的置信度是80%，那么它的预测正确的概率也应该接近80%。如果LM的置信度和概率不一致，那么就说明LM是不校准的（miscalibrated），可能会导致一些问题，比如过于自信（overconfident）或过于谨慎（underconfident）。预训练的LM，通常 置信度≈准确率（概率）；微调后的LM / RLHF之后的LM，通常 置信度不约等于准确率（即置信度校准变差），往往是会变得“过于自信”，置信度&gt;准确率。那么，if LM的置信度低，就很可能代表它没有足够的数据或信息来支持它的responce/predict（缺乏知识）。**此时，就是LM需要外部知识的时候。**本文的设定是，if 任何1个token的概率（置信度）低于阈值$\\theta \\in [0,1]$ → 检索else →不检索如下：​\t\\(\\theta = 0:不检索\\)​\t\\(\\theta&lt;1:当句子中存在概率低于\\theta的token时，触发检索\\)​\t\\(\\theta = 1:每句话都检索\\)其中，$q_t$是基于$\\hat{s}_t$生成的。3.2.2 基于置信度的query公式 先检索后生成or先生成后纠正？方式1：使用$\\hat{s}_t$直接作为$q_t$，这种方式放在long-form生成中，就必须有主动信息访问（active information access）。 参考：使用LMs生成的假设标题或段落作为query/evidence的方法。(Gao et al., 2022; Sun et al., 2022; Yu et al., 2022; Mao et al., 2021)previous context指的是生成过程中已经生成的句子，也就是当前句子之前的内容。这篇论文的作者发现，如果只根据previous context来检索文档，那么检索到的文档可能与当前句子的主题不相关，或者与下一句的内容有冲突。因此，他们提出了使用next sentence来检索文档，也就是根据当前句子的生成概率分布，选择一个最可能的下一句作为查询。这样做的好处是可以提前预测未来的内容，从而检索到更相关的文档，同时也可以避免生成重复或无意义的句子。我们发现，与前一个上下文相比，使用下一个句子进行检索可以获得更好的结果，如后面的第6.2小节所示。弊端是，这种方式有很高的风险出现“错误传播”的问题。比如：如果LM生成句子”Joe Biden attended the University of Pennsylvania”，事实是他就读于the University of Delaware，使用这个包含事实错误的句子可能会导致后续的检索都被误导，本文提出了2个简单的方法来客服这个问题，如下图3：图中下划线是概率（置信度）较低的token。方法A：隐式query公式（Masked sentences as implicit queries.）低于阈值$\\beta$的token mask掉。这消除了句子中潜在的干扰，从而提高了检索的准确性。\\[mask(\\hat{s}_t)\\]方法B：显式query公式（Generated questions as explicit queries.）针对$\\hat{s}_t$中的低置信度的token生成明确的问题，例如，如果LM不确定“宾夕法尼亚大学”，那么像“乔·拜登上过哪所大学？”这样的问题可以帮助检索相关信息。这种方法参考Self-Ask。 Self-ask（Press et al.，2022）通过手动将后续问题插入下游任务示例来实现这一点，如稍后提示D.2所示，这需要特定任务的注释工作。本文的方法在其基础上升级，在没有额外注释下生成低置信区间的问题。Step 1：首先，提取概率低于$\\beta$的所有token组成的区间$z$，用prompt3.2这样的模板来使LM生成$q_{t,z}$，这个问题的答案刚好能落在区间$z$中:Step 2：使用每个$q_{t,z}$进行检索，对于返回的文档交错排列到1个带index的list中。\\[qgen(\\hat{s}_t)\\]总结方法A和方法B：实现细节：基础模型：GPT-3.5 LMs text-davinci-003 访问API文档语料集&amp;检索器：由于本文关注的是检索生成一体化的机制，所以使用现成的检索器，将query作为input并返回相关文档的list。 依赖wikipedia的数据集：使用Wikipedia dump转储外部知识文档。检索器采用BM25. 依赖开放网络的数据集：检索器采用Bing 搜索引擎API[^ 3]。 [^ 3](https://www.microsoft.com/en-us/bing/apis/ bing-web-search-api) 多次检索的Baselines目前常见的被动多次检索增强语言模型也可以采用FLAREdirect框架。针对When&amp;What去检索，本文梳理了3个baseline大类，这些baselines并不是相应论文的精确复制品，因为许多设计选择不同，因此无法进行直接比较。本文使用相同的设置来实现它们，仅控制When&amp;What to retrieve变化。Previous-window这种方法生成一些tokens就使用当前句子之前的 $l$ 个token作为query触发检索，其中 $l$ = 窗口大小。前一个窗口生成的token用于query：RETRO、IC-RALM、KNNLM都是属于这一类，前两个是每隔一些tokens触发检索，最后一个是每个token都触发检索。本文follow IC-RALM设置 $l=16$ 。 这个公式是从这篇论文的第三页的第一段引用的1。它是在介绍一种名为RETRO的方法，它是一种用于文本生成的方法，它可以根据输入和生成过程中的信息来动态地检索外部知识资源，从而提高生成质量。这个公式的意思是，将生成的文本分成长度为l的窗口，每个窗口包含l个词。yt表示第t个窗口，它是一个向量，包含从第(t−1)l+1个词到第tl个词的所有词。例如，如果l=3，那么y1=[w1,w2,w3]，y2=[w4,w5,w6]，以此类推。这样做的目的是为了方便地使用当前句子之前的l个词作为查询，来检索与未来内容相关的文档。 假设设定窗口=3，那么是不是就一定是在3的倍数的时候触发检索？ 不一定是这样的。RETRO的检索方法是每生成几个词，就使用当前句子之前的l个词作为查询，来检索与未来内容相关的文档。这里的“几个词”是一个可调节的参数，它可以根据不同的任务和数据集来设置。如果这个参数等于l，那么就相当于每生成一个窗口，就触发一次检索。如果这个参数小于l，那么就相当于每生成几个词，就触发一次检索。如果这个参数大于l，那么就相当于每生成几个窗口，就触发一次检索。这篇论文中的实验结果表明，这个参数的选择会影响生成的质量和效率，因此需要根据具体的情况来确定。Previous-sentence这种方法每个句子都触发检索，并且使用前一个句子作为query。IRCoT属于这类。Question decomposition这种方法针对于特定任务的样本，人工标注以引导语言模型生成分解的子问题、同时生成输出。比如，Self-ask属于这类，它在样本中人工插入子问题，使用的Prompt如下图。对于测试样本，每当模型生成子问题时，都会动态触发检索。这三类方法都是在生成过程中检索外部知识。他们的缺点有： 使用前面生成的结果中的token作为query可能无法反映LMs将来打算生成什么 以固定间隔的token作为query检索信息的效率可能很低，因为它很可能选择的不是合适的token 问题分解方法要求特定任务的prompt工程，限制了在新任务中的泛化性实验设置（FLARE=FLAREdirect）策略：few-shot incontext learning评估任务：4个知识密集型任务，每个数据集抽样500个样本。如表7follow的工作：Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.超参数：根据dev集选择，如表9Multihop QA：通过信息检索和推理来回答复杂问题。2WikiMultihopQA数据集：来自维基百科文章的2-hop复杂问题，回答这些问题需要模型具备“组合”、“比较”、“推理”的能力。本文中，采用Self consistency improves chain of thought reasoning in language models.中提出的方法生成思维链和最终答案。具体设置参考表7。 “Why did the founder of Versus die?”(“范思哲创始人为何去世？”) 目标输出：（”The founder of Versus was Gianni Versace. Gianni Versace was shot and killed on the steps of his Miami Beach mansion on July 15, 1997. So the answer is shot.”）”范思哲的创始人是詹尼-范思哲。詹尼-范思哲于 1997 年 7 月 15 日在迈阿密海滩豪宅的台阶上被枪杀。所以答案是枪杀” 使用检索器为BM25，检索预料是维基百科文章。prompt为：其中这个8个示例是examples 与Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.这篇文章类似，本文也发现了将examples的检索结果纳入其中可以提高性能，使用每个example的输入x作为query来检索一些文档，然后用下面这个prompt的格式添加这些文档。 我们发现，检索文档数量越多能提高性能，所以本文使用了text-davinci-003 输入长度限制内可容纳的最大文档数，即2WikiMultihopQA 的文档数为 2Commonsense reasoning：需要世界知识和常识知识来回答的问题。StrategyQA：这是一个众包的是/否问题的集合本文中，采用Chain of thought prompting elicits reasoning in large language models.中提出的方法生成思维链和最终答案。具体设置参考表7。我们提取最终答案，并使用精确匹配将其与黄金答案进行匹配。 prompt是6个example和3个文档： Long-form QA：必须回答出复杂问题的全面答案。ASQA作为测试平台，其中输入是具有多种解释的模糊问题，输出应该涵盖所有这些问题。 例如，“费城老鹰队在哪里进行主场比赛？”可以询问城市、体育综合体或体育场。我们发现，在许多情况下，甚至对人类来说，识别问题的哪个方面是模糊的也是一项挑战。本文创建了另一个设置（ASQA-hint），在这里我们提供了一个简短的hint，以指导LMs在生成答案时保持正轨。上述例子的hint是“这个问题在具体地点或地点方面不明确。”指标：follow ASQA: Factoid Questions Meet Long-Form Answers这篇文章的指标，包括EM、基于RoBERTa的QA评分（DisambigF1）、ROUGE（Lin，2004），以及Disambig-F1和ROUGE的综合评分（DR）。prompt：手动注释了8个example，和3个文档。Open-domain summarization：通过从开放网络收集信息来生成关于主题的全面摘要。WikiAsp：从维基百科的20个领域中生成关于实体的aspect-based 的摘要。例如，”生成关于回声学校（俄勒冈州）的摘要，包括以下方面：学术、历史”。指标：包括ROUGE，基于命名实体的F1，以及衡量事实一致性的UniEval（Zhong et al.，2022）。实验结果 整体结果 在所有任务/数据集上，FLARE 的性能都优于所有基线，这表明 FLARE 是一种通用方法，可以在整个生成过程中有效地检索更多信息。 在各种任务中，多跳 QA 的改进最为显著。这主要是由于该任务定义明确，目标明确，即通过 2 跳推理过程生成最终答案，这使得 LM 更容易生成主题输出。 相比之下，ASQA 和 WikiAsp 更具开放性，增加了生成和评估的难度。 ASQA-hint 的改进幅度大于 ASQA，这是因为在很多情况下，识别模棱两可的方面即使对人类来说也是一项挑战，而提供通用提示则有助于 LM 紧扣主题。 FLARE和baselines在2WikiMultihopQA上的整体结果。 大多数情况下，多次检索增强都优于单次检索，但各有优势。 the previous sentence进行检索的改进相对较小，我们认为这主要是因为在 2WikiMultihopQA 中，上一句话描述的实体或关系往往与下一句话中的实体或关系不同。 the previous-window可能会利用句子的前半部分来检索可能有助于生成后半句的信息。 在所有基线中，问题分解方法（Press 等人，2022 年）的性能最好，这并不奇怪，因为用分解的子问题（提示 D.2）手动注释的非上下文示例可以引导 LM 生成与后代主题/意图一致的子问题。 FLARE 的表现优于这一基线，这表明手动注释示例对于有效的未来感知检索并非必要。 FLAREinstruct 与问题分解之间的差距很大，这表明使用一个固定的检索指令和示例来生成搜索查询具有挑战性。比如语言模型可能无法理解检索指令的含义，或者无法根据示例生成合适的查询，或者生成的查询过于简单或重复，导致检索出的文档质量不高 在 ASQA 数据集上，使用the previous window 进行检索的性能低于单次检索，我们假设这是因为前一个窗口不能准确反映未来的意图。由于我们专注于评估事实性，强调事实内容的指标（例如 EM、Disambig-F1、UniEval）比在所有标记上计算的指标更可靠（ROUGE-L）。 消融实验前瞻性检索和基于过去上下文的检索哪个更有效？在 2WikiMultihopQA 和 ASQA-hint 上进行了消融实验，比较了使用上一句和下一句进行检索的效果。具体来说，这两种方法都检索每个句子，并直接使用完整的上一句/下一句作为查询。如表 3 所示，使用下一句进行检索的效果明显优于使用上一句，这证实了我们的假设。我们还使用不同数量的过去标记作为查询，运行了前一窗口方法。如表 4 所示，使用过多的过去标记（大于 32 个）会降低性能，这进一步证实了我们的假设，即过去的上下文可能与后代的意图无关。主动检索的重要性策略是调整主动检索的阈值，从不检索到逐句检索，$\\theta$从0-1。然后计算激活检索的steps/sentences的比例，基于此呈现性能。如图5所示：在2WikiMultihopQA上，当检索百分比超过60%时，性能趋于平稳，这表明在lm是自信的情况下检索是不必要的。在StrategyQA上，当检索百分比超过50%时，性能下降，表明不必要的检索会引入噪声，阻碍原始生成过程。我们发现对40%-80%的句子触发检索通常会导致跨任务/数据集的良好性能。不同query公式的影响在表 5 中，我们比较了 FLARE 在不同屏蔽阈值 β 下的性能。 直接检索完整句子（β = 0）比屏蔽低概率标记更差，这证实了我们的假设，即低置信度的错误标记会分散检索者的注意力。 我们在表 6 中比较了隐式和显式查询表述方法。 两种方法的性能相似，表明这两种方法都能有效反映信息需求。局限性FLARE在Wizard of Wikipedia (Dinan et al.， 2019)和ELI5上没有提高性能。 Wizard of Wikipedia是一个知识密集型对话生成数据集，其输出相对较短(平均约20个令牌)，因此可能不需要检索多个不同的信息片段。 在ELI5数据集上，生成的答案内容比较长，所以需要检索多个相关的信息源，但是这也带来了一些困难，比如如何将检索到的信息与生成的文本进行有效的对齐，以及如何评估生成文本的质量和事实性4。这些困难导致了单次检索和FLARE的方法都没有比不使用检索的方法有显著的提升。 从工程角度看，交错生成和检索的天真实施会增加生成的开销和成本。LMs 需要激活多次（每次检索一次），而且无缓存实现还需要在每次检索后重新计算之前的激活。如果采用特殊的架构设计，对检索文档 Dqt 和输入/生成 (x/y&lt;t) 进行独立编码，就有可能缓解这一问题。 3. MAKING RETRIEVAL-AUGMENTED LANGUAGE MODELS ROBUST TO IRRELEVANT CONTEXThttps://github.com/oriyor/ret-robust针对的问题：RALM 的一个重要要求是检索到的信息在相关时有助于模型性能，并且在不会损害性能。这在多跳推理场景中尤其重要，因为滥用不相关的证据会导致级联错误。本文的工作： 对5个开放域问答基准进行了彻底的分析，描述了检索何时降低准确性的情况。 提出了2种缓解方法： 方法1：（Baseline）根据小型自然语言推理 (NLI) 模型过滤掉不包含问答对的检索到的段落。这可以有效地防止性能下降，但代价是也丢弃相关段落。 方法2：一种自动生成数据的方法来微调语言模型（训练llm何时使用检索），具体，在训练时使用相关和不相关上下文的混合。 实验表明，即使是 1,000 个示例也足以对模型进行训练，使其对无关上下文具有鲁棒性，同时在具有相关上下文的示例中保持较高的性能。 工作1：对5个开放域问答基准进行了彻底的分析，描述了检索何时降低准确性的情况。工作1.1 对5个开放域问答基准进行了彻底的分析lama-2- 13b在五个QA任务中提示的准确性，三种设置下:(a)没有检索，(b)从强大的搜索引擎检索top-1，以及(c)随机检索的通道。发现：检索增强可以提高性能，但即使是强大的检索也会损害StrategyQA和Fermi上的性能，并且随机上下文会显著降低性能。工作1.2 将evidence/文档纳入RALM的常用方法概述RALMs和LMs的定义\\[p_{LM}=\\prod_{i=1}^{n}p_\\theta(x_i|x_{&lt;i})\\]\\[p_{RALM}=\\prod_{i=1}^{n}p_\\theta(x_i|R_C(x_{&lt;i});x_{&lt;i})\\]其中$R_C$是检索操作。本文follow Self-Ask和IR-CoT，关注ODQA（开放域问答）的多跳问答交错检索。 多跳问答交错检索： 对每个中间问题进行检索，并为每个问题准备上下文。 单跳VS多跳 在单跳设置中，模型必须在“给定问题”和“检索到的上下文”的情况下生成答案。 在多跳设置中，模型必须生成中间问题和答案，直到得到最终答案，并且在每个中间问题之后调用原始问题的检索器。工作2：2种缓解方法工作2.1：探讨使用NLI模型识别不相关上下文的潜力NLI的一个用法是判断一个假设（hypothesis）是否由一个前提（premise）推出，或者是否与前提相矛盾，或者是否与前提无关1。例如：前提：有一只猫在沙发上睡觉。 假设：有一只动物在沙发上睡觉。 结果：蕴含（entailment），因为前提可以推出假设。前提：有一只猫在沙发上睡觉。 假设：有一只狗在沙发上睡觉。 结果：矛盾（contradiction），因为前提与假设不一致。前提：有一只猫在沙发上睡觉。 假设：今天是星期三。 结果：中立（neutral），因为前提与假设无关。本文提出首先利用一个检索模型（retrieval model）从外部文档（external document）中找到与问题相关的上下文（context），然后利用一个生成模型（generation model）根据上下文生成答案。生成模型有两种，一种是LM，另一种是RALM。论文中提出了一种简单的回退策略（back-off strategy），就是先用LM生成一次答案，然后用RALM生成一次答案，最后用一个NLI模型判断哪个答案更符合上下文的逻辑关系，如果RALM生成的答案被NLI模型判断为蕴含，就选择RALM的答案，否则就选择LM的答案。工作2.2：对模型进行微调以使其对无关上下文具有鲁棒性的过程由于仅引入了检索增强的LM（RALM）在他原本的训练时没有引入外部文档检索，所以比posthoc filtering更有效的解决方案可能是训练RALM忽略无关的上下文。本文关注在相对较小的数据集（几百个）上训练是否足够单跳：$R$ 检索到的文档（context）扩充问题即可。relevant context：创建训练样本（使用的是$R_C$返回的top1 context）irrelevant context：要么是用$R_C$返回的低排序的结果，要么是另一个问题的随机context。多跳（本文研究目标）：主要挑战是生成训练示例，具体目标是“自动生成检索增强分解步骤”。基模型：Llama-2-13B训练数据：3 ODQA benchmarks=NQ的1000个训练样本（单跳）+2WIKIQA的500个训练问题1539个样本（显示）+StrategyQA的414个问题和1584个样本（隐式） 首先，它用 GPT-3作为一个大型语言模型，用 SA-NR提示来生成复杂任务的分解方案。这些分解方案是一系列的简单子任务，每个子任务都有一个输入和一个输出。 然后，它用不同的方法来验证生成的分解方案的质量。对于有中间答案的数据集，它用中间答案来过滤掉不正确的分解方案。对于没有中间答案的数据集，它用自洽性来检查分解方案是否能够得到正确的最终答案。 接着，它用 Llama-2-13B作为一个微调过的 LLM，用 SA-RetRobust提示来根据分解方案和检索到的证据来解决复杂任务。它在训练过程中用相同的概率来选择最相关的、低排名的或随机的证据，以提高模型的鲁棒性。 首先，它需要用一个已经生成了分解方案的 LLM，比如 GPT-31，来提供每个子任务的输入和输出。这些分解方案是一系列的简单问题和答案，比如“谁是美国总统？”和“乔·拜登”。 然后，它需要用一个检索模型，比如 DPR2，来根据每个子任务的输入，从一个大规模的文档集合中检索出最相关的证据。它只用最高排名的证据，也就是 SA-R@1 提示的意思。这些证据是一些包含相关信息的文本片段，比如“乔·拜登于 2021 年 1 月 20 日宣誓就职，成为美国第 46 任总统”。 接着，它需要用一个微调过的 LLM，比如 Llama-2-13B3，来根据分解方案和检索到的证据来解决复杂任务。它用 SA-R@1 提示来指导 LLM 的输入和输出的格式，比如“输入：谁是美国总统？证据：乔·拜登于 2021 年 1 月 20 日宣誓就职，成为美国第 46 任总统。输出：乔·拜登”。 最后，它需要用一个优化方法，比如最大似然估计（MLE）4，来调整 LLM 的参数，使其能够生成正确的子任务和最终任务的输出。它用一些已知答案的训练数据来计算 LLM 的输出的概率，然后找到使这个概率最大的参数值。 最后，它在三个开放域问答（ODQA）的基准数据集上评估了它的方法的效果，分别是单跳的 NQ，显式的 2WIKIMQA 和隐式的 STRATEGYQA。它还与其他的对比方法进行了消融实验，分析了不同的分解方案和证据的影响。实验结果： 即使不检索外部文档，那用分解问题数据集训练之后在 5 个数据集上表现都提升了，这说明分解问题数据集的训练的确提高了模型的解决复杂问题的能力。 在 prompt 中增加外部文档的 RALM 能够提升 LM 在单跳问题和显式问题上的表现，但隐式数据上的表现反而下降了。这可能是因为隐式问题需要更多的推理和创造力，而不是简单的检索和匹配。 当使用 NLI 模型来辅助区分外部文档是否相关后，检索就不会再伤害 LM 的性能了。这是因为 NLI 模型可以帮助过滤掉那些与问题无关或有误导性的证据，让 LM 只关注有用的信息。但是，这样做也有一个代价，就是当检索能够提高模型的性能时，这种提升会减少。这是因为 NLI 模型可能也会过滤掉一些有助于解决问题的证据，导致模型缺少一些重要的信息。 本文的模型性能最优。这是因为它综合了分解方案、检索证据和 NLI 模型的优势，使得 LM 能够更好地解决复杂任务，尤其是那些需要多跳推理和检索的任务。 当引入的外部文档是low-rank排名低的内容的时候，本文的模型能够提高NQ和2WIKIMQA的性能，保持STRATEGYQA的性能。 当引入的外部文档是随机内容的时候，只有本文的模型能够保持住性能，也就是说能够避免收到随机内容的影响。4. SELF-RAG: LEARNING TO RETRIEVE, GENERATE, ANDCRITIQUE THROUGH SELF-REFLECTION---Arxiv，华盛顿大学 IBM研究院，23年8月尽管具有非凡的能力，大型语言模型(llm)经常产生包含事实不准确的响应，因为它们只依赖于它们封装的参数化知识。RAG改善了这个问题，然而，不加选择地检索和合并固定数量的检索段落，无论检索是否必要，或者段落是否相关，都会降低LM的通用性，或者可能导致无益的响应生成。我们引入了一个名为 “自我反思检索-增强生成（SELF-RAG）”的新框架，通过检索和自我反思来提高 LM 的质量和事实性。Self-RAG的做法：训练一个任意的LM，它可以自适应地按需检索段落，并使用特殊的token(称为[反射token])生成和反映检索到的段落及其自己的生成结果。生成[反射token]使LM在推理阶段可以控制，使其能够根据不同的任务需求调整其行为。实验表明，Self-RAG (7B和13B参数)在不同的任务集上显著优于最先进的llm和检索增强模型。具体来说，Self-RAG在开放域QA、推理和事实验证任务上优于ChatGPT和检索增强的Llama2-chat，并且相对于这些模型，它在提高长格式代的事实性和引用准确性方面显示出显著的进步。Introduction如果需要，我们的端到端训练可以让LM $M$根据检索到的段落生成文本，并通过学习生成特殊标记来批评输出。这些反射token（表1）表示需要检索或确认输出的相关性、支持性或完整性。具体来说：给定input（或还有前几轮生成）Self-RAG首先确定 是否有必要触发检索​\t\tif yes：​\t\t\t1. 输出检索token retrieve ，然后（如上图中step1：Retrieve on demand）调用检索器。​\t\t\t2. Self-RAG同时评估检索到的这些外部文档的相关性​\t\t\t3. 给每个文档构造1个prompt并传给LM生成response（如上图中step2：Generate segment in parallel）​\t\t\t4. 生成批评tokenCritique评价responses，然后根据真实性和整体质量选择最佳输出（如上图中step3：Critique outputs and select best segmentSELF-RAG① SELF-RAG Inference② Self-RAG Training$R$-检索器模型：得到相关段落，交错排序。$C$-评论家模型：需要训练$C$能生成IsRel IsSup IsUse$M$-生成器LM：在包含了相关文档和反射tokens的精选语料库中训练$M$，沿用传统的LM目标训练。3.2.1 Training The Critic Model for评论家模型的数据收集 人工标注反射token昂贵，pass。用GPT-4来生成这样的反馈（feedback），但是依赖这种专有的LMs会增加API成本并降低再现性。本文通过prompt GPT4生成反射token创建可复用的监督数据，训练一个评论家模型$C$来学习这些监督数据的知识。 由于不同的反射令牌组有自己的定义和输入如表1，对于每组反射token，都分别从原始训练数据中给他们采样一批样本，并且分别设计不同的prompt指令。 比如retrievetoken，我们用”Given an instruction, make a judgment on whether finding some external documents from the web helps to generate a better response.”这个few-shot prompt来引导GPT-4生成反射token $r$。这个过程形式化表示为$p(r I,x,y)$，$I=$few shot examples。 人工评估了GPT-4生成的结果和人类标注的结果高度一致。 最终给每组token形成了4k-20k的监督数据，详见Section D和附录A.1。 下面展示了使用GPT4生成数据阶段的所有instruction和demonstrations。 表8：生成初始检索token[Retrieval]，7个example。 表9：生成[Continue to Use Evidence] [No Retrieval] [Retrieval]。 (如果输出句子只能用证据进行验证 如果句子不需要任何事实验证（例如，关于常识的主观句子或句子） 如果需要额外的信息来验证输出句子并提供解释) 表10：生成[Relevant] [Irrelevant] 表11：生成[Fully supported] [Partially supported] [No support / Contradictory] 表12：生成[IsUse]token，这个token为数值形式，是有用程度。 output几乎没有主题或完全不相关 output解决了主要请求，但与instruction/query不完整或不相关 output是可以接受的，但需要一些关键信息的添加和改进 output大部分内容满足query的需求，需要细微的改进，比如更详细的信息、更好的响应结构、语言流畅性 output提供了对查询的完整、高度详细和信息丰富的响应，完全满足信息需求 评论家模型训练过程 采集了训练数据$D_{critic}$之后，本文用Llama 2-7B来初始化$C$(跟生成器LM $M$ 保持一致)， 1月底之前复现一个论文，follow他的包括数据集、实验设置等。5. Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning吗qx7dz2qo" }, { "title": "对话意图识别", "url": "/posts/%E5%AF%B9%E8%AF%9D%E6%84%8F%E5%9B%BE%E8%AF%86%E5%88%AB/", "categories": "", "tags": "", "date": "2023-12-01 00:00:00 +0800", "snippet": "对话意图识别目前的最新研究方向有以下几个： 面向共融机器人的自然交互——人机对话意图理解1。这是一本介绍人机对话中的意图识别、未知意图检测和新意图发现的方法的专业书籍，旨在为读者提供共融机器人研究领域人机对话意图分析的关键技术和基础知识。 对话上下文解耦4。这是一种利用对话历史和外部知识来提高对话意图识别的性能的方法，通过将对话上下文分解为多个子上下文，并使用注意力机制和图神经网络来建模它们之间的关系。 背景知识引入4。这是一种利用大规模的常识知识库或领域相关的知识库来增强对话意图识别的方法，通过将知识表示 多意图识别：传统的对话意图识别方法通常假设用户的话语只包含一个意图，但在实际的对话场景中，用户可能会同时表达多个意图，例如“我想去北京旅游，顺便看看有没有便宜的机票”。这种情况下，单一意图识别的方法就无法满足需求，需要识别出用户话语中的所有意图，并分别处理。多意图识别的方法主要有两类：基于序列标注的方法和基于多标签分类的方法。基于序列标注的方法将对话意图识别转化为序列标注问题，为每个词分配一个意图标签，从而识别出话语中的多个意图。基于多标签分类的方法将对话意图识别转化为多标签分类问题，为每个话语分配一个或多个意图标签，从而识别出话语中的多个意图。12 领域适应：对话意图识别的性能很大程度上依赖于训练数据的质量和数量，但在实际应用中，不同的领域或场景可能有不同的意图类别和话语表达方式，因此需要针对不同的领域或场景收集和标注大量的数据，这是一项耗时耗力的工作。领域适应的方法旨在利用已有的源领域的数据，提高目标领域的对话意图识别的性能，减少对目标领域数据的需求。领域适应的方法主要有两类：基于特征的方法和基于模型的方法。基于特征的方法通过提取或构造跨领域的特征，降低不同领域之间的差异，提高模型的泛化能力。基于模型的方法通过设计或修改模型的结构，增加或减少模型的参数，实现模型在不同领域之间的迁移或共享。34 对话历史信息的利用：对话意图识别的任务通常是在多轮对话的环境中进行的，因此需要考虑对话的历史背景和上下文信息，而不是仅仅根据当前的话语进行判断。对话历史信息的利用可以帮助模型更好地理解用户的意图，避免歧义或错误。对话历史信息的利用的方法主要有两类：基于记忆的方法和基于注意力的方法。基于记忆的方法通过引入记忆模块，存储和更新对话的历史信息，从而提供对话的上下文信息。基于注意力的方法通过引入注意力机制，计算和加权对话的历史信息，从而提取对话的关键信息。56 图神经网络（GNN，Graph Neural Networks）：这是近年来最受关注的算法之一，它可以处理图结构的数据，例如社交网络、知识图谱、分子结构等。GNN的基本思想是通过传播和聚合节点和边的信息，来学习图的表示和特征。GNN在图分类、图生成、图匹配、图推荐等任务上都有很好的表现。12 生成对抗网络（GAN，Generative Adversarial Networks）：这是一种创造性的算法，它可以生成逼真的图像、文本、音频等内容。GAN的核心思想是由两个神经网络组成，一个是生成器（Generator），负责生成新的数据；另一个是判别器（Discriminator），负责区分真实数据和生成数据。通过不断地对抗和学习，生成器可以逐渐提高生成数据的质量，判别器也可以逐渐提高判别能力。GAN在图像合成、图像编辑、图像转换、文本生成、语音合成等领域都有广泛的应用。34 变分自编码器（VAE，Variational Autoencoder）：这是一种无监督的算法，它可以学习数据的潜在分布，并从中采样生成新的数据。VAE的结构类似于自编码器（Autoencoder），都是由一个编码器（Encoder）和一个解码器（Decoder）组成，但是VAE的编码器不是直接输出一个潜在向量，而是输出一个潜在向量的均值和方差，然后通过重参数化（Reparameterization）的技巧从中采样得到一个潜在向量，再输入到解码器中重建数据。VAE的目标函数是最大化数据的重建概率和潜在向量的后验概率的下界（Evidence Lower Bound，ELBO）。VAE在图像生成、图像去噪、图像插值、文本生成、异常检测等任务上都有应用。5 注意力机制（Attention Mechanism）：这是一种增强神经网络性能的算法，它可以让神经网络在处理数据时，更加关注重要的部分，忽略不重要的部分。注意力机制最初是用于解决序列到序列（Seq2Seq）模型中的长距离依赖问题，后来被广泛应用于各种神经网络结构中，如CNN、RNN、Transformer等。注意力机制的基本思想是通过计算查询（Query）和键（Key）之间的相似度，得到一个注意力权重（Attention Weight），然后用这个权重对值（Value）进行加权求和，得到一个注意力输出（Attention Output）。注意力机制在自然语言处理、计算机视觉、语音识别等领域都有重要的作用。 Transformer：这是一种基于注意力机制的神经网络架构，它可以有效地处理序列数据，如文本、语音、图像等。Transformer的特点是完全摒弃了CNN和RNN，只使用了自注意力（Self-Attention）和多头注意力（Multi-Head Attention）来捕捉序列中的依赖关系，同时使用了位置编码（Positional Encoding）来保留序列的顺序信息。Transformer的优势是可以并行计算，提高效率，也可以处理长序列，避免梯度消失或爆炸。Transformer在机器翻译、文本摘要、文本生成、语音识别、图像生成等任务上都有突出的表现。最新对话系统综述 - 知乎 (zhihu.com)【NLP实战笔记】对话系统开放意图检测与发现算法总结 - 简书 (jianshu.com)" }, { "title": "详解retromae算法", "url": "/posts/%E8%AF%A6%E8%A7%A3RetroMAE%E7%AE%97%E6%B3%95/", "categories": "", "tags": "", "date": "2023-11-02 00:00:00 +0800", "snippet": "详解RetroMAE算法I BGE和M3E的异同区别1：基础模型不同BGE：RetroMAEM3E：RoBERTa-small &amp; RoBERTa-large区别2：预训练算法不同BGE：RetroMAE的预训练算法M3E：MLM区别3：微调方法不同BGE：2种对比学习策略（in-batch 负采样、cross-device 共享负样本）+ instruct tuningM3E：1种对比学习策略（in-batch 负采样）+ instruct tuning相同点：训练规模大致应该差不多，没有找到完整的资料，网友普遍猜测。共使用了29个数据集。II RetroMAE 与 Transformer对比区别1：输入输出不同。Transformer：输入=句子1、输出=句子2RetroMAE：输入=句子1、输出=句子1区别2：交叉注意力（cross attention）公式中的Q、K、V不同。Transformer：$Q=(y+pe)W^Q$ $K=cW^K$ $V=cW^V$RetroMAE：$Q=H_1W^Q=(c+pe)W^Q$ $K=H_2W^K=(c,x+pe)W^Q$ $V=H_2W^V=(c,x+pe)W^Q$区别3：损失函数不同。Transformer：基于对数似然的交叉熵RetroMAE：条件交叉熵区别4：Decoder部分掩码策略不同。Transformer：mask后面的全部RetroMAE：位置掩码，在Decoder的输入序列中按照一定的概率，从左到右依次掩盖每个词，然后让模型预测被掩盖的词。III 研究1：Encoder15%~30%掩码 &amp; Decoder 50%~70%掩码如图(A)和图(B)。对比传统的Transformer-Decoder思考，区别在图(B)中mask的比例，mask的比例更大给Encoder加高了理解的难度，迫使它提炼出更高质量的句子嵌入，从而使原始输入能够在恢复的时候保持良好的保真度。V 研究2：Enhanced Decoder在研究1的基础上进行了进一步的大胆创新，重构Decoder的输入，研究1只是修改了mask的比例，研究2修改了Attention机制，创新了一个新的Attention公式 $A$，如下：其实Attention公式 $A$ 的形式依然是传统的Transformer中的Self-Attention的形式，研究2的创新点在修改了其中的 $Q$、$K$、$V$ 的取值，传统的Transformer的取值可以简单理解为TE+SE+PE（暂不提dropout、归一化层等），即input-embedding层的初始编码结果。损失函数-条件交叉熵VI 拓展RetroMAE并没有利用到知识库的信息，它主要是为目前的检索器模型提供一个更适合检索任务的句子嵌入而已。而另一个模型——RETRO Transformer在预训练过程中的确利用到了知识库的信息，它是在解码器中增加了RetroDecoder模块通过分块交叉注意力CCA来从两个临近块中检索信息。 DeepMind 的 RETRO Transformer模型的性能与 GPT-3 相当，尽管其大小只有 4%（75 亿个参数，而 GPT-3 Da Vinci 的参数为 1850 亿个）。并且该模型与Google的WebGPT的论文都论证了“如果我们通过搜索/查询信息的方式来增强较小的生成语言模型，其性能可以与大规模模型相媲美。”" }, { "title": "可控文本生成", "url": "/posts/%E5%8F%AF%E6%8E%A7%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90/", "categories": "", "tags": "", "date": "2023-10-26 00:00:00 +0800", "snippet": "" }, { "title": "Baai向量模型bge", "url": "/posts/BAAI%E5%90%91%E9%87%8F%E6%A8%A1%E5%9E%8BBGE/", "categories": "", "tags": "", "date": "2023-10-24 00:00:00 +0800", "snippet": "BAAI 向量模型BGEI 为什么要研究“向量模型”？embedding模型的应用主要包含以下几点: embedding模型可以将各种数据（语言、图片等）转化为向量，并使用向量之间的距离来衡量数据的相关性。 在大模型时代，这种技术有助于解决大模型在回答问题时可能出现的问题，可以帮助大模型获取最新的知识。 OpenAI、Google、Meta等大厂也都推出了自己的语义向量模型和API服务，催生了大量的应用和工具，如LangChain、Pinecone等 1如BAAI最新的论文《Retrieve Anything To Augment Large Language Models》2所述，LLM的关键问题①幻觉；②指令敏感；③长上下文处理能力差的问题 源自于LLM的三个 边界： 知识边界： ①有限的模型参数无法存储无限的世界知识。 ②参数存储的知识是静态的。 ③LLM更新知识是通过使用高频公用数据进行训练，因此不能适应特定领域的、长尾知识。 内存边界：略 能力边界： ①局限在语言空间，不能在物理世界进行有效的互动 ②依赖于人类指令指导，缺乏自主性 ∴ 向量模型在LLM时代的作用——连接LLM内部知识和外部世界知识的桥梁，以检索增强器的形态为LLM更新知识注入知识。参考：II BGE和传统的embedding模型有什么区别？BGE立足于 检索任务，根据《Retrieve Anything To Augment Large Language Models》2所述，检索任务：捕捉不同的语义关系；面对的困难：往往受到相互干扰。LLM Embedder是核心是系统优化”训练方法“来改善向量模型的检索能力，具体包括四个点：①基于llm反馈的奖励制定②知识蒸馏的稳定③使用显式指令的多任务微调④使用同质的批内负采样LLM Embedder是BGE的更新版本，显然，和BGE的技术落点一脉相承（BGE的技术落点见III BGE是怎么做的？）。III BGE和M3E的异同见2023-11-02-详解RetroMAE算法V BGE 的预训练和微调已知，BGE的预训练阶段使用的是RetroMAE算法，微调阶段使用的是对比学习。另外，在FlagEmbedding库中除了BGE模型还提供了一个BGE Reranker模块（交叉编码器将对查询和答案实时计算相关性分数，这比向量模型(即双编码器)更准确，但比向量模型更耗时。数据格式与向量模型相同，因此您可以根据我们的示例 轻松地对其进行微调。）微调的教程： 基于对比学习的微调，训练数据的格式如下{\"query\": str, \"pos\": List[str], \"neg\":List[str]}提供了一个生成难负例的代码：python -m FlagEmbedding.baai_general_embedding.finetune.hn_mine \\--model_name_or_path BAAI/bge-base-en-v1.5 \\--input_file toy_finetune_data.jsonl \\--output_file toy_finetune_data_minedHN.jsonl \\--range_for_sampling 2-200 \\--use_gpu_for_searching 微调训练torchrun --nproc_per_node {number of gpus} \\-m FlagEmbedding.baai_general_embedding.finetune.run \\--output_dir {path to save model} \\--model_name_or_path BAAI/bge-large-zh-v1.5 \\--train_data ./toy_finetune_data.jsonl \\--learning_rate 1e-5 \\--fp16 \\--num_train_epochs 5 \\--per_device_train_batch_size {large batch size; set 1 for toy data} \\--dataloader_drop_last True \\--normlized True \\--temperature 0.02 \\--query_max_len 64 \\--passage_max_len 256 \\--train_group_size 2 \\--negatives_cross_device \\--logging_steps 10 \\--query_instruction_for_retrieval \"为这个句子生成表示以用于检索相关文章：\" 使用微调后的模型和直接调用原版BGE一样的调用。唯一一点需要注意的是：如果在微调训练时设置了query_instruction_for_retrieval这个参数，那调用的时候也要设置。预训练的教程 ： 基于RetroMAE的预训练，数据格式与传统预训练一致，如下{\"text\": str} 预训练torchrun --nproc_per_node {number of gpus} \\-m FlagEmbedding.baai_general_embedding.retromae_pretrain.run \\--output_dir {path to save model} \\--model_name_or_path BAAI/bge-large-en \\--train_data toy_pretrain_data.jsonl \\--learning_rate 2e-5 \\--num_train_epochs 2 \\--per_device_train_batch_size {batch size; set 1 for toy data} \\--dataloader_drop_last True \\--max_seq_length 512 \\--logging_steps 10 \\--dataloader_num_workers 12预训练后的模型保存在{output_dir}/encoder_model。参考： https://zhuanlan.zhihu.com/p/658112595 &#8617; https://readpaper.com/pdf-annotate/note?pdfId=4810140006232358913&amp;noteId=2018329187523227136 &#8617; &#8617;2 " }, { "title": "Metalearning", "url": "/posts/MetaLearning/", "categories": "", "tags": "", "date": "2023-09-13 00:00:00 +0800", "snippet": "Meta-Learning1. Why need Meta-Learning？1.1 Background大模型在具体的工业落地中，往往需要针对每一个数据集进行训练，训练的目标是找到一个可以拟合当前数据集的函数。每次都要训练，实在麻烦，那么有没有办法可以找到一个用少量样本即可拟合不同领域所有分类数据集的函数。1.2. What can meta-learning do?1.2.1. PLM V.S. PLM + Meta Learning如图，在Task-Oriented Semantic Parsing任务中，加持了元学习（Reptile方法）之后BART的准确率恒定提升。1.2.2. MT-DNN V.S. Meta Learning如图，当训练数据越来越少，甚至少样本时，BERT的性能下降明显接近50%，MT-DNN比较稳固，但不如元学习（Reptile方法）的性能，这也佐证了元学习更适合少样本的结论。1.2.3. Knowledge Distill V.S. Meta Learning知识蒸馏中，有多项研究表明Teacher-Net总是能自己学的很好，但教不会Student Net，因此能否让教师网络“learn to teach”？Meta Learning可以！1.2.4. Transfer Learning/Fine-tune V.S. Meta Learning其实元学习和迁移学习/微调/多任务学习的界限挺模糊的，思想上很不一样，但实际上做起来好像不太好说。比如，下面stackexchange上一位答友的回答，元学习是指“学会学习”，要学会的东西是一些更高阶的‘元知识’（超参数、初始参数等），就是你训练神经网络的工作；迁移学习是指固定一些层，剩下的层替换成新密基层，来新任务时调整新密集层的参数，在新数据集$B$上重新训练新模型。2. Meta-Learning Definition机器学习是先人为调参，之后直接训练特定任务下深度模型。元学习则是先通过其它的任务训练出一个较好的超参数，然后再对特定任务进行训练。其实就是所有在训练模型时人工设置的超参数都是元学习的目标。另一方面，元学习因为训练过程和机器学习不同，因此元学习和机器学习的数据集构造方式不一样，具体如下，在机器学习中，训练单位是样本数据，通过数据来对模型进行优化；数据可以分为训练集、测试集和验证集。在元学习中，训练单位是任务，一般有两个任务分别是训练任务（Train Tasks）亦称跨任务（Across Tasks）和测试任务（Test Task）亦称单任务（Within Task）。3. Meta-Learning Application Meta learning是一个通用性的方法论，Meta Learning就等价于汽车中的涡轮增压，可以应用到各种发动机中。3.1. Cross-Domain Training𝒯_𝑡𝑟𝑎𝑖𝑛 和 𝒯_𝑡𝑒𝑠𝑡属于同一个NLP 问题。比如都是分类数据集。𝒯_𝑛是不同领域，比如说𝒯_1 是通用领域文本分类数据， 𝒯_2是经济领域文本分类数据。3.2. Cross-Question Training𝒯_𝑡𝑟𝑎𝑖𝑛 和 𝒯_𝑡𝑒𝑠𝑡属于同一领域（或相似领域）不同的NLP 问题。𝒯_𝑛是不同问题，比如𝒯_𝑡𝑟𝑎𝑖𝑛用的是机器翻译任务和NLI任务，那么𝒯_𝑡𝑒𝑠𝑡 用的是QA和对话状态追踪（DST）。3.3. Domain Generalization需要和跨领域训练cross-domain training区分开。Domain Generalization和Cross-Domain Training的区别也在于各个任务的数据集构造上，应用了交叉构造数据集的方法。4. Meta-Learning in NLP4.1. Learning to initialize通过学习一个好的初始参数来进行快速适应新任务的方法都可以归为 learn-to-init 。MAML及其一阶近似算法（FO-MAML，Reptile，etc.）这个过程可以看作是构建一个适用于多个目标领域任务的内部表征，或者最大化新任务损失函数对于模型参数的敏感度。4.1.1. 利用元网络(Meta-Network)来生成一个好的初始参数该方法强调的是“生成”。利用元网络(Meta-Network)中的F()就是元网络，它可以根据任务数据生成初始参数，但需要针对不同的模型和任务设计不同的编码器和解码器。4.1.2. 直接用元模型(Meta-Model)来学习一个好的初始参数（MAML）元模型就是指在元学习阶段被训练的模型，它可以是任何基于梯度下降算法进行训练的模型，比如CNN、LSTM、RNN及MLP等。MAML中的F()就是元模型本身，它可以适用于任何深度学习模型和任务类型，但需要计算二阶梯度或使用一阶近似。4.1.3. Learning to initialize V.S. Self-supervised LearningLearning to initialize和Self-supervised Learning的区别是一个训练时带label，一个不带。4.1.4. Learning to Initialize v.s. Multi-task Learning元学习会训练一个通用的模型参数，也就是你的神经网络的初始值。当你遇到一个新的任务时，只需要用少量的样本快速适应（fast adaptation）就可以在新任务上达到很好的效果。多任务学习会训练一个特定的网络结构，也就是你的神经网络的形式和组成。当你遇到一个新的任务时，比如识别某个数据集中的图像，你会根据这个任务和其他任务之间的关系来决定哪些参数或层要共享，哪些要分离。4.2. Learning to Compare通过比较任务之间的关系来进行分类。 MAML中的F()就是元模型本身，它可以适用于任何深度学习模型和任务类型，但需要计算二阶梯度或使用一阶近似。 learn to initialize中的F()就是元网络，它可以根据任务数据生成初始参数，但需要针对不同的模型和任务设计不同的编码器和解码器。 learn to compare中的F()则与其他两种方法有本质上的区别，它更像是一个分类器而不是一个元函数。缺陷： learn to compare方法是一种基于已知分类的方法，它只能从支持集中已有的类别进行分类。 learn to compare方法需要对每个查询样本与所有支持集中的样本进行比较，这可能会导致计算量很大，尤其是在支持集较大或查询集较多的情况下。 learn to compare方法只考虑了单个查询样本与单个支持集样本之间的相似度，而没有考虑整个查询集与整个支持集之间的全局信息。 learn to compare方法依赖于一个有效的相似度计算模块，它需要能够捕捉不同任务或类别之间的语义或逻辑关系。然而，这种相似度计算模块可能很难设计或训练，尤其是在一些复杂或多样化的领域中。 5. Meta-Learning in Specific Domain呼应开头的第一张图，现在我们的目标是训练一个优秀的元学习算法！~Reference论文： Lee, H.-Y., Li, S.-W., Vu, N., n.d. Meta Learning for Natural Language Processing: A Survey. Yue, Z., Zeng, H., Zhang, Y., Shang, L., Wang, D., 2023. MetaAdapt: Domain Adaptive Few-Shot Misinformation Detection via Meta Learning. Lu, M., Huang, Z., Zhao, Y., Tian, Z., Li, Y., 2023. DaMSTF: Domain Adversarial Learning Enhanced Meta Self-Training for Domain Adaptation. Qin, C., Joty, S., Li, Q., Zhao, R., 2023. Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning? Antoniou, A., Edwards, H., Storkey, A., 2018. How to train your MAML. International Conference on Learning Representations,International Conference on Learning Representations. Sun, Q., Liu, Y., Chua, T.-S., Schiele, B., 2019. Meta-Transfer Learning for Few-Shot Learning., in: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). https://doi.org/10.1109/cvpr.2019.00049 Behl, H., Baydin, A., Torr, PhilipH.S., 2019. Alpha MAML: Adaptive Model-Agnostic Meta-Learning. Cornell University - arXiv,Cornell University - arXiv. Liu, Z., Zhang, R., Song, Y., Zhang, M., 2020. When does MAML Work the Best? An Empirical Study on Model-Agnostic Meta-Learning in NLP Applications. Cornell University - arXiv,Cornell University - arXiv.课程：Meta Learning –Hung-yi Lee- YouTubeML 2021 Spring (ntu.edu.tw)ML 2022 Spring (ntu.edu.tw)博客： Few-shot Learning（五）Learning to Compare: Relation Network for Few-Shot Learning - 知乎 (zhihu.com) 论文解读（MetaAdapt）《MetaAdapt: Domain Adaptive Few-Shot Misinformation Detection via Meta Learning》 - Wechat~Y466551 - 博客园 (cnblogs.com) What are the differences between transfer learning and meta learning? - Artificial Intelligence Stack Exchange Meta-Learning (fastforwardlabs.com) [Meta-Learning: Learning to Learn. Although artificial intelligence and… by Thomas HARTMANN DataThings Medium](https://medium.com/datathings/meta-learning-learning-to-learn-a55cadd32b17) " }, { "title": "Elsevier 期刊投稿踩坑", "url": "/posts/Elsevier-%E6%9C%9F%E5%88%8A%E6%8A%95%E7%A8%BF%E8%B8%A9%E5%9D%91/", "categories": "", "tags": "", "date": "2023-08-16 00:00:00 +0800", "snippet": "1.报错内容： ! Package pdftex.def Error: File `thumbnails/cas-email.jpeg’ not found: using draft setting.报错原因： 模板原本的图片存放在 thumbnails 文件夹中，tex 编译的寻址路径定义在 cas-common.sty 文件中，在 Elsevier 系统中提交文件时，不能提交文件夹，直接提交 cas-email.jpeg 就会报错。解决方案： 在 cas-common.sty 文件下找到：thumbnails/cas-email.jpeg，删掉路径 thumbnails/，并将 cas-email.jpeg 放在 .tex 的同级目录下即可。2.报错内容： I can’t figure out why you would want to use a tab markhere. If you just want an ampersand, the remedy issimple: Just type I\\&amp;' now. But if some right braceup above has ended a previous alignment prematurely,you're probably due for more error messages, and youmight try typing S’ now just to see what is salvageable.绝了，&amp;在bib中是控制符，用来对齐的，这里作为期刊标题需要转义成\\&amp;。而这一段bib是从百度学术导出来的。虽然在这上面浪费的时间只有几分钟，但是这个显然是一个低级的问题。珍爱生命，远离百度学术。" }, { "title": "强化学习", "url": "/posts/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/", "categories": "NLP, 基础知识", "tags": "nlp, 强化学习, 技术综述", "date": "2023-05-11 00:00:00 +0800", "snippet": "#0 一些共识 基本的NLP任务label是人工或自动标注的，这种既定的答案（又称为监督目标字符串）我们认为是“没有人类偏好的直接信号”，但语言技术的最终目标是与人类互动。因此这样的label其实是不太合理的。 The ultimate aim of language technology is to interact with humans. However, most language models are trained without direct signals of human preference, with supervised target strings serving as (a sometimes crude) proxy. 1 强化学习基础知识1.0. 马尔可夫决策过程马尔可夫决策过程（MDP）是一种用于对具有随机性和部分可控性的决策问题进行建模的数学框架。MDP的核心特征是具有马尔可夫性质，即下一个状态只依赖于当前的状态和动作，而与之前的历史无关。MDP的目标是找到一个最优的策略，即在每个状态下选择一个最优的动作，使得决策者从初始状态到终止状态的累积奖励最大化。MDP可以用一个四元组（S，A，P，R）来表示，其中S是状态集合，A是动作集合，P是状态转移概率，R是奖励函数。 PS. 马尔可夫性质：下一个状态只依赖于当前的状态和动作，而与之前的历史无关。 马尔可夫模型：具有马尔可夫性质的随机过程的数学模型。 马尔可夫模型分两类①马尔科夫链；②隐马尔科夫模型 马尔可夫链【二元组（S，P）】：一种状态和状态转移概率都是可观测的马尔可夫模型。 隐马尔可夫模型【五元组（ S , O , π , A , B ) 】：一种状态是不可观测的，只能通过输出来推断的马尔可夫模型。 马尔可夫决策过程【四元组（S，A，P，R）】：是马尔可夫链的推广，不同之处在于添加了动作（允许选择）和奖励（给予动机）。 其中 S是状态空间的集合，表示决策者可能处于的所有状态。 A是动作空间的集合，表示决策者在每个状态下可以采取的所有动作。 P是状态转移概率函数，表示在当前状态下采取某个动作后，下一个状态的概率分布。通常表示为 P(s’ s,a)，其中 s’表示下一个状态，s表示当前状态，a表示采取的动作。 R是奖励函数，表示在当前状态下采取某个动作后，决策者能够获得的即时奖励。通常表示为 R(s,a)，其中 s表示当前状态，a表示采取的动作。 初始状态概率向量 π，表示初始时刻处于每个状态的概率。 马尔科夫链 V.S 马尔可夫决策过程 马尔可夫决策过程是一种特殊的马尔可夫模型，它属于马尔可夫链的一种，但是比马尔可夫链更复杂，因为它涉及到决策和优化的问题。 马尔可夫链的目标是分析状态的长期行为，例如平稳分布，吸收概率，平均回报时间等。马尔可夫决策过程的目标是为决策者找到一个好的“策略”：一个函数 ，它指定决策者在状态 时将选择的动作 。这样，决策者可以在每个状态下选择一个最优的动作，使得从初始状态到终止状态的累积奖励最大化。关于初始状态为什么有的版本写的有有的无： 初始状态概率向量是指在初始时刻，系统处于每个状态的概率分布。它是描述随机过程的一个重要参数，但并不是必须的。（1）有些随机过程的初始状态是确定的，比如说从某个特定的状态开始，那么初始状态概率向量就是一个只有一个非零元素的向量。（2）有些随机过程的初始状态是不确定的，但是随着时间的推移，系统会收敛到一个平稳分布，那么初始状态概率向量就不会影响系统的长期行为。（3）有些随机过程的初始状态是不确定的，而且会影响系统的长期行为，那么初始状态概率向量就是一个需要考虑的参数。 马尔可夫决策过程和马尔可夫链的表示中没有初始状态概率向量，是因为它们通常假设初始状态是给定的，或者不影响系统的最优策略或长期行为。马尔可夫决策过程的目标是找到一个最优的策略，即在每个状态下选择一个最优的动作，使得决策者从初始状态到终止状态的累积奖励最大化。如果初始状态是确定的，那么初始状态概率向量就是多余的。如果初始状态是不确定的，但是最优策略是独立于初始状态的，那么初始状态概率向量就是无关的。马尔可夫链的目标是分析状态的长期行为，例如平稳分布，吸收概率，平均回报时间等。如果马尔可夫链是遍历的，即从任何状态都可以到达任何其他状态，那么初始状态概率向量就不会影响平稳分布。如果马尔可夫链是不遍历的，但是我们只关心吸收状态的性质，那么初始状态概率向量就不会影响吸收概率或平均回报时间。 当然，并不是说初始状态概率向量在马尔可夫决策过程和马尔可夫链中没有用处。有时候，我们可能需要考虑初始状态的不确定性或影响，那么初始状态概率向量就是一个重要的参数。例如，如果我们想要计算马尔可夫决策过程的平均累积奖励，或者马尔可夫链的平均转移次数，那么初始状态概率向量就是一个必要的输入。在这些情况下，我们可以将初始状态概率向量作为马尔可夫决策过程或马尔可夫链的表示的一部分，或者单独给出。1.1. 马尔可夫模型→强化学习传统强化学习的硬编码规则是指智能体（agent）在每个状态（state）下应该采取的最优动作（action），这些规则通常是通过数学公式或者程序代码来表示的。传统强化学习的环境特征是指环境（environment）的状态集合（state space）、动作集合（action space）、状态转移概率（transition probability）和奖励函数（reward function），这些特征通常是通过矩阵或者向量来表示的。具体来说，传统强化学习的目标是找到一个最优的策略（policy），即在每个状态下选择一个最优的动作，使得智能体从初始状态到终止状态的累积奖励（cumulative reward）最大化。为了找到这样的策略，传统强化学习需要知道或者估计环境的状态转移概率和奖励函数，以及智能体的状态和动作的范围，这就需要提供硬编码规则和环境特征。传统强化学习需要人工给定以下几个要素： 状态集合 S，表示智能体可能处于的所有状态。 动作集合 A，表示智能体在每个状态下可以采取的所有动作。 状态转移概率 P，表示在当前状态下采取某个动作后，下一个状态的概率分布。 奖励函数 R，表示在当前状态下采取某个动作后，智能体能够获得的即时奖励。其中，状态集合 S 和动作集合 A 是由问题本身决定的，通常不需要人工制定。状态转移概率 P 和奖励函数 R 是由环境的特性决定的，通常需要人工给出或者估计。状态转移概率 P 描述了环境的动态性，奖励函数 R 描述了环境的反馈性。这两个要素是传统强化学习的硬编码规则，它们决定了智能体的学习目标和学习效果。奖励函数 R 可以看作是一种奖惩机制，它可以通过正向奖励或者负向奖励来激励或者抑制智能体的行为。设计奖励函数是强化学习中的一个重要而困难的任务，它需要考虑如何平衡稀疏奖励和密集奖励，如何平衡探索和利用，如何平衡短期利益和长期利益，如何引入特定领域的知识等。奖励函数的设计往往需要人工的经验和创造力，也可能需要不断的调整和优化。强化学习（Reinforcement Learning）笔记——奖励和策略结构 - 知乎 (zhihu.com)以Q-learning为例：Q-Learning中的S、A、P、R分别是： S是状态空间的集合，表示智能体可能处于的所有状态。 A是动作空间的集合，表示智能体在每个状态下可以采取的所有动作。 P是状态转移概率函数，表示在当前状态下采取某个动作后，下一个状态的概率分布。通常表示为 P(s’ s,a)，其中 s’表示下一个状态，s表示当前状态，a表示采取的动作。 R是奖励函数，表示在当前状态下采取某个动作后，智能体能够获得的即时奖励。通常表示为 R(s,a)，其中 s表示当前状态，a表示采取的动作。Q-Learning的核心是学习一个Q值函数，表示在每个状态下采取每个动作的长期回报的期望。Q值函数可以用一个表格来表示，也可以用一个函数近似器来表示。Q-Learning的更新公式是：其中 α 是学习率，γ 是折扣因子，r 是即时奖励，$max_{a′}Q(s′,a′) $是下一个状态的最大Q值。https://mp.weixin.qq.com/s/e4lw9X_hlPopIvreMYCaFw1.2. RL in NLP强化学习在NLP中应用面临的四大挑战： 动作空间过大的问题 在强化学习中，智能体需要从一个非常庞大的动作集合中选择一个最优的动作，这会导致探索和利用的困难，以及训练的不稳定和低效。这个问题在NLP中也是存在的，在NLP中，RL的动作空间一般指的是在特定的语言任务中，智能体可以生成的所有可能的语言序列的集合。比如在文本生成任务中，动作空间就是所有可能的文本句子或段落的集合。在对话系统任务中，动作空间就是所有可能的对话回应的集合。在NLP中，动作空间的大小和复杂度往往取决于语言的规模，多样性，语法，语义等因素。因此，NLP中的动作空间通常是非常大的，甚至是无穷的，这就给强化学习的探索和利用带来了巨大的挑战。 什么样的奖励函数能更好的衡量语言的质量和效果？ 奖励函数是强化学习的核心，它决定了智能体的学习目标和反馈。但是，在NLP中，设计一个合适的奖励函数并不容易，因为语言的质量和效果往往是主观和多样的，而且很难用一个简单的数值来衡量。例如，对于文本生成任务，常用的评价指标如BLEU，ROUGE等，可能不能完全反映人类的偏好和期望，而且可能存在一些不一致和不可比较的问题12。因此，如何设计一个能够捕捉语言的复杂性和多样性的奖励函数，是一个重要而困难的任务。 如何建模语言环境呢？ 环境是强化学习的另一个关键要素，它决定了智能体的观察和状态转移。但是，在NLP中，建模一个合理的环境也不是一件容易的事，因为语言的环境往往是动态和不确定的，而且很难用一个简单的模型来描述。例如，对于对话系统任务，环境包括了对话伙伴的行为，情绪，反应等，这些都是难以预测和控制的，而且可能会随着时间和场景的变化而变化34。因此，如何建模一个能够反映语言的动态性和不确定性的环境，是一个有趣而挑战的任务。 如何获得适合RL训练的高质量数据？ 在NLP中，获取足够的数据也是一件不容易的事，因为语言的数据往往是稀疏和有噪声的，而且很难用一个统一的标准来标注和评估。例如，对于文本摘要任务，数据的来源可能是多种多样的，如新闻，论文，评论等，这些数据的风格，内容，长度等都可能有很大的差异，而且可能存在一些错误，歧义，重复等问题5 。因此，如何获取一个能够覆盖语言的稀疏性和有噪声性的数据，是一个实用而重要的任务。 2 Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization We tackle the problem of aligning pre-trained large language models (LMs) with human preferences. If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework. However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of opensource libraries and benchmarks customized for LM alignment. Thus, a question rises in the research community: is RL a practical paradigm for NLP?To help answer this, we first introduce an open-source modular library, RL4LMs1,2for optimizing language generators with RL. The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al., 2020) with an arbitrary reward function. Next, we present the GRUE (General Reinforced-language Understanding Evaluation)benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference. GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally, we introduce an easy-to-use, performant RL algorithm, NLPO(Natural Language Policy Optimization) that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al., 2017)), based on both automatic and human evaluations.如果我们将文本生成视为顺序决策问题，强化学习 (RL) 似乎是一个自然的概念框架。然而，使用基于 LM 的生成使得用 RL 面临着经验挑战，包括由于组合动作空间而导致的训练不稳定性，以及缺乏为 LM 对齐定制的开源库和基准。因此，研究界一个问题上升：RL 是 NLP 的实用范式吗？ 这句话的意思是，如果我们把文本生成看作是一个需要在每个时间步选择一个词或符号的问题，那么强化学习就是一个合适的方法，因为它可以通过不断地与环境交互，学习一个最优的策略，使得生成的文本能够达到某种目标或标准。 例如，如果我们想要生成一首诗，我们可以用强化学习来训练一个模型，让它在每个时间步根据已经生成的内容和给定的主题，选择一个词或符号，直到生成完整的诗句。我们可以用一些评价指标，如韵律、押韵、情感等，来给生成的诗句打分，作为强化学习的奖励信号，从而指导模型的学习过程。 由于组合动作空间而导致的训练不稳定性：在语言生成的过程中，每个时间步都需要从一个非常大的词汇表中选择一个词或符号，这就构成了一个组合动作空间。这个空间的大小随着时间步的增加而指数增长，导致训练过程变得非常复杂和不稳定。RL算法很难在这样的空间中找到最优的策略，而且容易受到噪声、过拟合、梯度消失等问题的影响123。 缺乏为 LM 对齐定制的开源库和基准：为了使用 RL 来优化 LM 的输出，需要有一些工具和评估方法来支持这个过程。例如，需要有一些开源的库来提供各种 RL 算法和奖励函数，以及一些基准测试来衡量和比较不同的方法在各种语言生成任务上的表现。然而，目前这方面的资源还很缺乏，导致研究人员难以进行有效的实验和分析45。 首先使用开源模块化库RL4LM：使用 RL 优化语言生成器。 该库由策略强化学习算法组成，可用于训练HuggingFace库(Wolf et al.， 2020)中具有任意奖励函数的任何编码器或编码器-解码器LM。GRUE（通用强化语言理解评估）基准：这是一组 6 个语言生成任务，这些任务不是由目标字符串监督的，而是通过捕获人类偏好自动化度量的奖励函数。GRUE 是 NLP 任务的 RL 算法的第一个排行榜式评估。最后，引入了一种易于使用、高性能的 RL 算法 NLPO（自然语言策略优化），它学习有效地减少语言生成中的组合动作空间。结论：1）RL 技术通常优于将 LM 与人类偏好对齐的监督方法；2）基于自动和人工评估，NLPO 表现出比先前策略梯度方法（例如 PPO (Schulman et al., 2017)）更大的稳定性和性能。Introduction human-in-the-loop：用户将被期望为模型训练时的每个在线样本提供反馈，但这种程度的密集监督通常是令人望而却步且效率低下的。 models of human preference：人类偏好模型相比于传统的BLEU、METEOR显著提高了与人类判断的相关性。 pairwise learned preference models BERTScore BLEURT 缺点：这些函数通常不是per-token可微分——这方面同人类一样，只能对整体生成结果进行质量估计。这些指标不能直接用于训练语言模型，所以需要使用强化学习来优化语言模型的生成策略。 将基于语言模型的生成（LM-based generation）视为一个序列决策问题（sequential decision-making problem）：强化学习（RL）为优化不可微分的标量目标（non-differentiable, scalar objectives）提供了一种自然的途径，当我们将基于语言模型的生成（LM-based generation）视为一个序列决策问题（sequential decision-making problem）时，就可以使用RL来训练语言模型，使其与人类偏好对齐。 However, Goodhart’s Law3 looms: particularly in the case of imperfect metrics that use neural networks, it is easy to find nonsense samples that achieve high-quality estimates.Goodhart’s Law2 是一个经济学的法则，它可以简单地表述为：当一个指标成为目标时，它就不再是一个好的指标。换句话说，当我们设定一个特定的目标时，人们会倾向于为了达到这个目标而忽略其他的后果。这会导致问题，当我们忽视了一个情况的其他同样重要的方面时。在使用神经网络的不完美的指标（imperfect metrics that use neural networks）的情况下，这个法则尤其突出：很容易找到一些无意义的样本（nonsense samples），它们可以获得高质量的评估（high-quality estimates）。这意味着，这些指标不能真实地反映生成文本的质量，而是容易被欺骗或操纵。例如，如果我们使用BLEURT 这样的指标来评估生成文本的质量，它是一个基于 BERT 的神经网络模型，它可以计算生成文本和参考文本之间的语义相似度。但是，这个指标并不完美，它可能会给一些无意义或错误的文本高分，只要它们在表面上看起来和参考文本相似。最近的一些研究 (Wu et al., 2021a; Ouyang et al., 2022)在使用强化学习（RL）来训练语言模型（LMs），使其与人类偏好（human preferences）对齐方面取得了令人鼓舞的结果（promising results）。它们通过限制基于偏好的奖励（preference-based rewards），使其包含流畅性（fluency）的概念，来提高生成文本的质量。But，这些工作目前都没有开源benchmark和算法实现。本文的Contribution： 1个RLinNLP的算法库——RL4LMs library：帮助使用RL算法微调LM。 支持多种生成式的Huggingface模型、多种RL方法（比如PPO、A2C等） 可以DIY构建模块，包括策略网络、奖励函数、评价指标、数据集等（“可以在任何数据集上，使用任何奖励函数，来训练语言模型。”） 包含一个新的RL算法NLPO（Natural Language Policy Optimization），它是专门为语言模型设计的，可以有效地处理大的动作空间和高方差的奖励。 1个Benchmark——GRUE(General Reinforced-language Understanding Evaluation) benchmark。 包含7个NLP任务 与其他的基准（benchmarks）不同，GRUE 不是使用有监督的训练（supervised training），而是使用奖励函数（reward function(s)）来指导每个任务的学习。奖励函数是一种用于评估生成文本的质量的函数，它可以反映人类的偏好或目标。 GRUE 挑战了语言模型（models）在优化这些奖励函数（optimize these reward functions）的同时，保持流畅的语言生成能力（remaining fluent language generators）。这意味着，语言模型不仅要生成符合任务要求的文本，还要生成通顺，自然，无误的文本。 用RL算法训练所有LM以优化reward。 1个创新算法——NLPO（Natural Language Policy Optimization）该算法在token-level动态学习语言分布的特定任务约束。 图 1 展示了一种新的强化学习算法，叫做自然语言策略优化（NLPO），它是专门为语言模型设计的。它的目标是在给定一个文本片段（比如一个评论）的情况下，生成一个符合特定情感（比如正面）的后续文本（这叫做情感引导的延续）。它需要考虑两个目标：1）一个自动的人类偏好的代理，它用来给生成文本提供奖励（这里是一个情感分类器）；2）一个“自然度”的度量，它用来衡量生成文本和没有经过强化学习训练的语言模型的差异（这里用的是 KL 散度）。右侧的曲线图显示了 NLPO 和另一种流行的强化学习方法 PPO 的对比结果。上面的图表明，如果去掉 KL 散度的惩罚，强化学习方法可以很容易地获得高的奖励，但是下面的图表明，这样做的代价是生成文本的困惑度（perplexity）会增加，也就是说，生成文本的质量会下降。而NLPO+KL，也就是我们提出的方法，能够更有效地平衡奖励和自然度，比以前的工作表现得更好。 GLUR和人类评估上，都发型NLPO相比于PPO等RLforLM的算法更能够在保持语言流利性的同时，更好地平衡学习偏好奖励。 KL散度（Kullback-Leibler Divergence）是一种用来度量两个概率分布之间的差异或相似度的指标。 在文本生成任务中，我们可以用 KL 散度来衡量生成文本的概率分布 Q 和没有经过 RL 训练的 LM 的概率分布 P 的差异。这个差异可以反映生成文本的“自然度”，也就是生成文本和 LM 的输出的一致性。如果 Q 和 P 很接近，那么 KL 散度会很小，表示生成文本很自然，符合 LM 的预期；如果 Q 和 P 很远，那么 KL 散度会很大，表示生成文本很不自然，偏离了 LM 的预期。 wow!这跟我周末想的idea一样！！ 3 PPOOpenRLHF源码解读：1.理解PPO单机训练 - 知乎基于OpenRLHF库的源码来理解RL的几个基本算法。 PPO训练bash脚本： ​\tOpenRLHF/examples/scripts/train_ppo_llama.sh ​\topenrlhf/cli/train_ppo.py # 已找不到 ​\topenrlhf/trainer/ppo_trainer.pyFig1. PPO训练全过程阶段1（准备阶段）先基于预训练模型训练一个精调模型（SFT Model）和一个奖励模型（Reward Model）。Reward Model一般可以基于SFT model热启或基于预训练模型热启。阶段2（模型初始化阶段）：PPO过程，在线同时有4个模型，分别为 Actor Model：是我们要优化学习的策略模型，同时用于做数据采样，用SFT model热启。 Actor Model=SFT model++ ​\t\t\t\t\t =决策模型（优化训练它） ​\t\t\t\t\t =数据采样模型 Reference Model：代码中为initial_model，是为了控制Actor模型学习的分布与原始模型的分布相差不会太远的参考模型，通过loss中增加KL项，来达到这个效果。训练过程中该模型不更新 这两个都是基于SFT model Critic Model：是对每个状态做打分的价值模型，衡量当前token到生成结束的整体价值打分，用Reward Model热启 Reward Model ：这里实现的是ORM（Outcome Reward Model），对整个生成的结果打分，是事先训练好的Reward Model。训练过程中该模型不更新 这两个都是基于Reward model阶段3：采样Experience数据，这个过程比较复杂，单独梳理一文。简述流程为： 首先采样一批随机指令集（Prompt） 调用Actor模型的generate()方法，采样1条或多条结果（sequences） 四个模型一起参与组装Experience的多个Tensor域，用于后续模型训练 阶段4: 用Experience样本，训练 Actor Model 和 Critic Model，后面单独一文介绍重复3-4阶段，循环采样Experience数据-&gt; 模型训练 ，直到loss收敛上面大体介绍了PPO训练的过程，下面会继续细化讨论几个关键的问题： 4个模型结构具体长啥样？Actor Model，Reference Model，Critic Model， Reward Mode 采样过程具体是如何做的？详见： 姜富春：OpenRLHF源码解读：2.PPO训练Experience数据采样过程 模型训练过程有哪些细节？详见：姜富春：OpenRLHF源码解读：3.PPO模型训练过程本文继续讲解下模型结构，采样和模型训练过程已单独拆成两篇文章介绍。TRL.PPO args (PPOConfig):配置对象，包含PPO算法的所有超参数，例如学习率、批量大小、梯度累积步数等。 processing_class:数据预处理类，通常是PreTrainedTokenizerBase或类似的类，用于对输入文本进行编码和解码。 model (nn.Module):策略模型（Policy Model），即需要训练的主模型。 ref_model (nn.Module):参考模型（Reference Model），通常是一个冻结权重的版本，用于计算KL散度或其他对比指标。 reward_model (nn.Module):奖励模型（Reward Model），用于评估生成的响应质量。 train_dataset (Dataset):训练数据集，包含用于训练的样本。 value_model (nn.Module):价值模型（Value Model），用于估计状态值函数。 data_collator:数据收集器，用于将样本批量化并填充到相同的长度。 eval_dataset (Dataset):验证数据集，用于评估模型性能。 optimizers:包含优化器和学习率调度器的元组，默认为(None, None)。 callbacks:回调函数列表，用于在训练过程中执行额外操作（如日志记录、保存模型等）。 peft_config:如果使用PEFT（Parameter-Efficient Fine-Tuning）技术，则需要提供PEFT配置。 args.stop_token 和 args.stop_token_id:指定生成过程中使用的停止标记，用于截断生成的序列。 args.per_device_train_batch_size:每个设备上的训练批量大小。 args.gradient_accumulation_steps:梯度累积步数，用于模拟更大的批量大小。 args.num_mini_batches:每个更新步骤中的小批量数量。 args.whiten_rewards:是否对奖励进行标准化处理。 is_deepspeed_enabled:是否启用DeepSpeed以加速训练。 is_fsdp_enabled:是否启用FSDP（Fully Sharded Data Parallel）以分布式训练。 args.total_episodes:总训练回合数。 args.response_length:生成响应的最大长度。 args.kl_coef:KL散度的系数，用于控制策略更新的幅度。 args.gamma:折扣因子，用于计算优势函数。 args.lam:泛化优势估计（GAE）的平滑参数。 args.cliprange:PPO策略裁剪范围。 args.cliprange_value:价值函数裁剪范围。 args.vf_coef:价值损失的权重。 args.missing_eos_penalty:对未包含EOS标记的生成序列施加的惩罚。模型区别 AutoModelForCausalLM 这是一个标准的因果语言模型（Causal Language Model, CLM），用于生成文本。 它没有额外的价值头（Value Head），仅用于生成任务。 AutoModelForSequenceClassification 这是一个用于序列分类任务的模型，通常用于情感分析、文本分类等任务。 它的输出是一个分类标签的概率分布，而不是生成的文本。 AutoModelForCausalLMWithValueHead 这是在 AutoModelForCausalLM 基础上扩展的模型，增加了价值头（Value Head），用于估计状态值函数。 它结合了生成能力和强化学习中的价值估计功能，是 PPO 训练中常用的模型。是否需要不同的模块加载？在 PPO 训练中，model、ref_model、reward_model 和 critic_model 的选择取决于具体需求： model 主策略模型，通常使用 AutoModelForCausalLM，因为它包含价值头，适合 PPO 训练。 ref_model 参考模型，通常使用 AutoModelForCausalLM，并冻结其参数，用于计算 KL 散度。 reward_model 奖励模型，可以使用 AutoModelForSequenceClassification 或其他适合评估生成质量的模型。 critic_model 批评模型，通常使用 AutoModelForSequenceClassification 或专门创建的批评模型。 测试1：bug卡在这里：调试：排查出来了，是value_model用的AutoModelForSequenceClassification，导致out_features=2换了官方example的代码也是报这个错，换了官方测试用的`trl-internal-testing/descriptiveness-sentiment-trl-style1`后可以了，~~所以是数据集`imdb`的问题。~~ 不对，我找到了是我没设置num_labels=1!!Related Work作者将RL在NLP的研究分为3类： 针对NLP的模仿学习：模仿学习是一种利用专家的指导来学习策略的方法，它适用于有明确的监督信号和反馈的任务，比如机器翻译或文本摘要。它的兴起是由于在强化学习（RL）中通过手动编程其行为或通过奖励函数来定义Agent的行为变得异常困难。这是因为这些环境需要高度的灵活性和适应性，很难指定一个能够考虑到所有可能情况的最佳规则或奖励信号集。这就是模仿学习（IL）发挥作用的地方 - 一种通过模仿专家的行为来学习所需行为的过程，这些行为是通过示范提供的。 针对大动作空间的RL大动作空间的强化学习是一种针对动作空间过大而导致探索困难的问题的方法，它适用于有多种可能的输出的任务，比如对话系统或图像描述。 针对NLP的一般RL一般的强化学习是一种根据环境的奖励来学习策略的方法，它适用于有不确定性或多目标的任务，比如问题生成或文本游戏。" }, { "title": "因果推理与大语言模型：开辟因果关系的新前沿", "url": "/posts/PaperNote_%E5%9B%A0%E6%9E%9C%E6%8E%A8%E7%90%86%E4%B8%8ELLMs/", "categories": "NLP, 论文笔记", "tags": "nlp, 预训练模型, 因果推理, 论文笔记", "date": "2023-05-09 00:00:00 +0800", "snippet": "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality 这篇论文一共42页，值得反复研读。当处理真实世界的因果任务时，人们在制定(子)问题、迭代和验证其前提和含义时，策略性地在基于逻辑和协方差的因果推理之间交替。简单来说，这句话描述了人们在处理真实世界中的因果任务时，会使用不同类型的推理方法来解决问题。基于逻辑的推理是指根据一些已知的事实和命题，依据规则推出其他命题的过程。而基于协方差的推理则是指通过分析两个或多个变量之间的协方差关系来推断它们之间是否存在因果关系。基于逻辑的推理是指根据一些已知的事实和命题，依据规则推出其他命题的过程。在数学学科中，逻辑推理需要运用数学运算、空间想象、数学公式定理等数学基础知识，对相应的数学问题进行分析和推断。在知识图谱领埴，基于逻辑规则的推理是一种常见的方法。它通过定义或学习知识中存在的规则进行挖掘与推理。例如，可以通过自动化的规则学习方法，快速有效地从大规模知识图谱中学习出置信度较高的规则，并且应用于推理任务。协方差是一种用来衡量两个变量之间线性关系强度的统计量。它表示两个变量在一起变化的程度，可以用来衡量两个变量之间的相关性。如果两个变量之间的协方差为正值，说明这两个变量在一起增加或减少；如果协方差为负值，则说明当一个变量增加时，另一个变量减少。如果协方差为零，则说明这两个变量之间没有线性关系。需要注意的是，协方差只能衡量两个变量之间的线性关系，而不能确定它们之间是否存在因果关系。" }, { "title": "自监督与注意力机制的关系", "url": "/posts/%E8%87%AA%E7%9B%91%E7%9D%A3%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E5%85%B3%E7%B3%BB/", "categories": "", "tags": "", "date": "2023-04-06 00:00:00 +0800", "snippet": "Transformer、BERT和“自监督学习”、“自注意力机制”的关系Transformer和BERT这两个模型到底算是“自监督学习”还是“自注意力机制”？正确的圈定到底是什么？ “交流中碰撞思想，沟通中凝聚共识”，今天跟王博的讨论时启发我关注到自监督学习这一块，之前对于BERT家族的理解基本都是从注意力机制这一条路探究的，受益匪浅，原来预训练任务还可以从自监督学习这个方向去定义。自监督和自注意力应该是属于两个山头，在AAAI2020大会上，Hinton、LeCun和Bengio分别发表了主旨演讲：Hinton主张无监督版本的Capsule网络；LeCun主张自监督学习；Bengio主张注意力机制。 插嘴一下，LeCun是Hinton的博士后，在麻省理工学院时Bengio又是Jordan的得意门生，随后Bengio在贝尔实验室与LeCun成为同事。Hugo Larochelle在Bengio下面读的博士，后成为Hinton的博士后；LeCun的一位博士生MarcAurelio Ranzato，后也成为的Hinton的博士后。我查阅了在Transformer的原文《Attention is All You Need》中全文没有提到自监督学习，同样在BERT的原文《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》中也是全文没有提到自监督学习。这两篇Paper都是Google AI Lab提出的，而Google Brain的机器学习科学家是Bengio的兄弟Samy Bengio。截至2023年，大家公认的预训练语言模型的关键技术分为三大块： 网络结构上说：Encoder-Decoder架构 &amp; 多头自注意力机制 预训练任务上说：隶属于“自监督学习”机制的预训练任务MLM和NSP 下游任务上说：Fine-tune和Prompt-predict所以，AI江湖上称： Hinton是AI教主，始作俑者，开创先河；Lecun是独行侠，负责东搞西搞，工业学术两不耽误；Bengio是金牌打手，坚守学术界阵地，做理论实验支持。相当有道理啊，现在主导AI的Transformer/BERT/GPT这一个家族的模型都是以自注意力机制为“骨”，以自监督学习为“血液”，而自注意力机制是Lecun提出的，自监督学习则是Bengio提出的。Reference： 预训练语言模型的进展与趋势 【AI大咖】扒一下低调的Yoshua Bengio大神 三巨头共聚AAAI:Capsule没有错，LeCun看好自监督，Bengio谈注意力 BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Attention Is All You Need" }, { "title": "知识增强技术总结", "url": "/posts/%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/", "categories": "NLP, 基础知识", "tags": "nlp, 知识增强, 技术综述", "date": "2023-03-15 00:00:00 +0800", "snippet": "知识增强的价值关键预训练语言模型目前的瓶颈： 不可解释，黑盒模型 下游任务需要大量标注数据虽然少样本中人工给出思维链提示的成本很小，但这种注释成本相对于微调还是令人望而却步（也可以用synthetic data generation合成数据生成, or zero-shot generalization零样本泛化来处理这个问题）。 推理能力差ChatGPT和之前PLM的创新： 小样本提示学习和指令学习 思维链（Chain of Thought，COT）补充了逻辑推理过程（知识）给模型 基于人类反馈的强化学习（Reinforcement Learning with Human Feedback，RLHF） 训练数据中补充了过程性知识（代码）ChatGPT在专业性强的问题上“一本正经的胡说八道” 提高结果的可信度【知识】 提高推理能力，改善模型的可解释性差的问题【知识】一、技术研究：知识增强【What】知识是什么？ 从业务的角度：事实知识（陈述性知识）、机理知识（过程性知识）、数据知识 从研究的角度：“知识”有两种不同的分类方法[灰色色块为研究中常用的，可实现的数据类型] 2.1. 按照不同来源分类：内部知识、外部知识 2.2. 按照不同性质分类 [根据北大《人工智能原理》]“可变性”：那么知识分为静态/动态；“可理解性”：那么知识分为表层/深层；“内容的性质”：那么知识分为陈述性/过程性；…【How】如何表征知识？根据上面的分类，内部或外部知识通常用(1)实体（三元组中的实体）词典；(2)知识图谱；(3)纯文本直接作为补充知识；(4)与上下文有关系的图像。确定性知识和不确定性知识确定性知识：通常用(1)语义网络；(2)知识图谱；(3)框架语言frame；(4)一阶逻辑；(5)命题逻辑；(6)模态逻辑；(7)描述逻辑；(8)本体…，他们分别由各自的使用情况和局限性。不确定性知识：不确定性知识指的是不精确（imprecise）、不完全（incomplete）、随机性（stochastic）的知识。表示方法如下过程性知识和陈述性知识 过程性知识描述“怎么做”的知识，描述解决问题的过程，通常也是从已有的知识中整理出来的规则，它具有动态的特征，不同情况不同任务下动态变化。陈述性知识描述“是什么”的知识，往往是事实性知识，包括事物、事件、过程描述、属性、关系这些知识。💡 ChatGPT引入了代码数据作为预训练数据—KEPLM：引入Knowledge到PLM—【How】（知识）增强/注入的方法PLM的网络结构层有：Input、Embedding层、Encoder层PLM的训练任务有：Masked Language 掩码任务和NSP下一句预测任务💡这些地方都可以作为知识注入的切入口。M1：修改Input思路1： 在知识图谱中找$e1$对应的实体的三元组插入在input文本中 把$e1$对应的实体描述插入在input文本中例子： 2个例子 ERNIE 3.0 框架图 \t 一篇来自中文信息处理实验室的发布在AAAI的论文：Benchmarking Knowledge-Enhanced Commonsense Question Answering via Knowledge-to-Text Transformation. \t 思路2：先把原始input文本组织成图结构，再和来自于知识图谱中的子图拼接（可以根据input中的实体词或者其他），构建成补充后的图结构，再重新展平成文本序列的形式作为PLM的输入。 2个例子 [ACL]CoLAKE 知识增强 \t [AAAI]K-BERT 知识增强 \t M2：在Encoder层中增加知识融合模块(1) on top of the entire PLM：我们可以在n个Encoder整体之后增加一个知识融合模块；(2) between the Transformer layers of PLM：我们也可以单层的Encoder增加知识融合模块，这样n个Encoder就重复n次；(3) inside the Transformer layers of PLM：Encoder包含着很多子层比如多头自注意力层，feed forward层等等，我们也可以在这其中插入知识融合模块。 三个例子 在n个Encoder整体之后增加知识融合模块的例子： 清华ERNIE \t 在单层的Encoder增加知识融合模块的例子： \t 在Transformer层内部增加知识融合模块的例子 [ACL]KALA \t M3：增加或修改预训练任务(1) 修改掩码任务 1个例子 [中科院]E-BERT \t (2) 增加知识相关的预训练任务 1个例子 [ACL Trans]KEPLER \t 【评估】如何评估KEPLM的好坏过去，评价KEPLM的优劣通常通过它生成的Representation（表征）的理解能力好坏来定义。现在以及以后，KEPLM的推理能力将是我们更关注的点，因为它们已经在GLUE/CLUE任务上表现得足够好了。💡 ChatGPT引入了思维链CoT增强PLM的推理能力Reference标题\t文献来源\t发表年份 1\tA Survey on Knowledge-Enhanced Pre-trained Language Models\tIEEE TRANS\t2023.01 2\tA Survey of Knowledge Enhanced Pre-trained Models\t\t2022.06 3\tA Survey of Knowledge-Intensive NLP with Pre-Trained Language Models\t\t2022.02 4\t知识图谱构建技术综述\t计算机工程\t2022 5\t新一代知识图谱关键技术综述\t计算机研究与发展\t2022 6\tA Survey on Knowledge Graphs: Representation, Acquisition and Applications\tIEEE transactions\t2021 7\tCoLAKE: Contextualized Language and Knowledge Embedding\tACL\t2020 8\tKEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation\tTransactions of ACL\t2021 9\tReasoning About Knowledge\t\t2003.01 10\tOn Commonsense Cues in BERT for Solving Commonsense Tasks\tACL-IJCNLP 2021\t2021.08 11\tWhat does BERT learn about the structure of language?\tACL\t2019.07 12\tA Structural Probe for Finding Syntax in Word Representations\tNAACL-HLT 2019\t2019.06 13\tA Closer Look at How Fine-tuning Changes BERT\tACL\t2022.03 14\tDirectProbe: Studying Representations without Classifiers\tACL\t2021.04 15\tEnhancing Self-Attention with Knowledge-Assisted Attention Maps\tNAACL 2022\t 16\tSKILL: Structured Knowledge Infusion for Large Language Models\tNAACL 2022\t 17\tKroneckerBERT: Significant Compression of Pre-trained Language Models Through Kronecker Decomposition and Knowledge Distillation\tNAACL 2022\t 18\tModularized Transfer Learning with Multiple Knowledge Graphs for Zero-shot Commonsense Reasoning\tNAACL 2022\t 19\tKG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning\tAAAI\t2021.01 20\tChain of Thought Prompting Elicits Reasoning in Large Language Models\tNeurIPS\t2023.01 21\tTraining Verifiers to Solve Math Word Problems\t\t2021.09 22\tJAKET: Joint Pre-training of Knowledge Graph and Language Understanding\tAAAI\t2021.03 23\tMemory and Knowledge Augmented Language Models for Inferring Salience in Long-Form Stories\tEMNLP\t2021.08 24\tBenchmarking Knowledge-Enhanced Commonsense Question Answering via Knowledge-to-Text Transformation.\tAAAI\t2021.03 25\tEntities as Experts: Sparse Memory Access with Entity Supervision\tEMNLP\t2020 26\tERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation\t\t2021 27\tAutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\tEMNLP\t2020.08 28\tSemantics-aware BERT for Language Understanding\tAAAI\t2020.05 29\tKALA: Knowledge-Augmented Language Model Adaptation\t\t2022.04 30\tKnowledge-driven Natural Language Understanding of English Text and its Applications\tAAAI\t2021 31\tCommon Sense or World Knowledge? Investigating Adapter-Based Knowledge Injection into Pretrained Transformers\t\t2020 32\tKgPLM: Knowledge-guided Language Model Pre-training via Generative and Discriminative Learning.\t\t2020 33\tKG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning\tAAAI\t2020.11 34\tSKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis\t\t 35\tDKPLM: Decomposable Knowledge-enhanced Pre-trained Language Model for Natural Language Understanding\tAAAI\t2022 36\tPre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing.\tACM\t2021.07 37\tCoLAKE: Contextualized Language and Knowledge Embedding\tACL\t2020.08 38\tLUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention\tEMNLP\t2020" }, { "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "url": "/posts/PaperNote_%E6%80%9D%E7%BB%B4%E9%93%BE/", "categories": "NLP, 论文笔记", "tags": "nlp, ChatGPT, 推理, 论文笔记", "date": "2023-03-13 00:00:00 +0800", "snippet": "Chain-of-Thought Prompting Elicits Reasoning in Large Language ModelsBackground目前，较为常见的 NLP 推理任务大概可以被分成上图这三种：数学推理，常识推理以及符号推理。前两种很容易理解，就是字面意思，而最后一种，即符号推理，通常指的是给定一些特定符号代表的状态，然后对它们做一些操作，问题就是最后的状态是什么样的，例如给定一个列表，对其进行旋转操作，问你最终的状态是什么。下图中列出了这三种类型的任务的一些代表性数据集。语言模型可以用自然语言生成中间推理步骤，相比那些直接输出概率分布的分类模型，这为我们观察模型如何一步步达到最终答案提供了一个窗口。例如一个情感分类问题，语言模型能告诉你某句话里的某个词表达的是不满情绪，因而它将其分为负类，而用类似BERT这样的模型，要么是直接输出一个二分类的概率分布，要么是通过设计模板+完形填空的方式 (This movie is awesome. Sentiment: [MASK])，得到一个看似具有解释性的答案，但是其实模型输出的只不过是从一个二分类概率分布变成了一个全词表的概率分布，经过一些后处理，最终又变成了一个二分类的概率分布。总体上，我把介绍的相关工作分成三个主要的类型，如下图所示：一个生成器加一个验证器，思维链提示 (CoT Prompt) 以及前两种的混合方法。在第一种类型中，生成器负责生成多个推理路径，验证器用于评估这些生成的解答，并选出最终的答案。思维链提示主要是靠人工编写一些带有详细中间推理步骤的问答范本作为 Prompt 拼在要问的问题前面，模型模仿前面的例子对问题做出详细的推理和解答，这通常被称为语境学习 (In-context learning)。Total思维链：一系列中间推理步骤。 本文探究了“如何通过生成思维链来有效提升大语言模型的复杂推理能力”。 用“思维链prompt”这种方法，展示了大语言模型驯化出推理能力的过程。给了一个演示示例。 用3个实验，证明了“思维链prompt”提高了大语言模型的算术、常识、符号推理任务的性能。 发现模型有惊人的经验收获。例如，仅用8个思维链示例提示PaLM 540B就能在GSM8K数学字谜基准测试中达到最先进的精度，甚至超过了带有验证器的微调GPT-3。成果3针对第3点，介绍三个针对不同推理任务的实验：1. 算术推理3种不同的思维链探究了三种不同的（思维链生效原因）猜测 assumption A：它产生了要计算的数学方程 assumption B：思维链允许模型在更难的问题上花费更多的计算（即中间token） assumption C：提示允许模型更好地访问在训练前获得的相关知识（模型是否主要依赖的是思维链prompt）(1) Equation only.在给出答案之前，模型被提示只输出一个数学方程。图5显示，只有方程式提示对GSM8K没有太大帮助，这意味着GSM8K中问题的语义太具有挑战性，无法在没有思维链中的自然语言推理步骤的情况下直接转换为方程式（模型还不会把方程直接理解为语言）。(2) Variable compute only.只提示变量，（将变量计算从思维链的推理过程种解放出来，就是说给模型减轻负担），给模型提示“仅输出一个…”，这个…等于解决问题所需的方程的字节数。这一变体的表现与基线大致相同，这表明变量计算本身并不是思想链提示成功的原因，而且通过自然语言表达中间步骤似乎很有用。(3) Chain of thought after answer.在答案之后给出思维链的提示，这样研究模型是否实际上依赖于产生的思维链来给出最终答案。这一变体的表现与基线基本相同，这表明思想链中体现的顺序推理在激活知识之外的原因上是有用的。2. 常识推理Benchmarks-5个常识数据集Dataset1：CSQA提出了关于世界的常识性问题，涉及复杂的语义，通常需要先验知识。Dataset2：StrategyQA需要模型推断多跳策略来回答问题。这两个都来自于BIG-bench的验证集：Dataset3.1：Date从给定的上下文中推断日期。Dataset3.2：Sports确定与体育相关的句子是否合理或不可信。Dataset4：SayCan从离散集合中将自然语言指令映射到一系列机器人动作。Prompts对于Dataset1：CSQA和Dataset2：StrategyQA，从训练集中随机采样一些样本，人工构造思维链形成few-shot范例。对于那两个来自BIG-bench的数据集Dataset3.1：Date&amp;Dataset3.2：Sports,由于没有训练集，所以选前十个样本作为范例，并报告了剩余验证集中的数量。对于Dataset4：SayCan，使用来自 Ahn 等人使用的训练集的六个示例。 (2022) 以及手动组合的思维链。Results3. 符号推理Tasks Last letter concatenation.接龙游戏 Coin flip.翻硬币游戏 Resultsout-of-domain (OOD)思维链的鲁棒性略发散在此基础上，针对零样本场景，利用推荐关键词“Let’s think step by step”生成中间步骤的内容，从而避免人工撰写中间步骤的过程。尽管思维链模拟了人类推理的思维过程，但仍然不能确定神经网络是真的“推理”了。仍是悬而未决的问题。虽然少样本中人工给出思维链提示的成本很小，但这种注释成本相对于微调还是令人望而却步（也可以用synthetic data generation合成数据生成, or zero-shot generalization零样本泛化来处理这个问题）。推理路径不一定正确，不正确的推理路径会导致错误的答案。改进语言模型的事实生成（improving factual generations of language models）是未来的方向。尽在大型模型上用思维链推理，成本太高，能否使用小模型诱导推理？Referencehttps://zhuanlan.zhihu.com/p/607212335" }, { "title": "A Closer Look at How Fine-tuning Changes BERT", "url": "/posts/PaperNote_%E5%BE%AE%E8%B0%83%E5%A6%82%E4%BD%95%E6%94%B9%E5%8F%98BERT/", "categories": "NLP, 论文笔记", "tags": "nlp, 预训练模型, 论文笔记", "date": "2023-03-02 00:00:00 +0800", "snippet": "A Closer Look at How Fine-tuning Changes BERT1 Introduction本文探究了这三个问题： Does fine-tuning always improve performance?微调是否的确是总是能提高性能？ How does fine-tuning alter the representation to adjust for downstream tasks? 微调是怎么修正representation以适应下游任务的？ How does fine-tuning change the geometric structure of different layers?微调具体是怎么改变不同层的几何结构的？ 我的问题：微调怎么调整模型参数的？本文用 [2个probing-tech] 来检测BERT不同变体模型在 [5个下游任务] 上，representation的情况的：2个probing tech classifier-based probing 1 2 DIRECTPROBE 35个下游任务 词性标注 dependency head prediction依赖头检测 preposition supersense role function prediction函数预测 文本分类previous发现微调改变较高层而不是较低层，并且语言信息在微调过程中不会丢失。4 5本文的新发现 a. 微调引入了训练集和测试集之间的分歧，在大多数情况下，这不会损害泛化性。 b. 有一种情况，微调会损害泛化性。这种情况时，经过微调后，训练集和测试集之间的差异（分歧）也是最大的。 微调如何改变the representation space的labeled region。a. 对于任务标签 不可线性分离的表征，发现微调通过将具有相同标签的点归入少量的群组（最好是一个）来进行调整，从而简化了基础表征。b. 这样做使使用微调表示的标签比未微调untuned表示的标签更容易线性分离。c. 对于任务标签已经是线性可分离的表示，我们发现微调会将表示不同标签的点簇彼此推开，从而在标签之间引入较大的分离区域separating regions。簇不是简单地缩放点，而是以不同的方向和不同的范围移动（通过欧几里德距离测量）。d. 总的来说，与untuned的表示相比，这些簇变得遥远。我们推测，组之间的扩大区域允许一组更大的分类器，可以将它们分开，从而导致更好的泛化（§4.3）。本文通过研究跨任务微调的效果来验证这个“距离假设”。观察到，通过改变代表不同标签的簇之间的距离，相关任务的微调也可以为目标任务提供有用的信号（§4.4）。 微调不会随意地改变the higher layers。 微调在很大程度上保留了标签簇的相对位置，同时重新配置空间以适应下游任务。 Informally，可以说微调只是“slightly”改变了the higher layers。2 [2个probing tech] classifier-based probing 1 2基于分类器的探针，用于评估在一个任务中，representation对分类器的support程度。 DIRECTPROBE 3用于分析representation的几何结构 (1) classifier-based probing 1 2研究中，“经过训练的分类器”是最常用的probes。具体来说，如果想要了解，在一个task中，一个representation对labels的编码程度，那么就在它上面训练一个probing classifier（训练的时候，embeddings本身保持冻结）。在本文的研究中，用的是一个2-layers的神经网络作为probe classifiers。其他细节： 用网格搜索选择the best 超参数； 每一个最好的分类器都被用了不同的初始化训练了5次； matrices是每个分类器的平均准确率和标准差； hidden layer sizes从${32,64,128,256} \\times {32,64,128,256}$选择的 正则化权重范围$[10^{-7},10^0]$ 所有模型的hidden layers都使用ReLU作为激活函数 所有模型的优化器都是Adam 训练的iterations的最大值都在1000 scikit-learn v0.22分类器探针旨在衡量 [基于上下文的representation] 如何捕捉语言属性。分类性能可以帮助我们评估微调的效果。 classifier-based probing的局限性：它将representation视为一个黑盒，只关注最终任务的性能，而不能揭示finetune是如何改变空间的基本几何结构的。为什么要引入DIRECTPROBE，因为这个prob tech是从几何角度分析embedding的技术。（2）DIRECTPROBE对于给定label的任务，DIRECTPROBE是把具有相同label的points聚成簇，然后返回簇。无论是左图还是右图，决策边界都必须穿过这些簇之间的间隔区域。左图：一个简单的二元分类任务，虚线是决策边界右图：DIRECTPROBE的结果图（之一），灰色是“必须穿过的这些簇之间的间隔区域”，连接起来的点是DIRECTPROBE生成的簇。finetune的时候，不同的任务下，同一个词基于上下文的representation会有不同的表示，因此，有必要在给定任务的前提下，probe这些representation。得到了这些簇之后，就可以测量这些簇的属性，比如下面这三个： Number of Clusters(1) 簇的数量 = label的数量 简单的线性多任务分类器(2) 簇的数量 &gt; label的数量 就说明至少有两个同label的样本没有聚到一个簇里，那就需要非线性分类器 Distances between Clusters簇之间的距离揭示了representation的内部结构，通过跟踪fine-tuning期间，这些距离的变化，可以研究representation是怎么变化的。为了计算这些距离，本文基于定理fact：一个簇代表一个凸对象a convex object。这样就可以使用max-margin separators最大边距分隔符来计算距离。本文训练了一个线性SVM来找到max-margin分隔符并计算它的margin。簇之间的距离=2*margin。 Spatial Similarity簇之间的距离也可以揭示2个representation的空间相似性。如果两个representation在簇之间有相似的相对距离，那么对于当前的任务，这2个representations是相似的。 用这些distance组成一个distance vector $v$,把它当作representation，$v_i$是一对label的簇们之间的距离。一个任务有n个label:(1) 当数据集在这个representation下线性可分离，也就是说簇的数量 = label的数量,那么：\\(size(v)=\\frac{n(n-1)}{2}\\)PS.本文研究的大多数representation都是这种情况。(2) 非线性可分的情况好像没讨论 ？对于一个带label的任务，还可以计算两个representation的距离向量之间的皮尔逊相关系数，把这个作为两个representation的一种相似性度量。另外，这个系数也可以用来衡量两个有标签的数据集在相同representation时的相似性。本文利用这一观察结果来分析训练集和测试集在fine-tuned representation下区别。实验设置3.1 Representation这些模型basic架构是相同的，但是能力不同（比如不同的layers和hidden size）。它们都是基于英文文本&amp;uncased。对于那些被tokenizer分解为subword的tokens，本文把这些token 的representation的subword embedding进行平均。 HuggingFace v4.2.1Pytorch v1.6.03.2 Tasks这些任务覆盖①syntactic语义②semantic语法，这两方面的BERT模型的能力。 Part-of-speech tagging (POS)词性标注这个任务帮助我们理解是否这个representation捕捉了粗粒度的语义分类 Dependency relation (DEP)预测两个tokens之间的语义依赖关系。这个任务帮助我们理解这个representation是否可以描述words之间的语义关系，以及能描述到什么程度。这个任务中，需要给一对tokens分配一个类别。具体来说就是，把两个token的（BERT生成的）基于上下文的representation连接起来，这个视为“对”的representation。数据集同POS。 Preposition supersense disambiguation介词超义消岐分类任务，为了消除介词的语法含义的歧义。本文仅在Streusle v4.2 corpus的single-token介词上训练和evaluate。 预测介词的语义角色semantic role(PS-role) 预测介词的语义功能semantic function(PSfxn) Text classificationTREC50数据集，每个句子都有50个语法label。用的是[CLS]token作为句子的representation。这个任务展示了representation表征一个句子的能力。3.3 Fine-tuning Setup分别单独在上面提过的5个任务上fine-tune那些models。fine-tuned之后的模型生成基于上下文的representation。初步实验： 通常，对于BERT_tiny来说，3-5个epoch的fine-tune不够，这些小的representation需要更多的epoch。 除了$BERT_{base}$，其他的models都用10epochs来fine-tune；$BERT_{base}$用3epochs。 PS.fine-tune阶段和用于probing的分类器训练阶段分开的，probing classifier是在原始representation和fine-tuned之后的representation之上从头开始train一个2layer神经网络，确保比较是公平的。发现和分析 用classifier去probing是否fine-tuning总是提高分类器的性能。(§4.1) 用DIRECTPROBE给出了一个几何解释，解释为什么fine-tuning提高分类器性能了。(§4.2 and §4.3) 用跨任务fine-tune确认几何解释。(§4.4) 分析fine-tuning是如何改变BERT_base的不同层的几何形状的。(§4.5)4.1 Fine-tuned的性能上面这个表是BERT_small的结果，tuned指的是在模型最后一层的基础上fine-tuned的结果。最后一列是训练集和测试集的空间相似度Spatial Similarity。这个表是上表的完整版。一些条目丢失，因为相似性只能在对给定任务线性可分的表示上计算。结论①：fine-tuning分散了训练集和测试集在微调之后，所有的相似性都会降低，这意味着由于微调，训练和测试集会有所不同。在大多数情况下，这种差异不足以降低性能。结论②：也有例外，fine-tuning损害了性能BERT_small在PS-fxn任务上，tuned之后的性能下降了，并且训练集和测试集的相似度仅0.44，所以作者猜测或许控制训练集和测试集的相似度可以确保微调是有益的。但不确定需要进一步研究。4.2 Representations的线性结论①：结论②：4.3 labels的空间结构结论①：结论②：4.4 跨任务fine-tuning结论①：结论②：4.5 Layer Behavior结论①：微调不会任意改变表示，即使对于更高的层也是如此结论②：通过上下两层的直观比较来分析不同层的变化。这里选的是BERT_base，POS tagging任务。基于POS标签任务和BERTbase的微调前后标签质心差向量的PCA投影。基于dependency prediction task和BERTbase的微调前后标签质心之间差异向量的PCA投影。基于Supersense function task和BERTbase的微调前后标签质心差向量的PCA投影.基于Supersense role task和BERTbase的微调前后标签质心之间差异向量的PCA投影。图7-10显示了基于BERTbase进行微调前后标签质心差向量的PCA投影。Note：1. BERT的预训练和微调，微调和P-tuningBERT的微调过程中是有反向传播的。微调是在预训练模型的基础上，为了适应下游任务而对所有参数进行调整的过程。反向传播是一种优化算法，用于计算梯度并更新参数。预训练和微调的区别是： 预训练是用大量的未标记数据来训练一个通用的模型，例如BERT，以学习语言的特征和表示。预训练：随机初始化一个网络模型的参数，然后用大量的未标记数据来训练模型，直到模型的损失越来越小。将训练好的模型的参数保存下来，作为预训练模型。 微调是用少量的标记数据来调整预训练模型的参数，以适应特定的下游任务，例如文本分类、命名实体识别等。微调：使用预训练模型的参数作为一个新任务的初始化参数，然后用少量的标记数据来训练模型，根据结果不断进行一些修改。将修改后的模型保存下来，作为微调模型。 P-tuning和微调的区别是： P-tuning是一种提示优化方法，它只更新预训练模型中的一些特殊的token（如[unused*]），而不更新整个模型的参数。这些特殊的token可以作为模板来引导模型进行下游任务。 微调是一种常用的迁移学习方法，它更新预训练模型中的所有参数，以适应下游任务。微调需要更多的内存和计算资源，而且容易过拟合。P-tuning更新参数，但只更新一些特殊的token，而不是整个模型的参数。这些特殊的token可以看作是模型的前缀，它们可以影响模型的输出。P-tuning只需要很少的参数来微调，因此可以节省内存和计算资源。P-tuning只更新了一些特殊的token，比如[unused1]～[unused6]，它们可以看作是模型的前缀。这些token可以根据标注数据来学习，从而影响模型的输出。其他的模型参数都是冻结的，不会更新。2. 代码不可复现因为用了gurabi optimizer，docker容器中的授权很难搞要联系客服3. 什么是凸对象？Reference What do you learn from context? Probing for sentence structure in contextualized word representations &#8617; &#8617;2 &#8617;3 Probing what different NLP tasks teach machines about function word comprehension. &#8617; &#8617;2 &#8617;3 DirectProbe: Studying Representations without Classifiers &#8617; &#8617;2 What Happens To BERT Embeddings During Fine-tuning? &#8617; On the Interplay Between Fine-tuning and Sentence-level Probing for Linguistic Knowledge in Pre-trained Transformers &#8617; " }, { "title": "KEPLER：知识嵌入和预训练语言表示的统一模型", "url": "/posts/PaperNote_KEPLER/", "categories": "NLP, 论文笔记", "tags": "nlp, 知识图谱, 知识表征, 预训练模型, 论文笔记", "date": "2023-02-13 00:00:00 +0800", "snippet": "KEPLER：知识嵌入和预训练语言表示的统一模型Figures1. 首图这是论文的首图，示范了给知识图谱中每个实体增加描述的效果。2. （MODEL）图一一句话概括KEPLER通过“联合训练两个目标objectives”来把事实知识隐式融合进语言表征中。组件1：Encoder每个Token经过L层hiddenlayers的词表征：\\(H_i∈ \\Bbb{R}^{N×d},1≤i≤L\\)\\[H_i = E_i(H_{i-1})\\]句子表征：\\(E_{&lt;s&gt;}(\\cdot)\\)PS:Encoder这部分本文没有调整Transformer编码器结构，没有增加额外的实体链接或者知识融合层。这样没有额外的推理开销，应用到下游任务也可以像RoBERTa一样用。组件2：知识Embedding一句话概括这个子组件在预训练中采用知识嵌入目标（KE Objective），把知识整合进去。具体来说：没有使用固定的嵌入（存储好的嵌入），而是使用他们对应的文本将实体编码为向量。 背景知识：在传统的 KE 模型中，每个实体和关系都被分配了一个 d 维向量，并定义了一个评分函数来训练嵌入和预测链接。通过选择不同的文本描述&amp;不同的KE评分函数得到了多种KE-objective选择方案（KEPLER的多种变体）。(1) 嵌入=实体描述对于一个三元组$(h,r,t)$:\\(h=E_{&lt;s&gt;}(text_h)\\)\\[r=T_r\\]\\[t=E_{&lt;s&gt;}(text_t)\\]注释： $text_{h/t}$指头节点/尾节点的那个实体词的文本描述。 $$是指序列第一个的那个句表征 $T \\in \\Bbb{R}^{ \\cal R \\times d} $ 是存储的关系嵌入，每一个都是d维 KE objective/损失函数[^方法1的损失函数]：（负采样作为有效优化）\\({\\cal{L}}_{KE}=-log \\sigma (\\gamma - d_r({\\bf h,t})) - \\sum_{i=1}^{n} \\frac{1}{n} log\\sigma(d_r( {\\bf h_i^{'},t_i^{'}} )-\\gamma)\\)注释： $(h_i^{‘},r,t_i^{‘})$是负样本 $\\gamma$是margin $\\sigma$是sigmoid函数 $d_r$是打分函数，打分函数是按照TransE的打分函数，因为简单\\[d_r {({\\bf h,t})} = \\lVert {\\bf h+r-t} \\rVert _{\\it p}\\]注释： 范数$\\it p$=1 负采样策略是固定头部实体，随机抽样尾部实体，反之亦然。(2) 嵌入=实体和关系描述(3) 嵌入=以关系为条件的实体嵌入组件3：MLM objective仅使用KE objective训练可能会造成灾难性遗忘（本文实验证明的确会差），因此保留MLM objective作为训练目标之一。使用$RoBERTa_{BASE}$作为预训练初始的checkpoint。组件4：训练Objectives设计了一个多任务loss把事实知识和语言理解整合到一个PLM中。 文中说到：把两个损失联合优化可以隐式整合外部KG的知识到the text encoder中，并且同时保持了PLM的语言学理解能力和语义学理解力。\\[{\\cal L = L}_{KE}+{\\cal L}_{MLM}\\]注释： ${\\cal L}_{KE}$是KE objective的损失 ${\\cal L}_{MLM}$是MLM objective的损失注意：（对于每一个mini-batch）这两个任务仅仅共享the text encoder，这两个任务采样的数据不一定一样。这是因为在 MLM 中看到各种文本（而不仅仅是实体描述）可以帮助模型具有更好的语言理解能力。变量和实现1.变量7个KEPLER变体来测试方法的有效性(1) KEPLER-Wiki【基准模型】KG：Wikidata5M知识表征方式：Entity Descriptions as EmbeddingsTraining Objectives：\\(\\mathcal{L} = \\mathcal{L}_{KE} + \\mathcal{L}_{MLM}\\)评价：大部分任务中表现最好(2) KEPLER-WordNetKG：WordNet3.0（节点是引理和同义词集，边是他们的关系）知识表征方式：Entity Descriptions as EmbeddingsTraining Objectives：\\(\\mathcal{L} = \\mathcal{L}_{KE} + \\mathcal{L}_{MLM}\\)评价：直观地说，结合WordNet可以带来词汇知识，从而有利于NLP任务。PS：KnowBert也是用的WordNet3.0,是从nltk2包中提取的(3) KEPLER-W+WKG：Wikidata5M，WordNet知识表征方式：Entity Descriptions as EmbeddingsTraining Objectives：\\(\\mathcal{L} = \\mathcal{L}_{Wiki} + \\mathcal{L}_{WorkNet} + \\mathcal{L}_{MLM}\\) 其中$\\mathcal{L}{Wiki}$和$\\mathcal{L}{WorkNet}$分别代表Wikidata5M和WordNet的损失。(4) KEPLER-RelKG：知识表征方式：Entity and Relation Descriptions as Embeddings\\(\\hat{r}=E_{&lt;s&gt;}(text_r)\\)Training Objectives：评价：由于Wikidata中的关系描述较短(平均11.7个单词)且同质，将关系描述编码为关系嵌入会导致较差的性能，如第4节所示。(5) KEPLER-CondKG：知识表征方式：entity embedding conditioned on relation method\\({\\bf h}_r=E_{&lt;s&gt;}(text_{h,r})\\)Training Objectives：评价：该模型在链接预测任务中取得了优异的结果，transductive and inductive(6) KEPLER-OnlyDesc直接在KE objective的实体描述上训练MLM objective，而不是使用英文维基百科和BookCorpus作为KEPLER的其他版本。实体描述数据仅2.3GB，并且是同质的评价：损害一般的语言理解能力，因此表现更差(章节4.2)。(7) KEPLER-KE只采用了KE目标，这是一个经过删减的KEPLER-Wiki。它用来表明MLM目标对于语言理解的必要性。2.预训练实现3.微调实现用不同的NLP任务和KE任务作为评估模型效果的下游任务 NLP任务(1) 关系分类给定2个实体，判断它们之间的关系benchmark：TACRED和FewRel2个框架：Proto + PAIR(2) 实体分类Entity Typing给定mention，把它分类到预定义的类型中。benchmark：OpenEntity(3) GLUE一般来说，解决GLUE不需要事实知识（Zhang等人，2019），我们使用它来检查KEPLER是否损害了一般语言理解能力。结论：这表明，在结合事实知识的同时，KEPLER保持了强大的语言理解能力。然而，KEPLER OnlyDesc的性能显著下降，这表明小规模实体描述数据不足以用MLM训练KEPLER。 KE任务(1) 实验设置链路预测： KEPLER获得实体（方程1）和关系（方程4）嵌入。 评估方法：3.3节讲过 baseline：RoBERTa和Our RoBERTa 打分函数：方程3In the transductive setting： 本文的方法和TransE做对比，维度=512，负采样size=64，batch size=2048，学习率=0.001（通过超参数检索得到） 负采样大小对KE任务的性能至关重要，但受模型复杂性的限制，KEPLER只能采用1的负采样大小。(2)transductive setting(3)Inductive Setting新数据集：Wikidata5MReference：" }, { "title": "图神经网络", "url": "/posts/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/", "categories": "NLP, 模型研究", "tags": "图神经网络", "date": "2022-10-20 13:56:56 +0800", "snippet": "图神经网络记录日期：2022/10/20📃参考文章[^GCN&amp;GNN]:https://blog.csdn.net/kc7w91/article/details/121595567欧几里得数据&amp;非欧几里得数据： 欧几里得数据：排列整齐的数据 这类型的数据排列整齐，不同样本之间可以容易的定义出“距离”这个概念出来。我们且思考，假设现在有两个图片样本，尽管其图片大小可能不一致，但是总是可以通过空间下采样的方式将其统一到同一个尺寸的，然后直接逐个像素点进行相减后取得平方和，求得两个样本之间的欧几里德距离是完全可以进行的。 非欧几里得数据:排列不整齐的数据 对于数据中的某个点，难以定义出其邻居节点出来，或者是不同节点的邻居节点的数量是不同的[5]，这个其实是一个特别麻烦的问题，因为这样就意味着难以在这类型的数据上定义出和图像等数据上相同的卷积操作出来，而且因为每个样本的节点排列可能都不同，比如在生物医学中的分子筛选中，显然这个是一个Graph数据的应用，但是我们都明白，不同的分子结构的原子连接数量，方式可能都是不同的，因此难以定义出其欧几里德距离出来，这个是和我们的欧几里德结构数据明显不同的。因此这类型的数据不能看成是在欧几里德样本空间中的一个样本点了，而是要想办法将其嵌入(embed)到合适的欧几里德空间后再进行度量。而我们现在流行的Graph Neural Network便可以进行这类型的操作。 如何处理 非欧式空间 的结构（典型：社交网络）？——引入了GNN与GCN网络：利用『边的信息』对『节点信息』进行『聚合』从而生成新的『节点表示』。图卷积网络GCNCNN中的卷积是一种离散卷积（即加权求和），本质上就是利用一个共享参数的过滤器（kernel），通过计算中心像素点以及相邻像素点的加权和来构成feature map、实现空间特征的提取，其中的加权系数就是卷积核的权重系数(W)。图神经网络GNN[^自然语言处理中的图神经网络]NLP任务中，文本序列是由token包/词袋，例如BoW,TF-IDF bag of tokens：最简单的方式是将自然语言表示为一袋tokens，这种方式完全忽略了文本中tokens出现的具体位置，而只考虑其在文本中出现的次数。采取这种观点的最具代表性的NLP技术是话题模型。[^面向NLP的GNNs论文解读]-&gt;而随着词嵌入技术的成功，句子被表示为一个tokens序列,它通过预测目标单词的上下文单词来学习单词嵌入。-&gt;利用文本序列中的句子结构信息，即依存树和选区分析树等句法分析树，通过以下2种方法 合并特定任务的知识来扩充原始序列数据。 用序列数据中的语义信息（即语义解析图，如抽象意义表示图和信息提取图）来增强原始序列数据 常见的文本或知识的图表示包括依赖图、成分图、AMR图、IE图、词汇网络和知识图谱。此外，还可以构建一个包含多个层次元素的文本图，如文档、段落、句子和单词。与上述两种观点相比，这种自然语言的观点能够捕捉到文本元素之间更丰富的关系。许多传统的基于图的方法（如随机游走、标签传播）已经成功地应用于具有挑战性的NLP问题，包括词义消歧、命名消歧、共指解决、情感分析和文本聚类。 然后，这些图结构的数据可以编码实体tokens之间复杂的成对关系，以便学习更多的语义。But基于序列数据的深度学习网络不能直接适用于图结构数据。因此产生了以下挑战：* **如何自动将原始文本序列数据转换为高度图结构化的数据*** **如何正确地确定图表示学习技术**:需要使用专门设计的GNN来学习不同图结构数据的独特特征，如无向、有向、多关系和异质图。* **如何高效地建模复杂数据**:因为许多NLP任务涉及学习基于图的输入和高度结构化的输出数据之间的映射，如序列、树，以及具有多种类型节点和边的图数据。Research Background传统基于图的算法 基于随机游走的图算法 图聚类算法 图匹配算法 标签传播算法这些传统的基于图的方法在NLP领域有其局限性，比如表征能力弱以及没有统一的学习架构。图神经网络基础:cat:输入的是图。Graph filtering图谱过滤的分类以及一些代表方法 spectral-based:基于谱图理论 spatial-based:利用其空间近邻节点来计算该节点的嵌入表示 attention-based：将不同的注意力权重分配给不同的邻居节点 recurrent-based：门控机制，模型参数在不同的GNN层之间共享Graph pooling图谱池化层是为了以图为中心的下游任务来生成图级的嵌入表示。例如：图的分类-依据从图谱过滤中学习到的节点嵌入表示来进行预测的。（这是因为对于以节点为中心的任务来说，学习到的节点嵌入表示已经足够了。然而，对于以图为中心的任务，需要整个图的嵌入表示。）池化操作就是为了总结节点嵌入信息和图结构信息。图池化层可以分为两类：平面图池化（flat pooling）和分层图图池化（hierarchical pooling）。 flat pooling平面图池化直接从每步中的节点单独嵌入。 hierarchical pooling分层图池化包含几个图池化层，每个池化层之后又有好多图过滤器。面向NLP的GNNs分类[^面向NLP的GNNs分类]图1. NLP中GNNs三个方面的研究以及GNN的应用图的构造:wind_chime:输入是文本序列:interrobang:How？ 文本序列——-》图输入构建方法合集：1.静态图构建利用现有的关系解析工具（例如：依存解析）或手动定义的规则在预处理过程中构造图结构。静态图可以为原始文本序列引入隐藏的领域或外部知识，用丰富的结构化信息来扩充原始文本。 dependency graph:依赖图通常用于捕捉给定句子中不同对象之间的依赖关系。具体流程为：给定一个段落---》使用各种NLP解析工具（例如Stanford CoreNLP）获得依存分析树（例如句法依存树或语义依存关系树）---》从依存分析树中提取依赖关系，并将依赖关系转换为依赖性图。PS.文本有“顺序信息”，而图节点是无序的，可以引入顺序的link，在图结构中保留“顺序信息”。概括一下流程为： 1）构建依赖关系，2）构建顺序关系，3）最终的图转换。 constituency graph成分图通常用于捕捉一个或多个句子中基于短语的句法关系。PS.与依存分析不同的是，依存分析只关注单个词之间一对一的对应关系（即词级），而构成分析则对一个或几个对应词的组合进行建模（即短语级）。成分关系的基本概念：在语言学中，成分关系是指遵循短语结构语法的关系，而不是依存关系和依存语法。一般来说，成分关系是由主语（名词短语NP）— 谓语（动词短语VP）关系衍生出来的。PS.与依存关系解析树不同的是，所有的节点都有相同的类型，成分分析树区分了终端节点和非终端节点，非终端类别标记分析树的内部节点，叶子节点标记为终端类别。节点集可以表示为：1）非终端节点集2) 终端节点集。构成关系集合与树的边相关。成分图由非终端节点和终端节点组成，以及成分边和序列边。对于原文中相邻的每个单词节点对，在它们之间添加一条具有特定序列类型的无向边，用来保留顺序信息。图2 Dependency Graph and Constituency Graph Construction AMR graph AMR图是有根、有标注、有向、无环的图，它被广泛用于表示非结构化的具体自然文本的抽象概念之间的高级语义关系。 :yellow_heart:与句法上的特异性不同，AMR是高层次的语义抽象。:yellow_heart:更具体地说，在语义上相似的不同句子可能共享相同的AMR解析结果，例如，”保罗描述自己是一个战士 “和 “保罗对自己的描述：一个战士”，如图3所示。与之前介绍的依赖树和成分树类似，AMR图是由AMR解析树衍生出来的。 信息抽取图构建（Information Extraction Graph Construction）图4 Information Extraction Graph信息抽取图（IE Graph）旨在提取结构信息来表示自然句子之间的高级信息，例如基于文本的文档。这些提取出来的关系，捕捉到远距离句子之间的关系，在许多NLP任务中很有帮助。在下文中，为给定段落构建IE图的过程分为三个基本步骤。1）指代消解，2）构建IE关系，3）图的构建。 话语图构建（Discourse Graph Construction） 当候选文档太长时，许多NLP任务会受到长距离依赖性的挑战。话语图描述了两个句子之间的逻辑联系，可以有效地解决这种挑战。 知识图谱构建（Knowledge Graph Construction） 捕捉实体和关系的知识图谱（KG）可以大大促进许多NLP应用中的学习和推理。KG可以表示为G(V, E)，它通常由知识库中的元素构建。形式上，定义三元组作为知识库的基本元素，包括是源实体，目标实体和关系类型。然后，在知识库中添加两个节点，即源节点和目标节点，并从节点v1到节点v2添加一条边类型为rel的有向边。构建KG的第一件事是获取给定查询中的术语实例。然后，通过一些匹配算法（如最大子串匹配）将术语实例与KG中的概念联系起来。这些概念被看作是提取的子图中的初始节点。下一步是获取初始节点在KG中的1跳邻居。此外，人们可以通过应用一些图节点相关性模型，如个性化的PageRank（PPR）算法，来计算邻居与初始节点的相关性。然后根据结果，进一步修剪出相关性分数低于置信度阈值的边，并删除孤立的邻居。剩余的最终子图随后被用来给任何图表示学习模块提供信息。 similarity graph 相似性图旨在量化节点之间的相似性，在许多NLP任务中被广泛使用。由于相似性图通常是面向应用的，因此我们重点关注构建实体、句子和文档等各种类型元素的相似性图的基本程序，而忽略了具体的应用细节。相似性图的构建是在预处理过程中进行的，而不是以端到端的方式与其余学习系统共同训练。 图6中显示了一个相似性图的例子。 共指图构建（Coreference Graph Construction）在语言学中，当某个段落中的两个或多个术语指代同一个对象时，就会出现共指。许多工作表明，这种现象有助于更好地理解语料库的复杂结构和逻辑，解决歧义。为了有效地利用共指信息，共指图用来显式的建模隐性共指关系。给定一组短语，共指图可以连接文本语料库中指代同一实体的节点（短语）。 共现图构建（Co-occurrence Graph Construction） 共现图旨在捕捉文本中词与词之间的共现关系，这在许多NLP任务中被广泛使用，共现关系描述了在固定大小的上下文窗口内共同出现的两个词的频率，是捕捉语料库中词之间语义关系的一个重要特征。共现图的例子见图7 topic graph 话题图是建立在几个文档上的，其目的是对不同话题之间的高层次语义关系进行建模。给定一组文档，首先用一些话题建模算法，如LDA，学习潜在的话题。然后构建话题图，只有当文档具有该话题时，那么在文档节点和话题节点之间构建一条无向边。 应用驱动图构建（App-driven Graph Construction） 应用驱动图指的是为特定的NLP任务专门设计的图。在一些NLP任务中，用特定应用的方法通过结构化的形成来表示非结构化的数据是很常见的。例如，SQL语言可以自然地通过SQL解析树来表示。因此，它可以被转换为SQL图。由于这些图在领域知识的基础上过于专业化，所以没有统一的模式来总结如何建立一个应用驱动的图。图9是这种应用驱动图的一个例子，如SQL图。大部分静态图构造方法只考虑节点之间的某些特定关系，构建的图在一定程度上很好地捕捉到了结构信息，但没法直接利用多种不同类型的图关系。:interrobang:How？将多个图组合起来构建混合图？（这样能丰富图中的语义信息）静态图构建的问题/缺陷：首先，为了构建合理性能的图拓扑结构，需要广泛的人力和专业知识领域。其次，手动构建的图结构可能容易出错（例如，嘈杂或不完整性）。第三，由于图构建阶段和图表示学习阶段是不相交互的，因此在图构建阶段引入的错误无法被纠正，可能会累积到以后阶段，进而会导致性能下降。最后，图的构建通常是由机器学习从业者提供见解信息，这对下游预测任务可能不是最理想的。为了解决上述问题，提出了动态图构建方法:point_down:2.动态图构建大多数动态图构建方法旨在动态地学习图结构（即加权邻接矩阵），图构建模块可以与后续的图表示学习模块共同优化，从而以端到端的方式完成下游任务。下面两个组件是动态图构建的两个部分，（不是两种方法） 图谱相似性度量学习方法similarity metric learning 图相似度度量学习组件用于学习嵌入空间中的成对节点相似性的邻接矩阵 图谱稀疏化方法graph sparsity 稀疏化组件用于从全连接图中提取稀疏图。 图表示学习构建好了图，接下来利用各种图表示学习技术进行学习和特征提取。目标：找到一种方法可以通过机器学习模型将图结构和其属性的信息合并到低维嵌入中。原始文本数据构建的图要么是“异构图”要么是“同构图”。 同构图的各种图表示学习方法 原始同构图的场景 从异构图转换的场景 用于多关系图的基于GNN的方法 异构图的基于GNN方法基于图的encoder-decoder模型包含了Graph2Seq和Graph2Tree的编码器-解码器GNN模型架构 Graph2Seq通常采用基于 GNN 的编码器和基于 RNN/Transformer的解码器与 Seq2Seq模型架构相比，Graph2Seq 更善于捕捉输入文本的丰富结构信息，可应用于任意图结构数据，而且，Graph2Seq 模型在更广泛的 NLP 任务，包括神经机器翻译，文本摘要，问题生成，图谱到文本，SQL到文本，代码摘要以及语义解析。 Graph2Tree结合输入端和输出端的信息，使编码解码过程中的信息流更加完整。与考虑输入端结构信息的 Graph2Seq 模型相比，许多NLP 任务还包含复杂结构表示的输出，例如树结构输出（语法解析，语义解析等），它们也是结构信息丰富输出端。 GNNs助力NLP应用场景GNN在NLP中的应用基准数据集，评估指标GNNs for NLP的挑战和未来研究方向Reference" }, { "title": "Paper Note:词格法", "url": "/posts/PaperNote_%E8%AF%8D%E6%A0%BC%E6%B3%95/", "categories": "NLP, 论文笔记", "tags": "nlp, 语言学, 论文笔记", "date": "2022-10-08 14:10:56 +0800", "snippet": "Paper Note:词格法《Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models》作者：Yuxuan Lai, Yijia Liu, Yansong Feng, Songfang Huang, Dongyan Zhao阿里巴巴达摩院 | 中国北京大学教育部计算语言学重点实验室会议：创新点：在中文预训练语言模型中的语言表示部分使用“多粒度表示”目标模型：预训练语言模型目标任务：语义表征摘要：中文预训练的语言模型通常将文本作为字符序列来处理，而忽略了更粗略的颗粒度，例如单词。在这项工作中，我们提出了一种新的中文预训练范式–格子图，它明确地将词的表征与字符结合在一起，因此可以以多粒度的方式对一个句子建模。具体来说，我们从句子中的字和词构建一个格子图，并将所有这些文本单元送入转化器。我们设计了一个格子位置关注机制，以利用自我关注层中的格子结构。我们进一步提出了一个屏蔽段预测任务，以推动模型从格子中固有的丰富但冗余的信息中学习，同时避免学习意外的技巧。实验结果：在11个中文自然语言理解任务上的实验表明，我们的模型在12层设置下可以带来1.5%的平均增长，这在CLUE基准上的基础大小的模型中达到了新的先进水平。进一步的分析表明，Lattice-BERT可以利用格子结构，其改进来自于对冗余信息和多角化表示的探索。1.许多汉语单词的意思并不能通过直接组合其汉字的意思来完全理解2.具体来说，我们将句子中的字符和单词组织为单词晶格（见图1），这使得模型能够从所有可能的分词结果中探索单词。 Question 怎么整理成预训练模型的input？ 如何让模型理解词格之间的绝对位置和相对位置关系？ MLM任务不能用了，怎么办？ Challenge 首先，BERT的原始输入是按位置排序的字符序列，这使得它很难消耗单词格子并保留多格子单元之间的位置关系。其次，传统的掩蔽语言建模（MLM）任务可能会使基于词格的plm学习到意想不到的技巧。原因是这样的字格自然会引入冗余，即一个字符可以包含在多个文本单元中。在MLM中，模型可能会参考与随机屏蔽的文本单元重叠的其他文本单元，而不是真实的上下文，这带来了信息泄露。解决方法： 为了应对这些挑战，我们提出了一个基于格子的双向编码器表示法（Lattice-BERT）。具体来说，我们设计了一个格子位置注意力（LPA），以帮助变换器直接利用格子中文本单元之间的位置关系和距离。此外，我们还提出了一个屏蔽段预测（MSP）任务，以避免语言建模中重叠的文本单元之间的潜在泄漏。有了LPA和MSP，Lattice-BERT可以利用格子中的多粒度结构，从而直接利用格子结构来聚合粗粒度的单词信息，使各种下游任务受益。除了MSP，我们还使用Lan等人（2020）的句子顺序预测（SOP）任务对模型进行预训练，该模型预测两个连续的句子是否在输入中交换。词格构建：我们在由102K词汇组成的 基于由102000个词汇组成的格子 高频开放领域的词汇。所有出现在词汇表中的输入序列的子串被认为是输入的格子标记。使用Aho-Corasick自动机（Aho and Corasick, 1975），这个构建过程可以在与语料库和词汇量的线性时间内完成。为了处理子串无意义的英语单词和数字，我们对那些词汇外的非中文输入使用字符序列，并保留词汇内的单词和词块。我们根据词汇表使用所有可能的词来构建词格，而不是采用更复杂的词格构建策略。之前关于格子构建的研究工作（Lai等人，2019；Chen等人，2020；Li等人，2020b）表明，使用所有可能的词通常会产生更好的性能。我们认为过度设计的格子构造方法可能会使我们的模型在某些类型的文本上出现偏差，而且很可能会损害泛化效果。因此，在我们的案例中，我们让模型自己学习，以过滤在大规模语料库的预训练中使用所有可能的词所带来的噪音。预训练细节为了与之前的预训练工作相比较，我们实现了基础大小的模型，其中包含12层，768维的隐藏大小，以及12个注意力头。为了证明格子如何在较浅的架构中获得收益，并提供轻量级的基线，我们还进行了6层、8个注意头和512个隐藏尺寸的轻型模型。为了避免大词汇量在嵌入矩阵中引入过多的参数，我们采用了Lan等人（2020，ALBERT）的嵌入分解技巧。因此，Lattice-BERT的参数基数为100M，仅比其字符级对应的参数（90M）多11%，并且小于RoBERTa-base（Liu等人，2019）（102M）和AMBERT（Zhang和Li，2020）（176M）。网格位置关注中的位置关系和距离的建模只引入了12K的参数。 这个是怎么量化的？在BERT模型的预训练阶段，我们使用了一系列的中文文本，包括中文维基百科、知乎和网络新闻。我们的无标签数据中的字符总数为18.3G。我们遵循Liu等人（2019）的做法，用8K实例的大批次规模训练PLMs，共100K步。超参数和细节在附录C中给出。我们详细介绍了Lattice-BERT在11个中文NLU任务上的微调结果。回答以下问题：(1) Lattice-BERT是否比单粒度PLM和其他多粒度PLM表现得更好？(2) 所提出的格子位置注意和遮蔽段预测对下游任务的贡献如何？(3) Lattice-BERT是如何优于原来的字符级PLMs的？我们用这些不同的下游任务对我们提出的Lattice-BERT模型进行了彻底的探测。每个任务的统计数据和超参数在附录B中详细说明。对于MSR和MSRA-NER，我们用最佳的学习率设置运行了五次，并报告了平均分数，以确保结果的可靠性。消融实验我们进行了消融实验，以调查我们提出的格子位置注意（LPA）和掩蔽段预测（MSP）在下游任务中的有效性。 序列长度：为了减少计算成本，我们将预训练设置在序列长度为128个字符的Lite-size上。 任务选择：我们从每个任务集群中选择一个任务。 评价指标：我们使用实体级的F1-score来突出对边界预测的影响，我们报告了5次运行的平均分数。 数据集选择：并使用CLUE任务的开发集。1. 消融实验检测MSP任务的效果：我们在表2中可以看到，任何一个模块（-Dis.-Rel. &amp; -MSP）的消减都会导致平均分数的大幅下降。 特别是，用vanilla MLM代替MSP，MSP的平均得分下降了1.6%。 在WSC.任务中，需要长距离的依赖关系来解决核心词，差距高达3.1%。 我们将这一下降追溯到预训练过程中，并观察到开发集（dev-data）上-MSP设置的MLM准确性为88.3%。 然而，如果我们掩盖段内的标记并避免潜在的泄漏，准确率急剧下降到48.8%，远远低于使用MSP的LBERT训练的性能（56.6%）。这一差距提供了证据，表明MSP任务阻止了PLM通过偷看一个语段中的重叠文本单元来欺骗目标，从而鼓励PLM对长距离的依赖性进行定性。2. 消融实验检测LPA格子注意力的效果： 对于LPA方法，如果没有位置关系（-Rel.），实体级的F1得分在NER上下降了0.4%，在CMRC上的表现下降了0.7%。 性能的下降与没有距离信息（-Dis.）的情况类似。 如果没有其中任何一个（-Dis.-Rel.），差距分别扩大到0.5%和2.8%。 NER和CMRC中的边界预测对局部语言结构如嵌套词或重叠的歧义更为敏感。 由于注意到了位置关系和距离特征，LBERT可以准确地模拟不同分割结果中的嵌套和重叠标记之间的互动。 同时，如果没有距离信息，WSC的准确性明显下降。 当代词和候选短语之间的字符数大于30，或在20到30之间时，性能分别下降了7.5%和5.8%。对于其他情况，下降幅度仅为0.4%。通过对距离的明确建模，LBERT更准确地预测了长距离的核心推理关系。 平均来说，如果没有LPA中的位置关系和距离建模，在三个任务上的性能下降了2.0%，这表明LPA在协助PLM利用词格中的多格结构方面的重要性。3. LBERT如何改进细粒度的plm？我们比较了LBERT和字符级BERT的预测结果-我们在开发集上的基础大小，以研究LBERT如何优于普通的细粒度plm。直观地说，格中的字级标记提供了粗粒度的语义，其参数是特征级输入。一些发现 LBERT在较短的实例中带来了更多的改进 我们观察到，在TNEWS这一短文分类任务中，LBERT在较短的实例中带来了更多的改进，因为这些实例中的语句可能太短，无法为预测提供足够的背景。根据句子长度将开发集分为五个大小相等的仓，LBERT在最短和第二短的仓中分别比BERT-our好2.3%和1.3%，比其他实例的平均增益（0.6%）大。我们认为，词格中的冗余标记为这些短语句的语义提供了丰富的背景。例如，对于我们村的电影院/我们村的电影院这个短标题，由于在网格中引入了电影/电影、影院/电影院和电影院/电影院这些冗余词，LBERT将该实例归类为娱乐新闻而不是新闻故事。 LBERT擅于“预测候选词是否是某个段落的关键词”，原理是：通过利用格子中的冗余表达，从不同方面理解关键词 另一个案例是CSL任务，其目标是预测候选词是否是某个段落的关键词。对于那些LBERT平均从每个候选词中识别出两个以上的词级标记的情况，占到了数据集的47%，其性能增益为3.0%，明显大于其余部分的平均改进，即1.0%。我们认为LBERT通过利用格子中的冗余表达，从不同方面理解关键词。例如，从候选关键词太阳能电 池/太阳能电池中，太阳/太阳能，电池/电池，以及太阳能电池/太阳能电池都是格子标记。有了这些词级标记，LBERT可以将这个候选人与该段中的表达方式相匹配，如阳极/正极，光/光，电子/电子，离子/离子等。 LBERT减少了识别具有嵌套结构的实体的错误。 另一方面，对于MSRA-NER，LBERT减少了识别具有嵌套结构的实体的错误。平均而言，在LBERT中，预测的实体与黄金实体嵌套的错误案例数量减少了25%。例如，组织实体解放巴勒斯坦运动/巴勒斯坦民族解放运动与地点实体巴勒斯坦/巴勒斯坦嵌套在一起，并以一个指向组织的指标运 动/运动结束。字符级基线模型错误地将巴勒斯坦/巴勒斯坦和运/动分别识别为一个地点和一个组织。而LBERT在整合了解放/解放、巴勒斯坦/巴勒斯坦和运动/运动这些词之后，正确地识别了这个实体。通过预先训练的多粒度表征，LBERT同时融合了单词和字符的上下文信息，并成功地检测出了正确的实体。 一些思考 案例研究案例研究。表3显示了CMRC中的一个例子，这是一个跨度选择MRC任务，其中模型从给定的文档中选择一个文本跨度来回答这个问题。在这种情况下，这个问题要求的是一款受其主题曲限制的游戏。BERTour错误地输出了主题曲《中国之歌》，因为在文档中没有明确的与游戏相关的表达。然而，LBERT找到了正确的答案，即剑仙传说v。一个可能的原因是，剑仙传说是格点结构词汇中的一个条目。LBERT可能是在训练前的语境中学习了一个著名电子游戏的实体，明确地利用它作为一个整体的表现。在训练前使用粗粒度文本单元，LBERT直接编码有关这些单元的知识，以有利于下游任务。 LBERT如何利用多粒度表示？LBERT同时消耗输入序列中的所有单词和字符，但是该模型如何在训练前和下游任务中利用这种多粒度表示呢？为了研究这一点，我们使用每个格点标记在所有层和所有头部中接收到的平均注意分数来表示其重要性。如图4所示的例子，在微调之前，LBERT关注的是包括活/活，充实/充实，研究/研究，研究生/研究生，研究/调查等的标记。在对具体任务进行微调之前，该模型捕获了句子的各个方面。在用MSRA-NER进行微调后，最集中的词成为充实/充实，很/很，生活/生活，和研究/研究，即黄金分割结果中的标记，”研究|生 活|很|充实”，这对NER任务来说是很直观的。错误的分割词 “研究生 “的注意力得分明显下降。另一方面，在对新闻标题分类任务进行微调后，TNEWS、LBERT倾向于关注充实/充实、研究生/研究生、生活/生活等内容。虽然这些标记不能在一个中文分词结果中共存，但LBERT仍然可以利用来自各种可信分割的冗余信息来识别输入的主题。这些结果表明，Lattice-bert可以通过根据特定的下游任务将注意力转移到多粒度表示之间的不同方面来很好地管理晶格输入。 如何证明纳入格子而不是额外计算所带来的收益？为了公平比较，我们确保LBERT和字符级基线（即BERT-our）在训练步骤相同的情况下，按照以前的工作（Diao等人，2020；Zhang和Li，2020）有相同的训练周期。因此，与BERT-our相比，在LBERT的预训练实例中多引入了35%的文本单元，与BERT-our相比，多引入了48%的计算资源来处理额外的词级标记（见附录C）。为了说明纳入格子而不是额外计算所带来的收益，我们研究了在预训练中输入序列较长的序列大小的BERT-our，它的计算成本与LBERT相同。我们发现在CLUE分类任务上，LBERT仍然比BERT-our平均多出2.2%。更多的细节将在附录D中阐述。" }, { "title": "自然语言理解", "url": "/posts/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%90%86%E8%A7%A3/", "categories": "", "tags": "", "date": "2022-09-27 00:00:00 +0800", "snippet": "" }, { "title": "Python常用操作", "url": "/posts/Python%E6%8A%80%E5%B7%A7/", "categories": "NLP, 代码技巧和踩坑", "tags": "nlp, 常用操作", "date": "2022-09-23 00:00:00 +0800", "snippet": "Python常用操作处理文本的操作 去除列表中的“换行符” 代码：list1 = ['\\n \\n', '\\n', '\\n 浔阳江头夜送客，枫叶荻花秋瑟瑟。','\\n \\n 。主人下马客在船，举酒欲饮无管弦。\\n\\n', '醉不成欢惨将别，别时茫茫江浸月\\n', '\\n\\n']lists = [x.strip() for x in list1]结果：['', '', '浔阳江头夜送客，枫叶荻花秋瑟瑟。', '主人下马客在船，举酒欲饮无管弦。', '醉不成欢惨将别，别时茫茫江浸月', ''] 处理通用数据的操作 list去重set()之后恢复原本的顺序原lists：['', '', '浔阳江头夜送客，枫叶荻花秋瑟瑟。', '主人下马客在船，举酒欲饮无管弦。', '醉不成欢惨将别，别时茫茫江浸月', '']代码：set = list(set(lists)) # 去重set.sort(key=lists.index) # 恢复原本顺序结果：['', '浔阳江头夜送客，枫叶荻花秋瑟瑟。', '主人下马客在船，举酒欲饮无管弦。', '醉不成欢惨将别，别时茫茫江浸月'] 删除list中的元素、 list.remove(要删除的字符串) 过滤series中value大于/小于/等于阈值的行filter(lambda x:x&gt;3,list) 判断series中value大于/小于/等于阈值的行map(lambda x:x&gt;3,list)" }, { "title": "调参技巧合集", "url": "/posts/%E8%B0%83%E5%8F%82tricks/", "categories": "NLP, 代码技巧和踩坑", "tags": "nlp, 调参技巧", "date": "2022-09-22 00:00:00 +0800", "snippet": "调参Tricks记录1. 样本不平衡 问题：label标注的0-3四类，0类的比重过大，1类其次，2，3类都很少，怎么使用loss的weight来减轻样本不平衡问题？weight参数该如何设置？ 思路：大体的思想应该是对　样本较多的类别，使用较小的权重惩罚；对于样本少的类别，使用较大的权重惩罚；如何设置weight阈值？ Trick： 原文链接：https://blog.csdn.net/chumingqian/article/details/126625183 2. 慢热学习的调参 解释：warmup_proportion=0.1，总步数=100，那么warmup步数就为10。在1到10步中，学习率会比10步之后低，10步之后学习率恢复正常。在1到10步之间，学习率的改变一般有以下几种方式： “warmup_cosine”: WarmupCosineSchedule, “warmup_constant”: WarmupConstantSchedule, “warmup_linear”: WarmupLinearSchedule " }, { "title": "Paddle写代码踩坑记录", "url": "/posts/paddle%E5%90%88%E9%9B%86/", "categories": "NLP, 代码技巧和踩坑", "tags": "nlp, paddle, 代码技巧和踩坑", "date": "2022-09-22 00:00:00 +0800", "snippet": "Paddle写代码踩坑记录1. 由于样本不平衡，指定交叉熵损失函数每个类别的权重# 由于样本不平衡，指定交叉熵损失函数每个类别的权重M2# M1.[1/count(0),1/count(1)]# M2.[max(count)/count(0),max(count)/count(1)]weight_data = np.array([2.31,1.0]).astype(\"float32\")weight = paddle.to_tensor(weight_data)criterion = paddle.nn.loss.CrossEntropyLoss(weight=weight) # 交叉熵损失函数官方文档：https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/nn/CrossEntropyLoss_cn.html#cn-api-nn-loss-crossentropyloss参考资料：https://blog.csdn.net/qq_48345413/article/details/117925597备注：需要注意的是weight_data的类型应该是要和logits的类型一致,label的类型必须是int64。2. DataFrame增加一列（逐行增加一个属性,全部都是同一个值）from pandas import DataFramemerge_dt_dict = {'date':date_list, 'update':update_list, 'serverip':serverip_list}data_df = DataFrame(merge_dt_dict)# add one column add_column and set values=1 data_df['add_column'] = 1 （逐行增加一个属性,是一个列表）from pandas import DataFramemerge_dt_dict = {'date':date_list, 'update':update_list, 'serverip':serverip_list}data_df = DataFrame(merge_dt_dict)# add one column add_column and set values=1 data_df['add_column'] = add_list3. 合并2个DataFrame（结构完全一样的2个DataFrame）按行合并pd.concat([df1,df2],axis=0)按列合并pd.concat([df1,df2],axis=1)4. DataFrame打乱数据和随机采样df.sample(frac=1)其中参数frac是要返回的比例，比如df中有10行数据，我只想返回其中的30%,那么frac=0.3。如果需要打混后数据集的index（索引）还是按照正常的排序。我们只需要这样操作df.sample(frac=1).reset_index(drop=True)使用Sklearn库from sklearn.utils import shuffledf = shuffle(df)5. dataframe取每一行的最大值/最小值,类似argmax的意思idxmax(axis=1) | idxmin(axis=1)predict = pd.read_csv(\"test_results.tsv\",sep=\"\\t\",header=0,index_col=None).idxmax(axis=1)6. 创建一个新的空的DataFramecompire = pd.DataFrame(None,index=None,columns=[\"predict\",\"answer\"])7. 比较两列是否一致:boom:要用lambda，有人测试了这个方法最快t = compire.apply(lambda x:x[\"predict\"]==x[\"answer\"],axis=1)compire[\"是否一致\"] = t8. 按照某一列取满足条件的行比如：按照“是否一致”这列，取其中值为True的行right = compire[compire.是否一致==True]9. 针对某一列数据/一个series,逐个元素进行相同的处理比如：删去这个series的每一行数据的前后括号def qukuohao(str): return str[1:-1]predict = predict.map(qukuohao)10. 统计label列的类别分布情况M1:data['positionName'].value_counts().sort_values(ascending=False)M2:data['positionName'].value_counts()sort_values()函数的具体参数用法：DataFrame.sort_values(by=‘##’,axis=0,ascending=True, inplace=False, na_position=‘last’)参数说明by：表示根据什么字段或者索引进行排序，可以是一个或多个axis：排序是在横轴还是纵轴，默认是纵轴axis=0ascending：排序结果是升序还是降序，默认是升序inplace：表示排序的结果是直接在原数据上的就地修改还是生成新的DatFramekind：表示使用排序的算法，快排quicksort,，归并mergesort， 堆排序heapsort，稳定排序stable ，默认是 ：快排quicksortna_position：缺失值的位置处理，默认是最后，另一个选择是首位ignore_index：新生成的数据帧的索引是否重排，默认False（采用原数据的索引）key：排序之前使用的函数上面的例子很好地显示了参数key的使用，解释下上面两行代码的运行结果，我们对col3字段排序：默认情况下，字母是按照它们对应的ASCII码进行比较的(A-65,a-97)；所以升序的结果就是：BDFace加上了key参数，我们写了一个匿名函数lambda，作用是将col3中的字符串全部变成小写字母，这样升序自然是aBcDeF，因为此时的BDF变成了bdfhttps://blog.csdn.net/MsSpark/article/details/83154128https://blog.csdn.net/qq_25443541/article/details/118697711" }, { "title": "记录：联通2022网络AI技能大赛", "url": "/posts/%E8%AE%B0%E5%BD%95-%E8%81%94%E9%80%9A2022%E7%BD%91%E7%BB%9CAI%E6%8A%80%E8%83%BD%E5%A4%A7%E8%B5%9B/", "categories": "NLP, 比赛记录", "tags": "nlp, 文本分类, 情感分类", "date": "2022-09-20 14:10:56 +0800", "snippet": "中国联通2022网络AI技能大赛复赛赛题（NLP赛道） 简介 任务：用户评论情感分类赛题意义：从①各大网站的用户评论区中；②用户和客服的聊天记录中，挖掘用户的情绪。数据集：本赛题需要参赛者解决的是文本情感分类问题。由于运营商的工单含有运营商用户敏感信息，基于公司用户隐私相关规范，本次比赛暂不提供真实用户数据，所以我们选取了外卖、酒店、旅游、美食评论、电影评论等多个生活中常用的领域第三方APP下的评论数据作为本次赛题的数据集。输入输出形式： input:一段文字 output：1|0（其中“积极”用1表示，“消极”用0表示）内容格式： 每一句是一个单独的一行。 review是用户评论数据，rating字段是输出。文件格式： csv数据分类： 电影评论 movie_train.csvmovie_train.csv的字段名称为：userId,movieId,rating,timestamp,comment,like，分别代表用户ID，电影ID，用户评分（只保留1和5分的记录，其中5分是积极，1分是消极），评分时间戳，评论内容，是否喜欢等。数据量：663226条 用户购物消费评论 shopping_train.csvshopping_train.csv的字段名称为：cat（物品类型）、label（评分0-1）、review（评论文字）数据量：50219条 用户外卖评论 waimai_train.csvwaimai_train.csv的字段名称为：label（评分0-1）、review（评论文字）数据量：9589条 注：由于训练集中的数据形式不一样，需要用户自行进行统一处理后送入模型训练。数据集中可能存在缺失、重复、脏数据、换行等少部分干扰数据，需要参赛者自己构建策略予以识别或者滤除。评分标准：按照准确率的F1值从高到低进行排名。提交结果格式：提交的答案形式也是csv形式，以标题栏为label，然后用1和0表示积极和消极即可。 例如在如下例子中： label 0 1 0 1 0 1 代表第1、3、5句识别结果是消极情绪，第2、4、6句识别结果是积极情绪。:trollface:使用到的优化方法：1. Word2Vec+TFIDF提取关键词，以便于数据增强时替换词语不替换掉关键词2. 数据增强：基于同义词词林替换词语3. 调整交叉熵分类的阈值4. 使用SKEP模型微调 数据预处理&amp;可视化分析 1. 数据可视化 训练集1 movie_train.csv 概况 import pandas as pdmovie_df = pd.read_csv(\"../train/movie_train.csv\",sep=\",\",header=0,index_col=None)print(len(movie_df))key = \"comment\"key_len = key+\"_len\"movie_df[key_len]=movie_df[key].apply(len)# 查看comment文本情况print(\"数据长度概览：\")print(movie_df[key_len].describe(percentiles=[0.1,0.25,0.75,0.8,0.9,0.95,0.999])) # percentiles指定排前%的取值，默认值是25 45 75print(\"中位数是：\")print(movie_df[key_len].median())print(\"数据各个长度分别有多少条：\")print(movie_df[key_len].value_counts()) # 查看rating的分布print(\"不同评分分别有多少条：\")print(movie_df[\"rating\"].value_counts()) movie_train.csv总行数：663226comment列平均长度：37comment列最短长度：1comment列最长长度：200comment列95%长度小于等于128comment长度=5的最多：20922条 rating列分布：=5 476130条 71.79%=1 187096条 28.21%测试集 test_set_data.csv 概况'''查看test_set_data.csv文件'''movie_df = pd.read_csv(\"../test/test_set_data.csv\",header=0,index_col=None)print(len(movie_df))key = \"review\"key_len = key+\"_len\"movie_df[key_len]=movie_df[key].apply(str).apply(len)# 查看review文本情况print(\"数据长度概览：\")print(movie_df[key_len].describe(percentiles=[0.1,0.25,0.75,0.8,0.9,0.95,0.999])) # percentiles指定排前%的取值，默认值是25 45 75print(\"中位数是：\")print(movie_df[key_len].median())print(\"数据各个长度分别有多少条：\")print(movie_df[key_len].value_counts()) test_set_data.csv总行数：22833comment列平均长度：45comment列最短长度：1comment列最长长度：733comment列95%长度小于等于143comment长度=15的最多：599条def scan_all(): ''' 查看raw_data.csv文件 ''' movie_df = pd.read_csv(\"../code/raw_data.csv\",sep=\",\",header=0,index_col=None) print(len(movie_df)) key = \"text\" key_len = key+\"_len\" movie_df[key_len]=movie_df[key].apply(len) # 查看comment文本情况 print(\"数据长度概览：\") print(movie_df[key_len].describe(percentiles=[0.1,0.25,0.75,0.8,0.9,0.95,0.999])) # percentiles指定排前%的取值，默认值是25 45 75 print(\"中位数是：\") print(movie_df[key_len].median()) print(\"数据各个长度分别有多少条：\") print(movie_df[key_len].value_counts()) # 查看rating的分布 print(\"不同评分分别有多少条：\") print(movie_df[\"label\"].value_counts()) 总行数：723034text列平均长度：38text列最短长度：1text列最长长度：2876text列95%长度小于等于130text长度=5的最多：21371条 label列分布：=disnegative 504703条 69.80%=negative 218331条 30.20%2. 数据预处理将训练数据集合并，把label变成int3. 数据清理训练和验证数据是队友直接合并的。测试数据有一行是Nan# 返回test_df中含有空值的行new_df = test_df[test_df.isnull().any(axis = 1)==True]# 返回test_df中不含有空值的行new_df = test_df[test_df.isnull().any(axis = 1)==False]# 返回test_df中含有空值的列new_df = test_df[test_df.isnull().any(axis = 0)==True]# 返回test_df中不含有空值的列new_df = test_df[test_df.isnull().any(axis = 0)==False]删除这一行，并生成新文件替换掉含有空值的旧文件def clean_test(): test_df = pd.read_csv(\"../test/test_set_data.csv\",header=0,index_col=None) print(len(test_df)) new_df = test_df[test_df.isnull().any(axis = 1)==False] new_df.to_csv(\"../test/test_set_data1.csv\",index=False) return new_dfclean_test() Method1：Bert-Base-Chinese作为预训练模型 队友做的93% F1 Method2：SKEP作为预训练模型 用Accuracy作为train过程中的evaluate函数的metric，测试的时候再用F1score 2.1 batch_size=16 Acc=63.85% 其他超参数： max_seq_length = 128 batch_size = 16 learning_rate = 4e-5 epochs = 3 warmup_proportion = 0.1 weight_decay = 0.01开始训练：2022-9-20 21:03结束训练：2022-9-21 15:18 手动停止进度： global step 12000, epoch: 1, batch: 12000, loss: 0.78122, accu: 0.66812, speed: 1.55 step/s 一个epoch没训练完，实在太慢了测试一下test_set_data.csv用当前保存的acc最高的一套模型参数预测,得到预测结果test_set_predict.csv，和test_set_answer.csv对比，计算Acc和F1-score。import paddlepaddle.device.set_device('gpu:1')print(paddle.device.get_device())import pandas as pdfrom paddlenlp.datasets import load_datasetfrom paddle.io import Dataset, Subsetfrom paddlenlp.datasets import MapDataset '''0. loading data...'''test = pd.read_csv('../test/test_set_data.csv')def read_test(pd_data): for index, item in pd_data.iterrows(): yield {'text': item['review'], 'label': 0} test_ds = load_dataset(read_test, pd_data=test,lazy=False)# 在转换为MapDataset类型def convert_example(example, tokenizer, max_seq_length=512, is_test=False): # 将原数据处理成model可读入的格式，enocded_inputs是一个dict，包含input_ids、token_type_ids等字段 encoded_inputs = tokenizer( text=example[\"text\"], max_seq_len=max_seq_length) # input_ids：对文本切分token后，在词汇表中对应的token id input_ids = encoded_inputs[\"input_ids\"] # token_type_ids：当前token属于句子1还是句子2，即上述图中表达的segment ids token_type_ids = encoded_inputs[\"token_type_ids\"] if not is_test: # label：情感极性类别 label = np.array([example[\"label\"]], dtype=\"int64\") return input_ids, token_type_ids, label else: return input_ids, token_type_ids, -1 def create_dataloader(dataset, trans_fn=None, mode='train', batch_size=1, batchify_fn=None): if trans_fn: dataset = dataset.map(trans_fn) shuffle = True if mode == 'train' else False if mode == \"train\": sampler = paddle.io.DistributedBatchSampler( dataset=dataset, batch_size=batch_size, shuffle=shuffle) else: sampler = paddle.io.BatchSampler( dataset=dataset, batch_size=batch_size, shuffle=shuffle) dataloader = paddle.io.DataLoader( dataset, batch_sampler=sampler, collate_fn=batchify_fn) return dataloader'''1. loading model...'''from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizermodel = SkepForSequenceClassification.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_2.0_large_en\", num_classes=2)tokenizer = SkepTokenizer.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_2.0_large_en\")'''2. define superparameters...'''from functools import partialimport numpy as npimport paddle.nn.functional as Ffrom paddlenlp.data import Stack, Tuple, Padbatch_size=16max_seq_length=128# 处理测试集数据trans_func = partial( convert_example, tokenizer=tokenizer, max_seq_length=max_seq_length, is_test=True)batchify_fn = lambda samples, fn=Tuple( Pad(axis=0, pad_val=tokenizer.pad_token_id), # input Pad(axis=0, pad_val=tokenizer.pad_token_type_id), # segment Stack() # qid): [data for data in fn(samples)]test_data_loader = create_dataloader( test_ds, mode='test', batch_size=batch_size, batchify_fn=batchify_fn, trans_fn=trans_func)'''3. evalue the model...'''import os # 根据实际运行情况，更换加载的参数路径params_path = 'best_checkpoint/best_model.pdparams'if params_path and os.path.isfile(params_path): # 加载模型参数 state_dict = paddle.load(params_path) model.set_dict(state_dict) print(\"Loaded parameters from %s\" % params_path) results = []# 切换model模型为评估模式，关闭dropout等随机因素model.eval()for batch in test_data_loader: input_ids, token_type_ids, qids = batch # 喂数据给模型 logits = model(input_ids, token_type_ids) # 预测分类 probs = F.softmax(logits, axis=-1) idx = paddle.argmax(probs, axis=1).numpy() idx = idx.tolist() qids = qids.numpy().tolist() results.extend(zip(qids, idx))'''4. save predict result...''' # 写入预测结果，提交with open( \"submission.csv\", 'w', encoding=\"utf-8\") as f: # f.write(\"数据ID,评分\\n\") f.write(\"label\\n\") for (idx, label) in results: f.write(str(label)+\"\\n\")Conclusion分析：结果不好，准确率只有63.85%Question问题1：loss曲线出现了震荡情况，且不收敛解决方案：根据知乎的解释a. batchsize太小，过拟合了当前batch，以至于batch上预估的梯度无法有效近似数据集分布上估计的梯度，从而无法泛化到下一个batch上。解决方案: 调大batchsize, 使用带动量的梯度。batchsize=16，的确有些小，但是已经是带动量的梯度了b. 学习率太大。解决方案：减小学习率。学习率已经是4e-5c. 收敛区域不太好但又跳不出来，想想进入了凹凸不平的盆地。解决方案：周期学习率。学习率已经是周期学习率2.2 batch_size=64 Acc=71.46%(存档1)其他超参数： max_seq_length = 128 batch_size = 64 learning_rate = 4e-5 epochs = 3 warmup_proportion = 0.1 weight_decay = 0.01开始训练：2022-09-21 15:56:282.2.1 存档1当前时间：2022-09-22 09:46:30进度： 模型保存在 16500 步， 最佳eval准确度为0.81037571！global step 16190, epoch: 2, batch: 7152, loss: 0.32919, accu: 0.82326, speed: 1.34 step/s速度快多了。并且acc收敛了，&lt;font color=&gt;为啥loss还在震荡？！！！&lt;/font&gt;。查阅资料[^trainloss震荡]得到以下分析： 1.train_loss 不断下降，val_loss(test_lost) 不断下降 说明网络训练正常，最好情况 2.train_loss 不断下降，val_loss(test_lost) 趋于不变 说明网络过拟合，可以添加dropout和最大池化max pooling 3.train_loss 趋于不变，val_loss(test_lost) 不断下降 说明数据集有问题，建议重新选择 4.train_loss 趋于不变，val_loss(test_lost) 趋于不变 说明学习遇到瓶颈，需要减小学习率或批量batch数目 5.train_loss 不断上升，val_loss(test_lost) 不断上升 说明网络结构设计不当，训练超参数设置不当，数据集经过清洗等问题，最差情况审阅这轮参数的模型给出的预测结果：import pandas as pdimport numpy as npanswer = pd.read_csv(\"../test/test_set_answer.csv\",header=0,index_col=None)predict = pd.read_csv(\"../test/test_set_predict.csv\",header=0,index_col=None)def analysis(predict,answer): if len(predict)!=len(answer): return distribution_dict = {'11':[],'01':[],'10':[],'00':[]} for idx,(pred,ans) in enumerate(zip(predict,answer)): if idx==0: continue #跳过首行 distribution_dict[str(ans)+str(pred)].append(idx) # 将distribution_dict的所有list填充到一致的长度，用0填充 pad_dict = {'11':[],'01':[],'10':[],'00':[]} for key,value_list in distribution_dict.items(): arr = np.array(value_list) arr_pad = np.pad(array=arr, pad_width = (0,max([len(l) for k,l in distribution_dict.items()])-len(value_list)),# 开头填充0个元素，结尾填充（四个list最长长度-当前list的长度）个元素 mode='constant') pad_dict[key] = arr_pad.tolist() ana_df = pd.DataFrame.from_dict(pad_dict) ana_df.to_csv(\"./bs64/analy_distribution.csv\",sep=\"\\t\",index=False) return ana_dfdef distribution_rate(ana_df): count11 = len(ana_df['11']) count10 = len(ana_df[~ana_df['10'].isin([0])]) count01 = len(ana_df[~ana_df['01'].isin([0])]) count00 = len(ana_df[~ana_df['00'].isin([0])]) print(\"答案是1，预测是1：\",count11) # 11列没有0 print(\"答案是1，预测是0：\",count10) print(\"答案是0，预测是1：\",count01) print(\"答案是0，预测是0：\",count00,\"\\n\") print(\"预测是1中，预测对的有：\",count11/(count11+count01)) print(\"预测是0中，预测对的有：\",count00/(count10+count00),\"\\n\") print(\"答案是1中，预测对的有：\",count11/(count11+count10)) print(\"答案是0中，预测对的有：\",count00/(count01+count00),\"\\n\") print(\"所有预测错的中，（答案=）1的有：\",count10/(count10+count01),\"所有预测错的中，（答案=）0的有：\",count01/(count10+count01))result = analysis(predict['label'],answer['label']) distribution_rate(result)很明显，样本不均衡导致预测结果中，积极情感预测效果好，而消极情感预测效果差。2.2.2 存档2当前时间：2022-09-22 11:12:30进度： 上一个存档：模型保存在 16500 步， 最佳eval准确度为0.81037571！global step 16190, epoch: 2, batch: 7152, loss: 0.32919, accu: 0.82326, speed: 1.34 step/s当前存档：模型保存在 17800 步， 最佳eval准确度为0.81231891！global step 17860, epoch: 2, batch: 8822, loss: 0.37988, accu: 0.81094, speed: 1.33 step/sQuestion问题1：loss曲线出现了震荡情况解决方案：根据csdn的解释，应当是数据集的问题，样本不均衡。根据知乎的解释，极不平衡要做的，除非小样本数据里有特别特别明显的特征，要不然会训练不出来。（1）对数据少的类别，要做数据增强。可以去查查，目前有很多数据增强的方法，例如同义词替换、生成、改写。（2）loss的设大，可以增加少样本类别的loss权重，让模型重视这一类别，而不是一视同仁2.3 batch_size=64 loss_weight=[2.31,1.0]其他超参数(与1.2保持一致)： max_seq_length = 128 batch_size = 64 learning_rate = 4e-5 epochs = 3 warmup_proportion = 0.1 weight_decay = 0.01开始训练：2022-09-22 14:12:282.3.1 存档1当前时间：2022-09-22 19:01:30在paddle_finetune.py文件中，165行设置如下：# 由于样本不平衡，指定交叉熵损失函数每个类别的权重M2# M1.[1/count(0),1/count(1)]# M2.[max(count)/count(0),max(count)/count(1)]weight_data = np.array([2.31,1.0]).astype(\"float32\")weight = paddle.to_tensor(weight_data) 当前存档：模型保存在 900 步， 最佳eval准确度为0.72405209！global step 3460, epoch: 1, batch: 3460, loss: 0.69813, accu: 0.49036, speed: 0.60 step/s是不是weight设反了？由于样本不平衡，指定交叉熵损失函数每个类别的权重M2M1.[1/count(0),1/count(1)]M2.[max(count)/count(0),max(count)/count(1)]weight_data = np.array([1.0,2.31]).astype(\"float32\")weight = paddle.to_tensor(weight_data)criterion = paddle.nn.loss.CrossEntropyLoss(weight=weight) # 交叉熵损失函数 模型保存在 3500 步， 最佳eval准确度为0.78014204！global step 11240, epoch: 2, batch: 2202, loss: 0.37571, accu: 0.70195, speed: 1.11 step/s为什么还是不行？？ 5.train_loss 不断上升，val_loss(test_lost) 不断上升说明网络结构设计不当，训练超参数设置不当，数据集经过清洗等问题，最差情况难道是超参数？2.3 数据增强-增加负样本数据微调'''生成负样本1：1'''import osimport jiebaimport numpy as npfrom gensim.models import KeyedVectors, TfidfModelfrom gensim.corpora import Dictionaryfrom utils.utils import read_samples, write_samples, isChinesefrom gensim import matutilsfrom itertools import isliceclass EmbedReplace(): def __init__(self, sample_path, wv_path): self.samples = read_samples(sample_path) self.samples = [list(jieba.cut(sample)) for sample in self.samples] self.wv = KeyedVectors.load_word2vec_format(wv_path, binary=False) if os.path.exists('tfidf_word2vec/tfidf.model'): self.tfidf_model = TfidfModel.load('tfidf_word2vec/tfidf.model') self.dct = Dictionary.load('tfidf_word2vec/tfidf.dict') self.corpus = [self.dct.doc2bow(doc) for doc in self.samples] else: self.dct = Dictionary(self.samples) self.corpus = [self.dct.doc2bow(doc) for doc in self.samples] self.tfidf_model = TfidfModel(self.corpus) self.dct.save('tfidf_word2vec/tfidf.dict') self.tfidf_model.save('tfidf_word2vec/tfidf.model') self.vocab_size = len(self.dct.token2id) def vectorize(self, docs, vocab_size): return matutils.corpus2dense(docs, vocab_size) def extract_keywords(self, dct, tfidf, threshold=0.2, topk=5): \"\"\" 提取关键词 :param dct (Dictionary): gensim.corpora.Dictionary :param tfidf (list): :param threshold: tfidf的临界值 :param topk: 前 topk 个关键词 :return: 返回的关键词列表 \"\"\" tfidf = sorted(tfidf, key=lambda x: x[1], reverse=True) return list(islice([dct[w] for w, score in tfidf if score &gt; threshold], topk)) def replace(self, sample, doc): \"\"\"用wordvector的近义词来替换，并避开关键词 :param sample (list): reference token list :param doc (list): A reference represented by a word bag model :return: 新的文本 \"\"\" keywords = self.extract_keywords(self.dct, self.tfidf_model[doc]) # num = int(len(sample) * 0.3) new_tokens = sample.copy() indexes = np.random.choice(len(sample), num) for index in indexes: token = sample[index] if isChinese(token) and token not in keywords and token in self.wv: new_tokens[index] = self.wv.most_similar(positive=token, negative=None, topn=1)[0][0] return ''.join(new_tokens) def generate_samples(self, write_path): \"\"\"得到用word2vector词表增强后的数据 :param write_path: \"\"\" replaced = [] for sample, doc in zip(self.samples, self.corpus): replaced.append(self.replace(sample, doc)) write_samples(replaced, write_path, 'a')if __name__ == '__main__': sample_path = 'data/train.txt' wv_path = 'tfidf_word2vec/sgns.weibo.bigram-char' replacer = EmbedReplace(sample_path, wv_path) replacer.generate_samples('data/replaced.txt')'''合并数据增强的结果到原训练数据中'''import pandas as pdtext = []with open(\"./NLP-Data-Augmentation-main/data/replaced.txt\",'r',encoding=\"utf-8\") as f: text = f.readlines()text= [x.strip() for x in text] # 删除换行符d = {\"text\":text}nega_df = pd.DataFrame(d)nega_df[\"label\"] = 0old_df = pd.read_csv(\"raw_data.csv\",sep=\"\\t\",header=0,index_col=None)merge_df = pd.concat([old_df,nega_df],axis=0).sample(frac=1)merge_df.to_csv(\"enhanced_data.csv\",sep=\"\\t\",header=True,index=False)增强后的正负样本比例接近1：1，不再需要调整交叉熵类别权重，weight删除。criterion = paddle.nn.loss.CrossEntropyLoss() # 交叉熵损失函数因为上面的实验超参数貌似不合适，全部根据官方文档1修改：'''0. define super-parameters'''max_seq_length = 128batch_size = 64learning_rate = 5e-5epochs = 3warmup_proportion = 0.0weight_decay = 0.00 Method3：SKEP-large作为预训练模型 3.1 调用API from paddlenlp import Taskflow import pandas as pdall_df = pd.read_csv(\"../test/test_set_data.csv\",sep=\"\\t\",index_col=None)senta = Taskflow(\"sentiment_analysis\", model=\"skep_ernie_1.0_large_ch\", device_id=1) d = {\"negative\":0,\"positive\":1}for text in all_df[\"review\"]: # print(senta(text)[0]) with open(\"try/test_set_predict.csv\",\"a\",encoding=\"utf-8\") as f: f.write(str(d[senta(text)[0]['label']])+'\\n')3.2 数据增强-增加负样本数据微调其他超参数： max_seq_length = 128 batch_size = 64 learning_rate = 5e-5 epochs = 3 warmup_proportion = 0.0 weight_decay = 0.00 开始训练：2022-9-23 17:47 结束训练： 进度：不知道为什么，数据量一多nohup就不训练。只能vscode中训练，但是一直挂机不显示。大概是1w的数据量可以训练。9w不行。3.3 根据概率值增加If-Else，逼近答案:link:Reference https://github.com/PaddlePaddle/PaddleNLP/tree/develop/examples/sentiment_analysis/skep &#8617; " }, { "title": "深入研究BERT位置编码的原理", "url": "/posts/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/", "categories": "NLP, 模型研究", "tags": "nlp, 语言学, BERT", "date": "2022-09-14 10:10:56 +0800", "snippet": "深入研究BERT位置编码的原理记录日期：2022/08/29📃参考文章 [1] 苏建林博客 [2] Transformer学习笔记一：Positional Encoding（位置编码） [3] 知乎.实践中BERT如何对长度大于500的文本进行处理？ [4] 位置编码全面研究-好文！💎 [5] how to generate position embedding heatmap [6] The Illustrated Transformer [7] BERT的三个Embedding详解0：什么是位置编码？为什么破解BERT处理长文本的问题要研究位置编码？研究位置编码的动机在transformer的encoder和decoder的输入层中，使用了Positional Encoding，使得最终的输入满足：input=input_embedding+positional_encoding这里，input_embedding是通过常规embedding层，将每一个token的向量维度从vocab_size映射到d_model，由于是相加关系，自然而然地，这里的positional_encoding也是一个d_model维度的向量。（在原论文里，d_model = 512）📌通过常规embedding层，将每一个token的向量维度从vocab_size映射到d_model这句话表述的流程如下：那么，我们为什么需要position encoding呢？在transformer的self-attention模块中，序列的输入输出如下（不了解self-attention没关系，这里只要关注它的输入输出就行）：在self-attention模型中，输入是一整排的tokens，对于人来说，我们很容易知道tokens的位置信息，比如：（1）绝对位置信息。a1是第一个token，a2是第二个token......（2）相对位置信息。a2在a1的后面一位，a4在a2的后面两位......（3）不同位置间的距离。a1和a3差两个位置，a1和a4差三个位置....但是这些对于self-attention来说，是无法分辩的信息，因为self-attention的运算是无向的。因为，我们要想办法，把tokens的位置信息，喂给模型。 *以上引自[2] Transformer学习笔记一：Positional Encoding（位置编码）*为什么？在做NLP的相关任务中，最常见的一个问题便是当输入序列过长时应该如何进行处理。例如在谷歌开源的预训练模型中，最大长度只支持512个Token。大家都知道，目前的主流的BERT模型最多能处理512个token的文本。导致这一瓶颈的根本原因是BERT使用了从随机初始化训练出来的绝对位置编码，一般的最大位置设为了512，因此顶多只能处理512个token，多出来的部分就没有位置编码可用了。当然，还有一个重要的原因是Attention的O(n2*d)复杂度，导致长序列时显存用量大大增加，一般显卡也finetune不了。[1]对于原因一应该怎么进行处理呢？是简单的进行截断处理吗？总的来说，对于这类问题可以有两种方式来进行解决：第1种是从模型入手，例如消除最大长度为512的限制；第2种是从输入入手，改变输入序列的长度重新构造任务。[3]对于原因二？稀疏注意力位置编码[2]：• 绝对位置（最自然的想法）一种自然而然的想法是，给第一个token标记1，给第二个token标记2…，以此类推。这种方法产生了以下几个主要问题：（1）模型可能遇见比训练时所用的序列更长的序列。不利于模型的泛化。（2）模型的位置表示是无界的。随着序列长度的增加，位置值会越来越大。• 用浮点数表示（3个字：[0,0.5,1]）为了解决整型值带来的问题，可以考虑将位置值的范围限制在[0, 1]之内，其中，0表示第一个token，1表示最后一个token。比如有3个token，那么位置信息就表示成[0, 0.5, 1]；若有四个token，位置信息就表示成[0, 0.33, 0.69, 1]。但这样产生的问题是：（1）当序列长度不同时，token间的相对距离是不一样的。 例如在序列长度为3时，token间的相对距离为0.5；在序列长度为4时，token间的相对距离就变为0.33。因此，我们需要这样一种位置表示方式，满足于：（1）它能用来表示一个token在序列中的绝对位置（2）在序列长度不同的情况下，不同序列中token的相对位置/距离也要保持一致（3）可以用来表示模型在训练过程中从来没有看到过的句子长度。• one-hot编码（[0,1]标记）考虑到位置信息作用在input embedding上，因此比起用单一的值，更好的方案是用一个和input embedding维度一样的向量来表示位置。input embedding和位置编码怎么concat见：下一节 | 先说Transformer这时我们就很容易想到二进制编码。如下图，假设d_model = 3，那么我们的位置向量可以表示成：这下所有的值都是有界的（位于0，1之间），且transformer中的d_model本来就足够大，基本可以把我们要的每一个位置都编码出来了。但是这种编码方式也存在问题：（1）这样编码出来的位置向量，处在一个离散的空间中，不同位置间的变化是不连续的。这样无法体现相对位置关系。假设d_model = 2input sequence=4sequence vector=[ [0,0,1,...,0]1 * vocab_size , [0,0,...,1,0]1 * vocab_size , [0,1,0,...,0]1 * vocab_size , [0,0,...,0,1]1 * vocab_size ]sequence vector经过embedding层变成token embedding vector=[ [0.xx,0.xx]1 * d_model=1 * 2 , [0.xx,0.xx]1 * d_model=1 * 2 , [0.xx,0.xx]1 * d_model=1 * 2 , [0.xx,0.xx]1 * d_model=1 * 2 , ]我们有4个位置需要编码，这四个位置向量可以表示成[0,0],[0,1],[1,0],[1,1]。然后我们就可以把token embedding vector和位置向量加起来。我们把它的位置向量空间做出来：🌀按理说，[0,0]和[1,0]的距离是2，[0,0]和[1,1]的距离是3，[0,1]和[1,0]的距离是1，但显然图上黑线不是这样表示相对距离的，我认为（索引）这种一维序列本身就不适合用高维向量表示。 &lt;u&gt;如果我们能把离散空间（黑色的线）转换到连续空间（蓝色的线）&lt;/u&gt;，那么我们就能解决位置距离不连续的问题。同时，我们不仅能用位置向量表示整型，我们还可以用位置向量来表示浮点型。 ##### • 连续空间的编码（一维表示“索引”） ![](/assets/img/2022-09-14-位置编码/2022-09-19-17-46-01.png)用连续函数编码，那么要保证①每个位置都有独一无二的编码，②编码空间不是无限域。出于②的原因，正余弦函数是一个很好的选择，值域不是无穷区间，在[-1,1]。如果我们是用绝对位置也就是0，1，2，3，…这些用正余弦函数sin,cos来编码，自然会出现不同位置有相同的编码结果值。例如下面图中的第0位和第6位就是相同的编码结果。###################################################正弦图像是具有周期性的，不巧的话就会出现不同的位置是一个值，真是那样的话，位置编码的意义就失去了，如何让不同词之间分开是位置编码的目的，接下来引入i，依照公式，只有i为偶数才为正弦图，引入i之后代表进一步细分，不止是按照一个一个词来分，而是按照词和词嵌入的特征来分，其实可以想想，人其实大多数情况的行为举止都是差不多的，细分才能看出不同，这里用了 p0 和 p6 第1，3，5个特征：按照词和词嵌入的特征来分：也就是说，如果我们用维度当做正选函数的周期，位置越远正余弦函数的震荡周期越大，那么即使是同一个位置在不同词的表征向量中也能得到不同的数值。 位置1 位置2 位置3tensor([[ 0.4924, 0.3564, 0.4850],词1的表征向量 [ 1.0244, -0.0162, -0.0017],词2的表征向量 [ 0.6738, 0.7967, 0.6629]])词3的表征向量 ![f30695e06a59a713fa45ceb33544cc4d.jpeg](en-resource://database/517:1)不难发现，不仅不同词之间分开了，同一个词不同的特征之间也能区分出来当两个参数都变得很大的时候，又会发生什么呢？transformer默认的词嵌入维度是512，这里举个特例，假如句子很长，以第101个词为例y = sin( \\frac {100} {10000^{(2i/512)}})显然，前期有大量特征会出现重合，且会出现较高频率变换，后期几乎趋于一致（奇数情况也如此），我们其实可以猜想出后面的其实几乎不带有位置信息，下图为奇偶混合的情况，可以看出有很多重合，当i较大时，两者都不带任何信息（第三节会有验证）下图为小demo的位置编码的可视化结果深度是一个词的特征维度，Position代表的是哪一个词 为什么会出现：当i较小时，特征之间重合较大，而且离得近的位置之间特征变化很快，当i大到一定程度，几乎没有任何区别我的猜想是当词与词之间离得很近时（pos差得小）或者对于一个词不同特征来说，携带信息相同其实是符合逻辑的。在word2vec里面，如果两个词意相同，转换为坐标空间内的向量就会离得近（对应于一个词不同特征），有时候，一个句子之间离得近的词可能有关联效果，如”I love reading books.”，reading和books它们一起出现的概率是较大的（对应于离得近的词）。 位置编码是加上去的，为什么不拼接上去呢？拼接的话会扩大参数空间，占用内存增加，而且不易拟合，而且其实没有证据表明拼接就比加来的好 那么，既然有了公式，那位置编码是算出来的还是学出来的呢？其实算出来和学出来的效果差不多，但是考虑到算出来的可以接受更长的序列长度而不必受训练的干扰，所以在这个模型中，位置编码是通过公式算出来的。 “不足”：先对位置编码进行可视化（见下图，这里深度变成了512，与Transformer默认参数保持一致）先看第一列，即每个词的第一个维度，可以发现相似度变化很大，有差不多相似的，但也有几乎无关的，相似的原因猜测是序列所需要的（之所以不用决定是因为提前公式决定的和学出来的差不多），每个句子中都有一个词与另一个词词义相近。因而一开始看好像在低维时位置编码似乎不错，可是当维度显著提升后，300维之后全部都一模一样的，这显然不符合位置编码设计的初衷（捕捉句子间的序列关系），不妨猜测可能是真正起作用的是较小维度的值或者是经过dropout之后将一部分完全相同的值给剔除掉来一定程度提升模型，可是经过我的语言翻译模型实验发现其实加不加结果差不多。 总结就是当嵌入维度显著上升后，模型捕捉词与词之间关系的能力大大下降。 既然400维往后都一样，全部不要怎么样？接下来我将400维及其后面的全部重置为0（见上），进行了seq2seq法译英任务（基于Transformer），结果是几乎无影响，左图为未做改变的训练和测试准确率图，下面翻译的结果也很丝滑。结果放到了colab，对结果存有疑虑可以去复现一下，请确保你对每一个部分都很了解，复制代码到你本地或者云端，按照注释来添加不同的对照组。`input: je pars en vacances pour quelques jours .target: i m taking a couple of days off .pred: i m leaving on vacation days .input: je recherche un assistant .target: i am looking for an assistant .pred: i am looking for an assistant .`因为一开始为了收敛迅速选取了长度小于10的中短句子，为了防止因为句子因素而造成的影响，下面特地选取了长度在 [11,17] 的句子，（注意这次实验并没有短句子）作为对照组（考虑到这个数据集大部分都是中短句子，长句子并不多，因而模型未充分拟合，正确率维持在 74% ，好在不影响实验），左图为原始的：![31968bb03715b5d009515a62aedbf250.jpeg](en-resource://database/527:1)考虑到即使位置不对，词对了准确率依然很高，所以还随机地让模型翻译句子，结果表明如果句子长度是中短型的，似乎模型可以学习足够的位置信息而不依赖于位置编码，可是当句子过长时，即使是位置编码的400维以后也发挥了作用。下面选取了两个例子，一个是较短的，另一个较长（上面的图为原始的）：有趣的是，位置编码的400维以后的信息依然发挥了作用，可以看见注意力图相对来说左侧更加整齐，同时因为两者整体说都不整齐，说明短句子的位置信息没有那么的明确，而400维置为0的也学到了较为重要的位置信息，因而可以翻译的与原始的相当：可是，句子较长的时候，模型不能够依赖自学而获取位置信息，而原始的却依然能够沿轴线对齐，保持位置信息： 如果直接不要位置编码，会怎么样？在短句子中两者无论从准确度还是翻译上来看都是相当的。这也说明了位置编码在短句子（长度小于10）和固定开头的句子中并非发挥应有的作用，句子简单时很可能机器能够自己学会位置信息，这也可以推出短句时加完位置编码后根本没必要Dropout，因为信息本来就不重要嘛`input: je pars en vacances pour quelques jours .target: i m taking a couple of days off .pred: i m taking a couple of days off .input: je recherche un assistant .target: i am looking for an assistant .pred: i am looking for an assistant .`可是当遇到长句子时，位置变多，没有位置编码几乎都是随机排列的，准确率几乎一致，说明词还是都能猜的差不多，猜词和位置编码没关系：虽然是乱序，但是模型在较短的时候还是能够自己独立学会位置信息可是真正遇到长句子时，没有位置编码就完全乱套了 **以上引自[4]** 01：BERT和Transformer的位置编码的区别Transformer中的位置编码（PE）在Transformer中，位置编码是由sin/cossin/cos函数生成的固定值。具体做法：用不同频率的正余弦函数对位置信息进行编码，位置编码向量的维度与文本编码向量的维度相同。因此二者（位置编码向量，文本编码向量）可以直接相加作为token最终的编码向量。02：Experiment实验一：sin&amp;cos交叉、纯sin、纯tanh说明：位置编码的函数三种设置方案，预训练之后position embedding的向量结果可视化对比🔮 基础模型：google_zh_model 预训练数据：corpora/book_review_bert.txt 预训练后得到的模型： models/book_review_model.bin512长度的长文本预训练后的位置编码图像sin&amp;cos交叉sintanh" }, { "title": "HowNet知网", "url": "/posts/HowNet/", "categories": "NLP, 基础知识", "tags": "nlp, 语言学", "date": "2022-09-14 10:10:56 +0800", "snippet": "HowNet知网机构：清华大学人工智能研究院定义：HowNet是董振东先生、董强先生父子毕三十年之功标注的大型语言知识库，主要面向中文（也包括英文）的词汇与概念。HowNet秉承还原论思想，认为词汇/词义可以用更小的语义单位来描述。这种语义单位被称为“义原”（Sememe），顾名思义就是原子语义，即最基本的、不宜再分割的最小语义单位。在不断标注的过程中，HowNet逐渐构建出了一套精细的义原体系（约2000个义原）。HowNet基于该义原体系累计标注了数十万词汇/词义的语义信息。[2]成果：2000 多个义原的精细的语义描述体系，并为十几万个汉语和英语词所代表的概念标注了义原。整体包含有 229,767 个中英文词条， 35,202 个概念以及 2,196 个义原。API：OpenHowNet功能：义原查询、基于义原的词相似度计算等功能基础概念： 义原： Motivation：有的语言学家认为，包括词在内的所有概念的语义都可使用一个有限的义原集合去表示。而义原是比较隐含的语义单位，所以人们需要利用已经构建好的义原知识库才能够获取一个词所对应的义原。 定义：义原是最基本的、不易于再分割的意义的最小单位。 例如：“顶点”一词在HowNet有两个代表义项，分别标注义原信息如下，其中每个“xx yy”代表一个义原，“ ”左边为英文右边为中文；义原之间还被标注了复杂的语义关系，如host、modifier、belong等，从而能够精确地表示词义的语义信息。 顶点#1DEF={Boundary|界限:host={entity|实体},modifier={GreaterThanNormal|高于正常:degree={most|最}}}顶点#2DEF={location|位置:belong={angular|角},modifier={dot|点}} 知网： 定义：知识网络，是一个知识系统，而不是一部语义词典。 组成：知网的所有组成文件（包括知识词典）构成了一个有机结合的知识系统。例如，主要特征文件、 次要特征文件、同义、反义以及对义组的形成，以及事件关系和角色转换等都是系统的重要组成部分， 而不仅仅是标注的规格文件。 特质：董振东提出(1)首先应由知识工程师来设计知识库的框架，并建立常识性知识库的原型。(2)在此基础上再向专业性知识库延伸和发展。专业性知识库或称百科性知识库主要靠专业人员来完成。 这里很类似于通用的词典由语言工作者编纂，百科全书则应由各专业的专家编写。知网的研究和建设是实践上述观点的努力。 知网表征的内容： 概念的共性和个性 “人” 是 “医生”和“患者”的共性。 “医生”的个性是他是“医治”的施事，“患者”的个性是他是“患病”的经历者。 概念和概念的关系 概念和属性的关系2和3包含以下关系： (a) 上下位关系 ( 由概念的主要特征体现，请参看《知网管理工具》) (b) 同义关系（可通过《同义、反义以及对义组的形成》获得） (c) 反义关系（可通过《同义、反义以及对义组的形成》获得） (d) 对义关系（可通过《同义、反义以及对义组的形成》获得） (e) 部件- 整体关系（由在整体前标注 % 体现，如“心”，“CPU”等） (f) 属性- 宿主关系（由在宿主前标注 &amp; 体现，如“颜色”，“速度”等） (g) 材料- 成品关系（由在成品前标注 ? 体现，如“布”，“面粉”等） (h) 施事/ 经验者/ 关系主体- 事件关系（由在事件前标注 * 体现，如“医生”，“雇主”等） (i) 受事/ 内容/ 领属物等- 事件关系（由在事件前标注 $ 体现，如“患者”，“雇员”等） (j) 工具- 事件关系（由在事件前标注 * 体现，如“手表”，“计算机”等） (k) 场所- 事件关系（由在事件前标注 @ 体现，如“银行”，“医院”等） (l) 时间- 事件关系（由在事件前标注 @ 体现，如“假日”，“孕期”等） (m) 值- 属性关系（直接标注无须借助标识符，如“蓝”，“慢”等） (n) 实体- 值关系（直接标注无须借助标识符，如“矮子”，“傻瓜”等） (o) 事件- 角色关系（由加角色名体现，如“购物”，“盗墓”等） (p) 相关关系（由在相关概念前标注 # 体现，如“谷物”，“煤田”等）API试用：import OpenHowNet as hownet# 第一次使用的时候下载# hownet.download()hownet_dict = hownet.HowNetDict()result_list = hownet_dict.get_sense(\"包袱\")print(\"The number of retrievals: \", len(result_list))print(\"An example of retrievals: \", result_list)for sense_example in result_list: print(\"Sense example:\", sense_example) print(\"Sense id: \",sense_example.No) print(\"English word in the sense: \", sense_example.en_word) print(\"Chinese word in the sense: \", sense_example.zh_word) print(\"HowNet Def of the sense: \", sense_example.Def) print(\"Sememe list of the sense: \", sense_example.get_sememe_list()) sense_example.visualize_sememe_tree()输出结果：包袱这个词的概念数量，包袱这个词的所有概念展示：概念1的详细信息和概念1的义原树：概念2的详细信息和概念2的义原树：概念3的详细信息和概念3的义原树：概念4的详细信息和概念4的义原树：概念5的详细信息和概念5的义原树：获取所有中文词语# 获取所有中文词语zh_word_list = hownet_dict.get_zh_words()print(\"The number of all Chinese words in HowNet: {}\".format(len(zh_word_list)))print(\"Chinese words in HowNet: \",zh_word_list[:30])# 获取所有概念all_senses = hownet_dict.get_all_senses()print(\"The number of all senses: {}\".format(len(all_senses)))print(\"some senses in HowNet: \",all_senses[:30])# 获取所有英文词语en_word_list = hownet_dict.get_en_words()print(\"The number of all English words in HowNet: {}\".format(len(en_word_list)))print(\"English words in HowNet: \",en_word_list[:30])# 获取所有义原all_sememes = hownet_dict.get_all_sememes()print('There are {} sememes in HowNet'.format(len(all_sememes)))以不同的形式，直接获取这个词的义原。# 获取以list形式展示的义原集合hownet_dict.get_sememes_by_word(word = '包袱', display='list', merge=False, expanded_layer=-1, K=None)# 当 display='list' 时，可以通过设置merge将所有Sense的义原列表合并到同一个列表，以及通过expanded_layer设置每个概念的义原树展开的层数等（expanded_layer默认为-1表示展开所有层）。hownet_dict.get_sememes_by_word(word = '苹果', display='list', merge=True, expanded_layer=-1, K=None)# 获取以词典形式展示的义原集合hownet_dict.get_sememes_by_word(word='包袱',display='dict')[0]# 获取以树的形式组织的义原（得到义原树的根节点）hownet_dict.get_sememes_by_word(word='包袱',display='tree')[0]# 可视化展示义原树 (通过设置参数K来控制需要打印的义原树的数量)hownet_dict.get_sememes_by_word(word='包袱',display='visual',K=2)查看义原之间的关系：# 查找 FormValue|形状值 和 round|圆 这两个义原之间的关系，同时可以选择将整个三元组输出：relations = hownet_dict.get_sememe_relation('FormValue','圆', return_triples=False)print(relations)triples = hownet_dict.get_sememe_relation('FormValue','圆', return_triples=True)print(triples)查看与当前义原存在某种关系的所有义原。# 检索与输入义原存在某种关系的所有义原# 输入的义原可以使用任意语言，但是关系必须为英文小写。同样的，可以选择将整个三元组输出。triples = hownet_dict.get_related_sememes('FormValue', relation = 'hyponym', return_triples=True)print(triples)Reference： 《知网》董振东，董强 HowNet介绍及使用https://blog.csdn.net/ltochange/article/details/119925907 在深度学习时代用HowNet搞事情https://zhuanlan.zhihu.com/p/32688983" }, { "title": "BERT百科大全", "url": "/posts/BERT%E7%99%BE%E7%A7%91/", "categories": "NLP, 模型研究", "tags": "nlp, BERT, 面经", "date": "2022-04-14 13:10:56 +0800", "snippet": "BERT百科大全Attention attention结构的神经网络与递归神经网络相比有什么优点答： attention结构的输入是句子即可，递归神经网络的输入需要包含句子和句子结构 训练Recursive neural net之前，你需要句法树；句法树是一个离散的决策结果，无法连续地影响损失函数，也就无法简单地利用反向传播训练Recursive neural net。 attention结构的神经网络和循环神经网络相比有什么优点？ 循环神经网络是时序的，串行的，不能并行的同时处理句子的所有词，而attention结构是并行的，利用点乘同时计算所有token之间的相似度。 循环神经网络会有梯度爆炸和梯度消失的问题，因为它是前面的不断累乘，可能会越乘越大乘到正无穷，也可能会越乘越小乘到0，但是因为attention是并行点乘的，不会有梯度累计的问题。 也是由于同样的原因，循环神经网络面对长文本的时候会出现遗忘，忘掉比较远的位置的信息。attention不会遗忘，但是由于计算成本，BERT也是设定了512字节的长度限制。 RNN只考虑左边的信息，除非用双向RNN，不过双向RNN需要完整的数据序列，才能预测任意位置。比如说你要用双向RNN模型构建一个语音识别系统，你需要等待这个人说完，然后获取整个语音表达才能处理这段语音，并进一步做语音识别。 attention结构中Q、K、V的含义及作用答：Q=Query，K=Key，V=Value，Q点乘K得到Q中每个token的重要性权重，然后再点乘V，这样就能起到给V中每个token加一个权重，使模型学会“有的放矢”。 :paw_prints:transformer网络中一共有多少种不同的attention（3种）答：[^3种attention] multihead-selfattention:encoder中，Q=K=V。 masked-multihead-selfattention: Decoder中，当前位置只能注意到其位置之前的信息，通过将注意力矩阵做mask实现，如图1所示。 Cross-attention: query 来自于decoder中上一层的输出，而K 和V使用的是encoder中的输出。 注意力机制有哪几种？答：1.intra attention和inter attentionintra attention就是self attention就是Q=K=V，inter attention就是Q和kV不相等，也就说计算两个句子的相似度，比如transformer的decoder部分的cross attention 2.global attention和local attention注意力机制计算的是所有token，local attention仅关注部分token，比如只关注当前词前k个和后k个 3.soft attention和hard attention如果注意力机制能跟随神经网络后向传播的过程中得到优化那么，就是soft attention，否则就是hard attention。 BERT bert的原理、结构答：bert的目标是生成一个更好的语义表征向量，它创新点在于利用注意力机制替代之前的循环神经网络，注意力机制的原理是将输入文本当作三个相同的向量QKV，以Q点乘K计算出该文本中各部分的权重，把这个权重再点乘V，得到对这个句子各部分的一个“有的放矢”的向量表征。结构上，bert是Transformer的encoder部分，第一层是embedding层，包含position embedding，segment embedding，token embedding，position embedding是位置编码，采用sin/cos交替的方法编码绝对位置，segment embedding是为了应对句对任务的，标识出token属于哪一个句子，token embedding是一个随机初始的各个token的表征向量；第二层是multihead-selfattention层，这一层的输入是三个embedding concat拼接的一个向量，输出是一个中间语义向量，第三层是残差层和归一化层，残差层是为了:o:解决深度神经网络的退化问题[^残差网络1][^残差网络2]，归一化层是为了将当前层输出的向量做一个限定[^归一化层]，限定其最大值和最小值，这样有助于网络快速收敛，防止梯度爆炸和梯度消失；第四层是一个前馈神经网络；第五层又是一个残差层和归一化层。 bert中哪些结构（layer或block）的训练参数比较多答：从BERT的时间复杂度就可以看出，它参数最多的地方在注意力块矩阵点乘的地方。 以下为引用[^BERT中的参数]总体来说bert模型的参数主要包含3部分：Embeddding层的参数，transformer blocks的参数和最后输出的全连接参数。第一部分的参数：30522768+512768+4768第二部分参数：【（768768+768）4+（7682）+（30727682+3072）+7683】12第三部分参数：768*768+768参数个数总计：109482240~1.09亿而BERT-Base, Chinese BERT-Base, Chinese总是约为1.02亿。 bert模型采用mask的具体策略，以及为什么要这么设计答：把输入句子的15%的token替换掉：其中被替换的token有80%的概率被替换成[MASK]10%的概率被替换成任意一个其他token10%的概率原封不动 让模型预测和还原被遮盖掉或替换掉的部分，损失函数只计算随机遮盖和替换部分的Loss。这样做的原因类似于word2vec中的CBOW：给出上下文预测核心词。 bert及其变体中常用的mask方式及特点（dynamic mask，whole word mask，phrase mask， entity mask）答：whole word mask：起源于谷歌的bert-wwm模型，是一种全词屏蔽的方法，这个主要是针对中文的，因为中文的字蕴含的意义远远少于词，并且字屏蔽的话会导致信息泄露，全词屏蔽能够有效的让模型学习语义。百度进一步提出引入命名实体的外部知识，也就是实体屏蔽，在完形填空的测试上，显著提升。entity mask：实体屏蔽，随机屏蔽句子中的实体，有效的是模型学习句法关系。动态屏蔽：每次向模型输入一个序列时都会生成新的掩码模式。这样，在大量数据不断输入的过程中，模型会逐渐适应不同的掩码策略，学习不同的语言表征。huggingface中的data allcator也是动态屏蔽，它是每一个epoch的mask策略不同。[^mask的变种] 中文bert-base预训练模型所有的参数量，细分到每一个结构的参数量答：https://zhuanlan.zhihu.com/p/452369195embedding层：(21128+512+2)768attention层：()12 bert结构中embedding部分具体是怎么样的（word embedding+position embedding+segment embedding）答：略 绝对位置和相对位置的区别答：绝对位置就是1-512，相对位置就比较复杂，可能就需要包含多种标识，比如一种方法是按分词或词组标记位置，一个词内0，1，2…5，后一个词0,1,2.这样也可以用一个start position，一个end position同时标识一个token，标识这个token所在词语的起终位置。相对位置能帮助模型突破512长度的限制，在处理长文本数据的时候，往往需要这个策略。相对位置的代表是Nezha，他的创新点就是函数式相对位置编码，和transformer的sincos位置编码的区别是，sincos是sin（绝对位置/10000的d分之2-k次方）而nezha是sin（j-i的相对错位差/10000的d分之2-k次方）[^Nezha相对位置编码] position embedding的实现方式有哪两种（functional position embedding，如transformer和华为的NEZHA；parametric position embedding，如bert）答：函数相对位置编码：Nezha函数绝对位置编码：Transformer参数绝对位置编码：BertTransformer transformer decoder部分的inference过程重点看beam search的实现，使用tensorflow或torch框架（尽量按google源码的思路）答： transformer网络中一共有多少种不同的attention（3种）答：3种[^3种attention]（1）self-attention：encoder种的Q=K=V（2）masked self-attention：decoder中的，当前位置只能看到它前面的token（3）cross-attention：query来自于decoder中上一层的输出，而K和V来自于Encoder的输出。 GPT 如何用gpt-3的prompt机制挖掘以新冠疫情为主题的稀疏文本以及如何在保险场景实现zero-shot； 简单聊一下gpt-1到gpt-3的发展历程答：gpt-1:相比于bert，多头自注意力机制被替换成了masked多头自注意力。仍然是无监督预训练，有监督微调gpt-2:相比于gpt-1,去掉了微调层，通过引入多任务学习，不再针对下游任务微调模型，gpt2能自动识别是什么任务然后完成任务修改了layer normalization的位置，放在了sub-block之前，并在最后一层selfattention层之后增加了一层layer normalizationgpt-3进一步加大了参数量175billionERNIE 简单聊一下ERNIE1.0到3.0发展历程答：1.0版本使用了三种mask结合，传统的mask+wwm+entity mask；使用了大量的异质数据预训练2.0版本使用持续性的多任务学习，每次有新任务过来，就用上一个任务训练的参数，同时训练新任务和旧任务3.0版本引入了知识图谱拓展模型 deberta中的两个优化点是什么？disentangled attention和enhanced mask decoder RoBERTa相比与BERT的改进？答：1.去掉了NSP任务2.使用了动态mask3.更大更细粒度的词汇表，用更大的 byte 级别 BPE 词汇表来训练 BERT，这一词汇表包含 50K 的 subword 单元，且没有对输入作任何额外的预处理或分词。 SKEP相比BERT的改进？答：SKEP主要考虑的语言模型在情感词的解析能力，它通过在mask上引入情感词mask，属性词-情感词对mask，mask通用字来促使模型习得情感词的语义。1 Nezha的改进？答：1.函数式相对位置编码2.Nezha使用jieba分词完成wwm3.混合精度训练4.Lamb优化器 XLNet自回归模型解决mask的问题引入transformer-xl transformer-xl答：首先取第一个段进行计算，然后把得到的结果的隐藏层的值进行缓存，第二个段计算的过程中，把缓存的值拼接起来再进行计算优化方法 模型蒸馏的具体实现过程 损失函数是什么？答：损失函数是通过样本来计算模型分布和目标分布之间的差异 KL散度和交叉熵的联系与区别答：如果目标分布是常数，比如分类的分布是已知且不变的，这时就用交叉熵来作为损失函数。KL散度更通用的一种计算两个分布差异，但是交叉熵运算更简单。 KL散度和交叉熵都可以用来作为模型的loss函数，但二者的使用场景不一样。在这里引申一下模型loss的含义：“通过样本来计算模型分布与目标分布间的差异。”，这就是KL散度的作用。但有时候我们的目标分布会是常数，也就是这个分布是已知且不变的，例如分类任务，这个时候我们就会使用交叉熵来衡量模型的预测分布与实际分布之间的差异。 常用的文本数据增强方法（全面完整的回答可以从语种层面，letter、subword、word和语序层面，利用mlm过程、利用wordnet近义词替代等角度）答：（1）中英文：利用同义词反义词词典替换、语序颠倒、增加删除否定词、反向翻译（2）英文：利用字母替换，词根词缀替换 如何在一个3GB内存空间中部署一个深度学习模型； 说一下对多任务训练（multi task learning）和多领域训练（multi domain learning）的理解，最好举一个例子答：多任务学习：[^多任务学习]单任务学习（single task learning）：一个loss，一个任务，例如NLP里的情感分类、NER任务一般都是可以叫单任务学习。多任务学习（multi task learning）：简单来说有多个目标函数loss同时学习的就算多任务学习。也就是一个模型，后面分别接几个特定层分别计算不同的目标loss，有硬参数共享，也就是共享层训练，特定层隔离自己训练自己的，有软参数共享，也就是特定层每一层都在训练过程中互相交互着。多任务学习的好处是：有效节省计算资源的前提下，学习数据的多维度特征。多领域学习：多任务学习有一些弊端，一是需要样本标注好多个维度的标签，耗费人力，二是机器学习训练数据遵循独立同分布假设，可是需要多角度学习数据那肯定是有各自不同的侧重，也就是违背了同分布的假设。多领域学习引入了MMD-loss，来对多领域的特征数据进行分布约束，使不同领域的数据在特征空间趋向独立同分布。[^多领域学习]WWM-Whole word mask这篇文章讲了wwm怎么实现的：https://zhuanlan.zhihu.com/p/268515387?utm_source=qq这是哈工大wwm模型github下面的一个issue，网友提了一个问题： 在你们的工作中，比如mask词的时候，一个词为哈利波特，那么在你们的方法中，是不是只要这个词被mask，那一定是[mask][mask][mask][mask]的形式，还是偶尔会出现[mask]利[mask][mask]的形式，不知道你们是如何设置的（不考虑那个mask80%10%10%的那个随机概率），如果是前者，那么这种完全避免局部共现的设置会不会对结果有影响。https://github.com/ymcui/Chinese-BERT-wwm/issues/4Reference： https://zhuanlan.zhihu.com/p/267837817 &#8617; " } ]
