<!DOCTYPE html><html lang="zh-CN" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="A Closer Look at How Fine-tuning Changes BERT" /><meta property="og:locale" content="zh_CN" /><meta name="description" content="A Closer Look at How Fine-tuning Changes BERT" /><meta property="og:description" content="A Closer Look at How Fine-tuning Changes BERT" /><link rel="canonical" href="https://zoechen119.github.io/posts/PaperNote_%E5%BE%AE%E8%B0%83%E5%A6%82%E4%BD%95%E6%94%B9%E5%8F%98BERT/" /><meta property="og:url" content="https://zoechen119.github.io/posts/PaperNote_%E5%BE%AE%E8%B0%83%E5%A6%82%E4%BD%95%E6%94%B9%E5%8F%98BERT/" /><meta property="og:site_name" content="zoe Chen" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-03-02T00:00:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="A Closer Look at How Fine-tuning Changes BERT" /><meta name="twitter:site" content="@twitter_username" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-03-02T00:00:00+08:00","datePublished":"2023-03-02T00:00:00+08:00","description":"A Closer Look at How Fine-tuning Changes BERT","headline":"A Closer Look at How Fine-tuning Changes BERT","mainEntityOfPage":{"@type":"WebPage","@id":"https://zoechen119.github.io/posts/PaperNote_%E5%BE%AE%E8%B0%83%E5%A6%82%E4%BD%95%E6%94%B9%E5%8F%98BERT/"},"url":"https://zoechen119.github.io/posts/PaperNote_%E5%BE%AE%E8%B0%83%E5%A6%82%E4%BD%95%E6%94%B9%E5%8F%98BERT/"}</script><title>A Closer Look at How Fine-tuning Changes BERT | zoe Chen</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="zoe Chen"><meta name="application-name" content="zoe Chen"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/img/avatar.jfif" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">zoe Chen</a></div><div class="site-subtitle font-italic">nlper, dler, sims4er</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>首页</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>分类</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>标签</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>归档</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>关于</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/zoeChen119" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['zoe9698','163.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> 首页 </a> </span> <span>A Closer Look at How Fine-tuning Changes BERT</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> 文章</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="搜索..."> </span> <span id="search-cancel" >取消</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>A Closer Look at How Fine-tuning Changes BERT</h1><div class="post-meta text-muted"> <span> 发表于 <em class="" data-ts="1677686400" data-df="YYYY-MM-DD" data-toggle="tooltip" data-placement="bottom"> 2023-03-02 </em> </span><div class="d-flex justify-content-between"> <span> 作者 <em> <a href="https://github.com/zoeChen119">陈政伊</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3561 字"> <em>19 分钟</em>阅读</span></div></div></div><div class="post-content"><h1 id="a-closer-look-at-how-fine-tuning-changes-bert">A Closer Look at How Fine-tuning Changes BERT</h1><h2 id="1-introduction"><span class="mr-2">1 Introduction</span><a href="#1-introduction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h5 id="本文探究了这三个问题"><span class="mr-2">本文探究了这三个问题：</span><a href="#本文探究了这三个问题" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><ol><li>Does fine-tuning always improve performance?微调是否的确是总是能提高性能？<li>How does fine-tuning alter the representation to adjust for downstream tasks? 微调是怎么修正representation以适应下游任务的？<li>How does fine-tuning change the geometric structure of different layers? 微调具体是怎么改变不同层的几何结构的？</ol><blockquote><p>我的问题：微调怎么调整模型参数的？</p></blockquote><h5 id="本文用-2个probing-tech-来检测bert不同变体模型在-5个下游任务-上representation的情况的"><span class="mr-2">本文用 [2个probing-tech] 来检测BERT不同变体模型在 [5个下游任务] 上，representation的情况的：</span><a href="#本文用-2个probing-tech-来检测bert不同变体模型在-5个下游任务-上representation的情况的" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>2个probing tech</p><ul><li>classifier-based probing <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup><li>DIRECTPROBE <sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup></ul><p>5个下游任务</p><ul><li>词性标注<li> <font color="OrangeRed">dependency head prediction依赖头检测</font><li> <font color="OrangeRed">preposition supersense role</font><li>function prediction函数预测<li>文本分类</ul><h5 id="previous发现"><span class="mr-2">previous发现</span><a href="#previous发现" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>微调改变较高层而不是较低层，并且语言信息在微调过程中不会丢失。<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup> <sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup></p><h5 id="本文的新发现"><span class="mr-2">本文的新发现</span><a href="#本文的新发现" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><ol><li><p>a. 微调引入了训练集和测试集之间的分歧，在大多数情况下，这不会损害泛化性。 b. 有一种情况，微调会损害泛化性。这种情况时，经过微调后，训练集和测试集之间的差异（分歧）也是最大的。</p><li><p>微调如何改变the representation space的<font color="OrangeRed">labeled region</font>。 a. 对于<font color="OrangeRed">任务标签 不可线性分离</font>的表征，发现微调通过将具有相同标签的点归入少量的群组（最好是一个）来进行调整，从而简化了基础表征。 b. 这样做使使用微调表示的标签比未微调untuned表示的标签更容易线性分离。 c. 对于任务标签已经是线性可分离的表示，我们发现微调会将表示不同标签的点簇彼此推开，从而在标签之间引入较大的分离区域separating regions。簇不是简单地缩放点，而是以不同的方向和不同的范围移动（通过欧几里德距离测量）。 d. 总的来说，与untuned的表示相比，这些簇变得遥远。我们推测，组之间的扩大区域允许一组更大的分类器，可以将它们分开，从而导致更好的泛化（§4.3）。 本文通过研究跨任务微调的效果来验证这个“距离假设”。 观察到，通过改变代表不同标签的簇之间的距离，相关任务的微调也可以为目标任务提供有用的信号（§4.4）。</p><li>微调不会随意地改变the higher layers。<li>微调在很大程度上保留了标签簇的相对位置，同时重新配置空间以适应下游任务。<li>Informally，可以说微调只是“slightly”改变了the higher layers。</ol><h2 id="2-2个probing-tech"><span class="mr-2">2 [2个probing tech]</span><a href="#2-2个probing-tech" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><blockquote><ol><li>classifier-based probing <sup id="fnref:1:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> <sup id="fnref:2:1" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> 基于分类器的探针，用于评估在一个任务中，representation对分类器的support程度。<li>DIRECTPROBE <sup id="fnref:3:1" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> 用于分析representation的几何结构</ol></blockquote><h3 id="1-classifier-based-probing--"><span class="mr-2">(1) classifier-based probing <sup id="fnref:1:2" role="doc-noteref"><span class="mr-2"><a href="#fn:1" class="footnote" rel="footnote"><span class="mr-2">1</a></sup> <sup id="fnref:2:2" role="doc-noteref"><span class="mr-2"><a href="#fn:2" class="footnote" rel="footnote"><span class="mr-2">2</a></sup></span><a href="#1-classifier-based-probing--" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>研究中，“经过训练的分类器”是最常用的probes。具体来说，如果想要了解，在一个task中，一个representation对labels的编码程度，那么就在它上面训练一个probing classifier（训练的时候，embeddings本身保持冻结）。</p><p>在本文的研究中，用的是一个2-layers的神经网络作为probe classifiers。 其他细节：</p><ul><li>用网格搜索选择the best 超参数；<li>每一个最好的分类器都被用了不同的初始化训练了5次；<li>matrices是每个分类器的平均准确率和标准差；<li>hidden layer sizes从${32,64,128,256} \times {32,64,128,256}$选择的<li>正则化权重范围$[10^{-7},10^0]$<li>所有模型的hidden layers都使用ReLU作为激活函数<li>所有模型的优化器都是Adam<li>训练的iterations的最大值都在1000<li>scikit-learn v0.22</ul><p>分类器探针旨在衡量 [基于上下文的representation] 如何捕捉语言属性。分类性能可以帮助我们评估微调的效果。</p><blockquote><p>classifier-based probing的局限性： 它将representation视为一个黑盒，只关注最终任务的性能，而不能揭示finetune是如何改变空间的基本几何结构的。 为什么要引入DIRECTPROBE，因为这个prob tech是从几何角度分析embedding的技术。</p></blockquote><h3 id="2directprobe"><span class="mr-2">（2）DIRECTPROBE</span><a href="#2directprobe" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>对于给定label的任务，DIRECTPROBE是把具有相同label的points聚成簇，然后返回簇。 <img data-src="/assets/img/2023-03-02-PaperNote_微调如何改变BERT/2023-03-03-10-06-28.png" alt="" data-proofer-ignore> 无论是左图还是右图，决策边界都必须穿过这些簇之间的间隔区域。 左图：一个简单的二元分类任务，虚线是决策边界 右图：DIRECTPROBE的结果图（之一），灰色是“必须穿过的这些簇之间的间隔区域”，连接起来的点是DIRECTPROBE生成的簇。</p><p>finetune的时候，不同的任务下，同一个词基于上下文的representation会有不同的表示，因此，有必要在<strong>给定任务</strong>的前提下，probe这些representation。</p><p>得到了这些簇之后，就可以测量这些簇的属性，比如下面这三个：</p><ol><li><p>Number of Clusters (1) 簇的数量 = label的数量 简单的<strong>线性</strong>多任务<strong>分类器</strong> (2) 簇的数量 &gt; label的数量 就说明至少有两个同label的样本没有聚到一个簇里，那就需要<strong>非线性分类器</strong></p><li><p>Distances between Clusters 簇之间的距离揭示了representation的内部结构，通过跟踪fine-tuning期间，这些距离的变化，可以研究representation是怎么变化的。 为了计算这些距离，本文基于定理fact：一个簇代表一个凸对象a convex object。这样就可以使用max-margin separators最大边距分隔符来计算距离。 本文训练了一个线性SVM来找到max-margin分隔符并计算它的margin。簇之间的距离=2*margin。</p><li><p>Spatial Similarity 簇之间的距离也可以揭示2个representation的空间相似性。 如果两个representation在簇之间有相似的相对距离，那么对于当前的任务，这2个representations是相似的。</p></ol><p>用这些distance组成一个distance vector $v$,把它当作representation，$v_i$是一对label的簇们之间的距离。 一个任务有n个label: (1) 当数据集在这个representation下线性可分离，也就是说<strong>簇的数量 = label的数量</strong>,那么： \(size(v)=\frac{n(n-1)}{2}\)</p><p>PS.本文研究的大多数representation都是这种情况。</p><p>(2) 非线性可分的情况好像没讨论 ？</p><p>对于一个带label的任务，还可以计算两个representation的距离向量之间的皮尔逊相关系数，把这个作为两个representation的一种相似性度量。 另外，这个系数也可以用来衡量两个有标签的数据集在相同representation时的相似性。本文利用这一观察结果来分析训练集和测试集在fine-tuned representation下区别。</p><h2 id="实验设置"><span class="mr-2">实验设置</span><a href="#实验设置" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="31-representation"><span class="mr-2">3.1 Representation</span><a href="#31-representation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="/assets/img/2023-03-02-PaperNote_微调如何改变BERT/2023-03-03-11-33-17.png" alt="" data-proofer-ignore> 这些模型basic架构是相同的，但是能力不同（比如不同的layers和hidden size）。它们都是基于英文文本&amp;uncased。 对于那些被tokenizer分解为subword的tokens，本文把这些token 的representation的subword embedding进行平均。</p><blockquote><p>HuggingFace v4.2.1 Pytorch v1.6.0</p></blockquote><h3 id="32-tasks"><span class="mr-2">3.2 Tasks</span><a href="#32-tasks" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>这些任务覆盖①syntactic语义②semantic语法，这两方面的BERT模型的能力。</p><ol><li>Part-of-speech tagging (POS)词性标注 这个任务帮助我们理解是否这个representation捕捉了<strong>粗粒度</strong>的语义分类<li>Dependency relation (DEP) 预测两个tokens之间的语义依赖关系。 这个任务帮助我们理解这个representation是否可以描述words之间的语义关系，以及能描述到什么程度。 这个任务中，需要给一对tokens分配一个类别。具体来说就是，把两个token的（BERT生成的）基于上下文的representation连接起来，这个视为“对”的representation。 数据集同POS。<li><p>Preposition supersense disambiguation介词超义消岐 分类任务，为了消除介词的语法含义的歧义。本文仅在Streusle v4.2 corpus的single-token介词上训练和evaluate。</p><ol><li><p>预测介词的语义角色semantic role(PS-role)</p><li><p>预测介词的语义功能semantic function(PSfxn)</p></ol><li>Text classification TREC50数据集，每个句子都有50个语法label。 用的是[CLS]token作为句子的representation。 这个任务展示了representation表征一个句子的能力。</ol><h3 id="33-fine-tuning-setup"><span class="mr-2">3.3 Fine-tuning Setup</span><a href="#33-fine-tuning-setup" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>分别单独在上面提过的5个任务上fine-tune那些models。fine-tuned之后的模型生成基于上下文的representation。</p><p>初步实验： 通常，对于BERT_tiny来说，3-5个epoch的fine-tune不够，这些小的representation需要更多的epoch。 除了$BERT_{base}$，其他的models都用10epochs来fine-tune；$BERT_{base}$用3epochs。 PS.fine-tune阶段和用于probing的分类器训练阶段分开的，probing classifier是在原始representation和fine-tuned之后的representation之上从头开始train一个2layer神经网络，确保比较是公平的。</p><h2 id="发现和分析"><span class="mr-2">发现和分析</span><a href="#发现和分析" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ol><li>用classifier去probing是否fine-tuning总是提高分类器的性能。(§4.1)<li>用DIRECTPROBE给出了一个几何解释，解释为什么fine-tuning提高分类器性能了。(§4.2 and §4.3)<li>用跨任务fine-tune确认几何解释。(§4.4)<li>分析fine-tuning是如何改变BERT_base的不同层的几何形状的。(§4.5)</ol><h3 id="41-fine-tuned的性能"><span class="mr-2">4.1 Fine-tuned的性能</span><a href="#41-fine-tuned的性能" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="/assets/img/2023-03-02-PaperNote_微调如何改变BERT/2023-03-03-15-32-23.png" alt="" data-proofer-ignore> 上面这个表是BERT_small的结果，tuned指的是在模型最后一层的基础上fine-tuned的结果。最后一列是训练集和测试集的空间相似度Spatial Similarity。 <img data-src="/assets/img/2023-03-02-PaperNote_微调如何改变BERT/2023-03-03-15-37-26.png" alt="" data-proofer-ignore> 这个表是上表的完整版。一些条目丢失，因为相似性只能在对给定任务线性可分的表示上计算。</p><p>结论①：fine-tuning分散了训练集和测试集 在微调之后，所有的相似性都会降低，这意味着由于微调，训练和测试集会有所不同。在大多数情况下，这种差异不足以降低性能。</p><p>结论②：也有例外，fine-tuning损害了性能 BERT_small在PS-fxn任务上，tuned之后的性能下降了，并且训练集和测试集的相似度仅0.44，所以作者猜测或许控制训练集和测试集的相似度可以确保微调是有益的。但不确定需要进一步研究。</p><h3 id="42-representations的线性"><span class="mr-2">4.2 Representations的线性</span><a href="#42-representations的线性" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>结论①：</p><p>结论②：</p><h3 id="43-labels的空间结构"><span class="mr-2">4.3 labels的空间结构</span><a href="#43-labels的空间结构" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>结论①：</p><p>结论②：</p><h3 id="44-跨任务fine-tuning"><span class="mr-2">4.4 跨任务fine-tuning</span><a href="#44-跨任务fine-tuning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>结论①：</p><p>结论②：</p><h3 id="45-layer-behavior"><span class="mr-2">4.5 Layer Behavior</span><a href="#45-layer-behavior" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>结论①：微调不会任意改变表示，即使对于更高的层也是如此</p><p>结论②：通过上下两层的直观比较来分析不同层的变化。这里选的是BERT_base，POS tagging任务。</p><p><img data-src="/assets/img/2023-03-02-PaperNote_微调如何改变BERT/2023-03-03-16-10-03.png" alt="" data-proofer-ignore> 基于POS标签任务和BERTbase的微调前后标签质心差向量的PCA投影。</p><p><img data-src="/assets/img/2023-03-02-PaperNote_微调如何改变BERT/2023-03-03-16-11-23.png" alt="" data-proofer-ignore> 基于dependency prediction task和BERTbase的微调前后标签质心之间差异向量的PCA投影。</p><p><img data-src="/assets/img/2023-03-02-PaperNote_微调如何改变BERT/2023-03-03-16-12-14.png" alt="" data-proofer-ignore> 基于Supersense function task和BERTbase的微调前后标签质心差向量的PCA投影.</p><p><img data-src="/assets/img/2023-03-02-PaperNote_微调如何改变BERT/2023-03-03-16-14-08.png" alt="" data-proofer-ignore> 基于Supersense role task和BERTbase的微调前后标签质心之间差异向量的PCA投影。</p><p>图7-10显示了基于BERTbase进行微调前后标签质心差向量的PCA投影。</p><hr /><h2 id="note"><span class="mr-2">Note：</span><a href="#note" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="1-bert的预训练和微调微调和p-tuning"><span class="mr-2">1. BERT的预训练和微调，微调和P-tuning</span><a href="#1-bert的预训练和微调微调和p-tuning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>BERT的微调过程中是有反向传播的。微调是在预训练模型的基础上，为了适应下游任务而对所有参数进行调整的过程。反向传播是一种优化算法，用于计算梯度并更新参数。</p><p>预训练和微调的区别是：</p><ul><li><p>预训练是用大量的未标记数据来训练一个通用的模型，例如BERT，以学习语言的特征和表示。 预训练：随机初始化一个网络模型的参数，然后用大量的未标记数据来训练模型，直到模型的损失越来越小。将训练好的模型的参数保存下来，作为预训练模型。</p><li><p>微调是用少量的标记数据来调整预训练模型的参数，以适应特定的下游任务，例如文本分类、命名实体识别等。 微调：使用预训练模型的参数作为一个新任务的初始化参数，然后用少量的标记数据来训练模型，根据结果不断进行一些修改。将修改后的模型保存下来，作为微调模型。</p></ul><p>P-tuning和微调的区别是：</p><ul><li>P-tuning是一种提示优化方法，它只更新预训练模型中的一些特殊的token（如[unused*]），而不更新整个模型的参数。这些特殊的token可以作为模板来引导模型进行下游任务。<li>微调是一种常用的迁移学习方法，它更新预训练模型中的所有参数，以适应下游任务。微调需要更多的内存和计算资源，而且容易过拟合。</ul><p>P-tuning更新参数，但只更新一些特殊的token，而不是整个模型的参数。这些特殊的token可以看作是模型的前缀，它们可以影响模型的输出。P-tuning只需要很少的参数来微调，因此可以节省内存和计算资源。P-tuning只更新了一些特殊的token，比如[unused1]～[unused6]，它们可以看作是模型的前缀。这些token可以根据标注数据来学习，从而影响模型的输出。其他的模型参数都是冻结的，不会更新。</p><h3 id="2-代码不可复现"><span class="mr-2">2. 代码不可复现</span><a href="#2-代码不可复现" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>因为用了gurabi optimizer，docker容器中的授权很难搞要联系客服</p><h3 id="3-什么是凸对象"><span class="mr-2">3. 什么是凸对象？</span><a href="#3-什么是凸对象" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h2 id="reference"><span class="mr-2">Reference</span><a href="#reference" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><div class="footnotes" role="doc-endnotes"><ol><li id="fn:1" role="doc-endnote"><p><a href="https://readpaper.com/paper/2953369973">What do you learn from context? Probing for sentence structure in contextualized word representations</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:1:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:1:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p><li id="fn:2" role="doc-endnote"><p><a href="https://readpaper.com/paper/2941666437">Probing what different NLP tasks teach machines about function word comprehension.</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:2:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:2:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p><li id="fn:3" role="doc-endnote"><p><a href="https://readpaper.com/paper/3154493564">DirectProbe: Studying Representations without Classifiers</a> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:3:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p><li id="fn:4" role="doc-endnote"><p><a href="https://readpaper.com/paper/3103368673">What Happens To BERT Embeddings During Fine-tuning?</a> <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:5" role="doc-endnote"><p><a href="https://readpaper.com/paper/3092448486">On the Interplay Between Fine-tuning and Sentence-level Probing for Linguistic Knowledge in Pre-trained Transformers</a> <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p></ol></div></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/nlp/'>NLP</a>, <a href='/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/'>论文笔记</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/nlp/" class="post-tag no-text-decoration" >nlp</a> <a href="/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/" class="post-tag no-text-decoration" >预训练模型</a> <a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" class="post-tag no-text-decoration" >论文笔记</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> 本文由作者按照 <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> 进行授权</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">分享</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=A+Closer+Look+at+How+Fine-tuning+Changes+BERT+-+zoe+Chen&url=https%3A%2F%2Fzoechen119.github.io%2Fposts%2FPaperNote_%25E5%25BE%25AE%25E8%25B0%2583%25E5%25A6%2582%25E4%25BD%2595%25E6%2594%25B9%25E5%258F%2598BERT%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=A+Closer+Look+at+How+Fine-tuning+Changes+BERT+-+zoe+Chen&u=https%3A%2F%2Fzoechen119.github.io%2Fposts%2FPaperNote_%25E5%25BE%25AE%25E8%25B0%2583%25E5%25A6%2582%25E4%25BD%2595%25E6%2594%25B9%25E5%258F%2598BERT%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fzoechen119.github.io%2Fposts%2FPaperNote_%25E5%25BE%25AE%25E8%25B0%2583%25E5%25A6%2582%25E4%25BD%2595%25E6%2594%25B9%25E5%258F%2598BERT%2F&text=A+Closer+Look+at+How+Fine-tuning+Changes+BERT+-+zoe+Chen" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="分享链接" data-title-succeed="链接已复制！"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">最近更新</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/C25_11%E5%85%A5%E9%97%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%860/">【NLP入门趣味题】肉眼找朋友</a><li><a href="/posts/C25_11%E5%85%A5%E9%97%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%861/">【NLP入门趣味题】探索语言模型与词向量</a><li><a href="/posts/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a><li><a href="/posts/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%AE%9E%E9%AA%8C/">多模态实验</a><li><a href="/posts/%E5%A4%9A%E6%A8%A1%E6%80%81RAG/">多模态rag</a></ul></div><div id="access-tags"><div class="panel-heading">热门标签</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/">技术综述</a> <a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> <a class="post-tag" href="/tags/rag/">RAG</a> <a class="post-tag" href="/tags/%E8%AF%AD%E8%A8%80%E5%AD%A6/">语言学</a> <a class="post-tag" href="/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/">预训练模型</a> <a class="post-tag" href="/tags/bert/">BERT</a> <a class="post-tag" href="/tags/courses/">Courses</a> <a class="post-tag" href="/tags/chatgpt/">ChatGPT</a> <a class="post-tag" href="/tags/mamba/">Mamba</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">文章内容</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>相关文章</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/PaperNote_KEPLER/"><div class="card-body"> <em class="small" data-ts="1676217600" data-df="YYYY-MM-DD" > 2023-02-13 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>KEPLER：知识嵌入和预训练语言表示的统一模型</h3><div class="text-muted small"><p> KEPLER：知识嵌入和预训练语言表示的统一模型 Figures 1. 首图 这是论文的首图，示范了给知识图谱中每个实体增加描述的效果。 2. （MODEL）图一 一句话概括 KEPLER通过“联合训练两个目标objectives”来把事实知识隐式融合进语言表征中。 组件1：Encoder 每个Token经过L层hiddenlayers的词表征： \(H_i∈ \Bbb{...</p></div></div></a></div><div class="card"> <a href="/posts/PaperNote_%E5%9B%A0%E6%9E%9C%E6%8E%A8%E7%90%86%E4%B8%8ELLMs/"><div class="card-body"> <em class="small" data-ts="1683561600" data-df="YYYY-MM-DD" > 2023-05-09 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>因果推理与大语言模型：开辟因果关系的新前沿</h3><div class="text-muted small"><p> Causal Reasoning and Large Language Models: Opening a New Frontier for Causality 这篇论文一共42页，值得反复研读。 当处理真实世界的因果任务时，人们在制定(子)问题、迭代和验证其前提和含义时，策略性地在基于逻辑和协方差的因果推理之间交替。简单来说，这句话描述了人们在处理真实世界中的因果任务时，会使...</p></div></div></a></div><div class="card"> <a href="/posts/PaperNote_%E8%AF%8D%E6%A0%BC%E6%B3%95/"><div class="card-body"> <em class="small" data-ts="1665209456" data-df="YYYY-MM-DD" > 2022-10-08 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Paper Note:词格法</h3><div class="text-muted small"><p> Paper Note:词格法 《Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models》 作者：Yuxuan Lai, Yijia Liu, Yansong Feng, Songfang Huang, Dongyan Zhao 阿里巴巴达摩院 | 中国...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/PaperNote_KEPLER/" class="btn btn-outline-primary" prompt="上一篇"><p>KEPLER：知识嵌入和预训练语言表示的统一模型</p></a> <a href="/posts/PaperNote_%E6%80%9D%E7%BB%B4%E9%93%BE/" class="btn btn-outline-primary" prompt="下一篇"><p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</p></a></div><script src="https://utteranc.es/client.js" repo="zoeChen119/zoeChen119.github.io" issue-term="title" crossorigin="anonymous" async> </script> <script type="text/javascript"> $(function() { const origin = "https://utteranc.es"; const iframe = "iframe.utterances-frame"; const lightTheme = "github-light"; const darkTheme = "github-dark"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } addEventListener("message", (event) => { let theme; /* credit to <https://github.com/utterance/utterances/issues/170#issuecomment-594036347> */ if (event.origin === origin) { /* page initial */ theme = initTheme; } else if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); } else { return; } const message = { type: "set-theme", theme: theme }; const utterances = document.querySelector(iframe).contentWindow; utterances.postMessage(message, origin); }); }); </script></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://github.com/zoeChen119">陈政伊</a>. <span data-toggle="tooltip" data-placement="top" title="除非另有说明，本网站上的博客文章均由作者按照知识共享署名 4.0 国际 (CC BY 4.0) 许可协议进行授权。">保留部分权利。</span></p></div><div class="footer-right"><p class="mb-0"> 本站由 <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> 生成，采用 <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> 主题。</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">热门标签</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/">技术综述</a> <a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> <a class="post-tag" href="/tags/rag/">RAG</a> <a class="post-tag" href="/tags/%E8%AF%AD%E8%A8%80%E5%AD%A6/">语言学</a> <a class="post-tag" href="/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/">预训练模型</a> <a class="post-tag" href="/tags/bert/">BERT</a> <a class="post-tag" href="/tags/courses/">Courses</a> <a class="post-tag" href="/tags/chatgpt/">ChatGPT</a> <a class="post-tag" href="/tags/mamba/">Mamba</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">发现新版本的内容。</p><button type="button" class="btn btn-primary" aria-label="Update"> 更新 </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">搜索结果为空</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/zh.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
