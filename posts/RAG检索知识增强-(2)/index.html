<!DOCTYPE html><html lang="zh-CN" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="RAG" /><meta property="og:locale" content="zh_CN" /><meta name="description" content="RAG调研" /><meta property="og:description" content="RAG调研" /><link rel="canonical" href="https://zoechen119.github.io/posts/RAG%E6%A3%80%E7%B4%A2%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA-(2)/" /><meta property="og:url" content="https://zoechen119.github.io/posts/RAG%E6%A3%80%E7%B4%A2%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA-(2)/" /><meta property="og:site_name" content="zoe Chen" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-12-04T00:00:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="RAG" /><meta name="twitter:site" content="@twitter_username" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-12-04T00:00:00+08:00","datePublished":"2023-12-04T00:00:00+08:00","description":"RAG调研","headline":"RAG","mainEntityOfPage":{"@type":"WebPage","@id":"https://zoechen119.github.io/posts/RAG%E6%A3%80%E7%B4%A2%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA-(2)/"},"url":"https://zoechen119.github.io/posts/RAG%E6%A3%80%E7%B4%A2%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA-(2)/"}</script><title>RAG | zoe Chen</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="zoe Chen"><meta name="application-name" content="zoe Chen"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/img/avatar.jfif" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">zoe Chen</a></div><div class="site-subtitle font-italic">nlper, dler, sims4er</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>首页</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>分类</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>标签</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>归档</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>关于</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/zoeChen119" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['zoe9698','163.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> 首页 </a> </span> <span>RAG</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> 文章</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="搜索..."> </span> <span id="search-cancel" >取消</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>RAG</h1><div class="post-meta text-muted"> <span> 发表于 <em class="" data-ts="1701619200" data-df="YYYY-MM-DD" data-toggle="tooltip" data-placement="bottom"> 2023-12-04 </em> </span><div class="d-flex justify-content-between"> <span> 作者 <em> <a href="https://github.com/zoeChen119">陈政伊</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="16790 字"> <em>93 分钟</em>阅读</span></div></div></div><div class="post-content"><h1 id="rag调研">RAG调研</h1><h2 id="一一些共识"><span class="mr-2">一、一些共识</span><a href="#一一些共识" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><blockquote><p>External knowledge is the key to resolving the problems of LLMs such as hallucination and outdated knowledge, which can make LLMs generate more accurate and reliable responses through retrieval-augmented generation (RAG). However, LLMs cannot always response as expected with RAG. For one thing, there are numerous irrelevant documents and false information on the Internet. Incorporating these external documents into LLMs could have a detrimental effect. For anthoer, LLMs suffer from the unreliable generation challenge. The generation of LLMs is often unpredictable, and we cannot guarantee that they will utilize the useful information entailed in the external documents. Additionally, LLMs can easily be misled by incorrect information in the document. To this end, we build RetrievalAugmented Generation Benchmark (RGB) to evaluate the retrieval-augmented generation of LLMs, and we concern about 4 specific abilities:[^Benchmarking Large Language Models in Retrieval-Augmented Generation]</p></blockquote><p>外部知识是解决幻觉和过时知识等llm问题的关键，它可以通过检索增强生成(RAG)使llm产生更准确、更可靠的响应。然而，</p><p>⭐<strong>LLM 并不总是像 RAG 预期的那样响应</strong>。</p><p>⭐<strong>将这些不相关的或者虚假的外部文档合并到 LLM 中可能会产生不利影响。</strong></p><p>⭐<strong>llm的生成通常是不可预测的，我们不能保证它们将利用外部文档中包含的有用信息。</strong></p><p>⭐<strong>LLM 很容易被文档中不正确的信息误导。</strong></p><blockquote><p>In real-world scenarios, it is not possible to obtain perfect documents with all the necessary external knowledge. Therefore, evaluating these four abilities of the model becomes essential in order to measure the RAG of LLMs.[^Benchmarking Large Language Models in Retrieval-Augmented Generation]</p></blockquote><p>⭐<strong>在现实世界的场景中，不可能获得具有所有必要的外部知识的完美文档。</strong>因此需要能够评估模型的这四个能力。</p><h2 id="二论文调研"><span class="mr-2">二、论文调研</span><a href="#二论文调研" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="1-benchmarking-large-language-models-in-retrieval-augmented-generationbenchmarking-large-language-models-in-retrieval-augmented-generation"><span class="mr-2">1. Benchmarking Large Language Models in Retrieval-Augmented Generation[^Benchmarking Large Language Models in Retrieval-Augmented Generation]</span><a href="#1-benchmarking-large-language-models-in-retrieval-augmented-generationbenchmarking-large-language-models-in-retrieval-augmented-generation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p align="right">---中科院计算所，23年9月4日</p><h4 id="摘要"><span class="mr-2">摘要：</span><a href="#摘要" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>检索增强生成 (RAG) 是一种<strong>减轻大型语言模型 (LLM) 幻觉</strong>的有前途的方法。然而，现有的研究缺乏对检索增强生成对不同大型语言模型的影响的严格评估，这使得识别RAG对不同llm能力的潜在瓶颈具有挑战性。在本文中，我们<strong>系统地调查了 Retrieval-Augmented Generation 对大型语言模型的影响</strong>。我们<strong>分析了不同大型语言模型在RAG所需的4个基本能力下的性能，包括噪声鲁棒性、负拒绝、信息集成和反事实鲁棒性</strong>。为此，我们<strong>建立了 Retrieval-Augmented Generation Benchmark (RGB)，这是一个用于英文和中文 RAG 评估的新语料库</strong>。RGB 根据解决案例所需的上述基本能力，将基准内的实例划分为 4 个单独的测试平台。然后我们在 RGB 上评估 6 个具有代表性的 LLM，以诊断应用 RAG 时当前 LLM 的挑战。<u>评估表明，虽然 LLM 表现出一定程度的噪声鲁棒性，但它们在负拒绝、信息集成和处理虚假信息方面仍然会遇到重大困难</u>。上述评估结果表明，在有效地将RAG应用于LLM之前，仍有相当大的旅程。</p><h4 id="introduction"><span class="mr-2">Introduction：</span><a href="#introduction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>🌱为什么要研究RAG模型？</p><blockquote><p>Recently, there have been impressive advancements in large language models (LLMs) like ChatGPT (OpenAI 2022), LLaMA-2 (Touvron et al. 2023), and ChatGLM (THUDM 2023a). Although these models have shown remarkable general abilities (Bang et al. 2023; Guo et al. 2023), they still suffer severely from challenges including factual hallucination (Cao et al. 2020; Raunak, Menezes, and JunczysDowmunt 2021; Ji et al. 2023), knowledge out-dating (He, Zhang, and Roth 2022), and the lack of domain-specific expertise (Li et al. 2023c; Shen et al. 2023).</p></blockquote><p>ChatGPT、LLaMA-2、ChatGLM等大模型虽然有优秀的通用能力，但是存在一些问题，①factual hallucination事实幻觉；②knowledge out-dating知识过时；③domain-specific expertise特定领域的专业知识。</p><p><img data-src="..\..\..\..\..\zoeChen119.github.io\assets\img023-12-04-RAG检索知识增强\RAG的研究动机-2023-12-04-1505.png" alt="RAG的研究动机-2023-12-04-1505" data-proofer-ignore></p><blockquote><p>With the help of external knowledge, LLMs can generate more accurate and reliable responses. The most common method is to use a search engine as a retriever such as New Bing.</p></blockquote><p>LLMs+RAG的一个典型应用就是New Bing。</p><blockquote><p>On the other hand, LLMs suffer from unreliable generation challenge. LLMs can be misled by incorrect information contained in the context (Bian et al. 2023) and also suffer from hallucination during the generation (Adlakha et al. 2023), resulting in generating content that goes beyond external information.</p></blockquote><p>RAG也会给LLMs带来负面影响，比如①互联网中存在虚假信息，②LLM可能会被上下文中包含的错误信息误导，③LLM在生成过程中存在幻觉，会生成超出外部信息的内容。</p><blockquote><p>Unfortunately, currently there lacks of comprehensive understanding on how these factors can influence RAG, and how could each model survives from these drawbacks and improvement their performance via information retrieval.</p></blockquote><p><strong>不幸的是，目前对这些因素如何影响RAG，以及每个模型如何从这些缺陷中幸存下来并通过信息检索提高其性能缺乏全面的了解。</strong>因此，迫切需要对LLM进行全面评估，评估其有效利用检索到的信息的能力，以及抵御信息检索中存在的各种缺点的能力。</p><p>💡为了确保LLM的内部知识不会在评估结果中引入偏差，RGB选择聚合<strong>最新的新闻信息</strong>，并基于新闻信息构建查询。然后，基于这些查询，我们使用搜索API获取相关文档，并从内容中<strong>选择最相关的片段作为外部检索文档</strong>。最后，基于查询和文档集对的不同组成，我们<strong>扩展语料库</strong>，并将其<strong>划分为4个测试平台</strong>，根据RAG中常见的挑战来评估LLM的以下基本能力，如图1所示：</p><p><img data-src="..\..\..\..\..\zoeChen119.github.io\assets\img023-12-04-RAG检索知识增强ccbadec7b7dfc8af05d8871f80c3983_0_Figure_1_645056946.png" alt="img" data-proofer-ignore></p><ol><li><p>噪声鲁棒性Noise Robustness：“从嘈杂文档中提取有用信息”的能力。</p><p>本文中，我们将【嘈杂的文档】定义为“与问题相关的文档，但不包含任何答案信息。”</p><p>噪声鲁棒性平台：</p><p>​ 所有相关外部文档=噪声文档+包含答案信息的文档</p><p>​ 噪声文档=噪声比*所有相关外部文档</p><li><p>负拒绝Negative Rejection：“拒绝回答无答案查询”的能力</p><p>无答案查询：所需知识不存在于任何检索到的文档中。此情况，LLM应给出“信息不足”或其他拒绝信号。</p><p>负拒绝平台：</p><p>​ 外部文档=噪声文档</p><li><p>信息整合Information Integration：“回答关联多个文档的复杂问题”的能力</p><p>信息整合平台：</p><p>​ 查询=只能用多个文档才能回答的实例</p><p>​ 外部文档=多个包含答案信息的文档+噪声文档</p><li><p>反事实鲁棒性Counterfactual Robustness：“通过指令提示LLMs’警告：检索到的信息存在潜在风险‘时，能够识别检索到的文档中的已知事实错误”的能力</p><p>反事实稳健性平台：</p><p>​ LLM已知的知识，即可以直接回答的query。</p><p>​ 外部文档=存在事实错误的文档</p><p>请注意，我们只评估 LLM 通过指令对检索到的信息中潜在风险的警告的情况。</p></ol><p>测评模型：</p><ol><li>ChatGPT<li>ChatGLM-6B<li>ChatGLM2-6B<li>Vicuna-7b<li>Qwen-7B-Chat<li>BELLE-7B</ol><p>结论：</p><blockquote><p>We found that even though RAG can improve the response accuracy of LLMs, they still suffer from the abovementioned challenges significantly. Specifically, we found that even though LLMs demonstrate some level of noise robustness, they tend to confuse similar information and frequently generate inaccurate answers when relevant information exists.</p></blockquote><p>尽管RAG可以提高llm的响应精度，但它们仍然受到上述挑战的显著影响。具体来说，</p><ol><li>我们发现，尽管 LLM 展示了一定程度的噪声鲁棒性，但当相关信息存在时，它们往往会混淆相似的信息并经常生成不准确的答案。</ol><p>例如，当面对有关 2022 年诺贝尔文学奖的问题时，如果有关外部文件文献中 2021 年诺贝尔奖的嘈杂文档，LLM 可能会混淆并提供不准确的答案。</p><ol><li><p>此外，当没有一个外部文档包含相关信息时，LLM 经常无法拒绝回答并生成不正确的答案。</p><li>此外，LLM 缺乏从多个文档中总结的能力，因此如果需要多个文档来回答问题，LLM 通常无法提供准确的答案。<li>最后，我们发现，即使llm包含所需的知识，并通过指令对检索到的信息中潜在风险的警告，它们仍然倾向于信任和优先考虑检索到的信息而不是他们自己的现有知识。</ol><h4 id="retrival-augmented-generation-benchmark"><span class="mr-2">Retrival-Augmented Generation Benchmark</span><a href="#retrival-augmented-generation-benchmark" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>数据集构建过程：</p><p><img data-src="..\..\..\..\..\zoeChen119.github.io\assets\img023-12-04-RAG检索知识增强ccbadec7b7dfc8af05d8871f80c3983_2_Figure_2_-1246759510.png" alt="img" data-proofer-ignore></p><p>News Collection–真实的</p><p>Step 0：收集最新的新闻文章</p><p>Step 1：QA实例生成（QA instances generation）：使用prompt让ChatGPT为每篇文章生成<strong>事件（Related event）、问题（Question）和答案（Key information）</strong>。这一步能够顺便初步过滤出不包含任何事件的新闻文章。</p><p>Step 2：人工检查调整、过滤<strong>难以通过搜索引擎检索的数据</strong>。</p><p>Step 3：使用搜索引擎API检索（Retrieve using search engine）：对于每个Query使用谷歌 API获取10个相关网页，并提取相应的文本片段。</p><p>Step 4：阅读这些网页，并将其文本内容转换为最大长度为300个token的文本块。使用现有的密集检索模型选择与查询最有效匹配的前30个文本块。这些检索到的文本块，以及搜索API提供的片段，将用作我们的<strong>外部文档</strong>。</p><p>Step 5：扩展语料库，为4种能力构建Testbeds（试验台）。</p><p>​ 对于噪声鲁棒性，根据所需的噪声比率对不同数量的负面文档进行采样。</p><p>​ 对于负样本拒绝，所有的外部分档都是从负文档中采样的。</p><p>​ 对于信息整合，基于前面生成的问题构造复杂问题。这涉及到扩展或重写这些问题，使它们的答案包含多个方面。例如，“谁获得了2023年超级碗的MVP？”这个问题可以改写为“谁赢得了2022年和2023年的超级碗MVP。因此，回答这样的问题需要利用来自各种文件的信息。</p><p>​ 对于反事实稳健性，反事实稳健性数据完全基于模型的内部知识构建，也就是说让模型自动生成已知的问题和答案，例如，基于“谁获得了 2022 年诺贝尔生理学和医学奖？”的问题，该模型将生成已知问题“谁获得了 2021 年诺贝尔文学奖？”并回答“Abdulrazak Gurnah”。然后人工验证生成的答案，并检索相关文档，为了使文档包含事实错误，我们人工修改答案并替换文档中的相应部分。</p><blockquote><p>密集检索模型:</p><p>for English：https://huggingface.co/sentence-transformers/all-mpnet-basev2</p><p>for Chinese：https://huggingface.co/moka-ai/m3e-base</p></blockquote><p>RGB统计数据：</p><p>共600个基本问题，200个信息整合能力的附加问题，200个反事实稳健性能力的附加问题。</p><p>所有问题，一半中文，一半英文。</p><p>评价指标：</p><p><strong>该基准的核心是评估 LLM 是否可以利用提供的外部文档来获取知识并生成合理的答案。</strong></p><ul><li><p>Accuracy：for 噪声鲁棒性、信息整合</p><p>​ 精确匹配-如果生成的文本包含与答案的精确匹配，则视为回答正确。</p><li><p>Rejection rate：for 负样本拒绝</p><p>​ 当只提供嘈杂的文档时，LLM应输出具体内容—— “I can not answer the question because of the insufficient information in documents.”（“由于文档中的信息不足，我无法回答问题。”）如果模型生成此内容，则表示成功拒绝。</p><p>​ PS.使用说明提示模型。</p><li><p>Error detection rate：for 反事实鲁棒性（衡量模型是否能检测出文档中的事实错误）</p><p>​ 当提供的文档包含事实错误时，模型应该输出特定的内容-“There are factual errors in the provided documents.”（“提供的文档中存在事实错误”。）如果模型生成该内容，则表明模型在文档中检测到错误信息。</p><p>​ PS.使用说明提示模型。</p><li><p>Error correction rate：for 反事实鲁棒性（衡量模型是否能在识别到事实错误之后仍能提供正确的答案）</p><p>​ 如果该模型生成正确的答案，则表明模型能够 修正 文档中的事实错误。</p>\[ACC=\frac{\#tt}{\#nums}\]<p>#tt：正确的response数量</p><p>#nums：所有待评估的❓实例数量</p></ul><p>考虑到模型可能无法完全遵守指令，对于拒绝率和错误检测率，我们还使用ChatGPT对答案进行额外评估。具体而言，❓我们通过使用指导（instructions）和演示（demonstrations）来评估模型的响应，以确定它们是否能够反映文件中没有的信息或识别任何事实错误。</p><h4 id="experiments"><span class="mr-2">Experiments</span><a href="#experiments" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><h5 id="settings"><span class="mr-2">Settings</span><a href="#settings" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>由于上下文限制，我们为每个问题提供<strong>5个外部文档</strong>。在我们关于噪声鲁棒性的实验中，我们评估了噪声比在0到0.8之间的场景。为了全面评估整体能力，我们<strong>对每种语言都采用了统一的指导</strong>（instructions），如图3所示。实验是使用NVIDIA GeForce RTX 3090进行的。</p><p><img data-src="..\..\..\..\..\zoeChen119.github.io\assets\img023-12-04-RAG检索知识增强ccbadec7b7dfc8af05d8871f80c3983_3_Figure_3_536383974.png" alt="img" data-proofer-ignore></p><p>include a system instruction followed by a user input instruction</p><h5 id="results-on-noise-robustness"><span class="mr-2">Results on Noise Robustness</span><a href="#results-on-noise-robustness" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>针对 噪声鲁棒性 实验：</p><p><img data-src="..\..\..\..\..\zoeChen119.github.io\assets\img023-12-04-RAG检索知识增强ccbadec7b7dfc8af05d8871f80c3983_4_Table_1_1366057918.png" alt="img" data-proofer-ignore></p><p><strong>结论</strong>：</p><ol><li><p>RAG能有效改善LLMs的响应。</p><p>即使在存在噪声的情况下，llm也表现出了很强的性能，这表明RAG是llm产生准确可靠响应的一种有希望的方法。</p><li><p>噪声率的不断提高对llm中的RAG提出了挑战。</p><p>具体来说，当噪声比超过80%时，模型的精度明显下降。例如，ChatGPT的性能从96.33%下降到76.00%，而ChatGLM2-6B的性能从91.33%下降到57.33%。</p></ol><p>深入分析这个实验中的模型答错的答案，发现错误通常源于<strong>3个原因</strong>：</p><p><img data-src="..\..\..\..\..\zoeChen119.github.io\assets\img023-12-04-RAG检索知识增强ccbadec7b7dfc8af05d8871f80c3983_4_Table_2_1780650320.png" alt="img" data-proofer-ignore></p><p>给出了噪声鲁棒性的误差案例，并且只给出了一个正面文档和一个负面文档。响应由ChatGLM2-6B生成。蓝色文本表示文档与问题或答案之间匹配的部分，而红色文本突出显示不匹配的部分。</p><ol><li><p>长距离信息</p><p>当外部文档中<strong>“与问题相关的信息”与“与答案相关的信息”相距甚远时</strong>，模型难以识别正确答案。这种情况中，大部分是“与问题相关的信息”首先出现在文档的开头，在下文中使用代词指代它。这种情况出现时，<strong>模型会去依赖其他文档的信息</strong>，产生错误的印象（幻觉）。</p><p>如上图中卡塔尔公开赛只在开头出现了一次，与答案文本Anett Kontaveit人名相距甚远。</p><li><p>证据的不确定性</p><p>在备受关注的事件发生前，人们倾向于<strong>预测、猜测、预言</strong>它。尽管这样的预测文档明确指出这是不确定或推测性的内容，但它们仍然会影响LLM的检索增强生成。</p><p>如上图中，错误文档中的内容都是关于苹果新耳机的预测（Apple Reality Pro），并且存在有正确答案的文档（Vision Pro），模型仍然被<strong>误导</strong>了。</p><li><p>概念混淆</p><p>外部文档中的概念可能与问题中的<strong>概念相似</strong>，但又不同。这会让LLM<strong>混淆</strong>，并生成不正确的答案。</p><p>如上图中，模型对问题的理解集中在”汽车收入“的概念上（特斯拉，收入-&gt;通常提到特斯拉都是谈论特斯拉汽车-&gt;特斯拉，汽车收入），但问题问的是特斯拉这个公司一季度全部“收入”。</p></ol><p>通过上述分析，LLM需要进一步详细的增强，比如长文档建模、精确的概念理解</p><h5 id="results-on-negative-rejection-testbed"><span class="mr-2">Results on Negative Rejection testbed</span><a href="#results-on-negative-rejection-testbed" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>针对 负拒绝 实验：（只提供负样本，看模型的拒绝率）</p><p>采用<strong>精确匹配</strong>评估拒绝率（Rej），利用<strong>ChatGPT</strong>来确定LLM的response是否包含拒绝信息（Rej*）。</p><p><img data-src="..\..\..\..\..\zoeChen119.github.io\assets\img023-12-04-RAG检索知识增强ccbadec7b7dfc8af05d8871f80c3983_5_Table_3_1815174249.png" alt="img" data-proofer-ignore></p><p><strong>结论</strong>：</p><ol><li><p>负拒绝对LLM中的RAG提出了挑战。</p><p>中文/英文LLM的最高拒绝率最高43.33%/45%。</p><li><p>通过比较Rej和Rej*，发现LLM不能严格遵循指令，且经常产生不可预测的response，因此LLM很难直接用来识别负样本并判定拒绝。</p></ol><p><strong>2个原因</strong>：</p><p><img data-src="..\..\..\..\..\zoeChen119.github.io\assets\img023-12-04-RAG检索知识增强ccbadec7b7dfc8af05d8871f80c3983_5_Table_4_-1001365273.png" alt="img" data-proofer-ignore></p><ol><li><p>证据不确定性</p><p>虽然文件中只提到与 “亚当-麦凯 “的联系，并没有明确指出他是这部电影的导演，但模型仍然断定他担任了这一角色。</p><li><p>概念混淆</p><p>答案中提供的信息与问题中提到的“2022 年冬季奥运会”而不是“2022 年奥运会”。与直接回答相比，检索增强生成对负拒绝提出了更大的挑战，因为它提供了可能误导 LLM 并导致错误响应的相关文档。</p></ol><h5 id="results-on-information-integration-testbed"><span class="mr-2">Results on Information Integration testbed</span><a href="#results-on-information-integration-testbed" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>针对 信息整合 实验：不同噪声率下，评估模型准确率。</p><p><img data-src="..\..\..\..\..\zoeChen119.github.io\assets\img023-12-04-RAG检索知识增强ccbadec7b7dfc8af05d8871f80c3983_6_Table_5_-1246759510.png" alt="img" data-proofer-ignore></p><p><strong>结论</strong>：</p><ol><li><p>信息整合 对于LLMs中的RAG来说是一个大挑战（<strong>LLM难以有效地整合信息，不太适合直接回答复杂的问题。</strong>）</p><p>即使噪声率=0，英文/中文中最高也只有60%/67%的准确率</p><p>添加噪声文档后，英文/中文下降到43%/55%</p><li><p>对于文档嘈杂的RAG来说，<strong>复杂的问题</strong>更具挑战性。</p><p>对比 噪声鲁棒性 实验（简单问题，添加噪声文档），如下图。简单问题时，噪声比达到0.8时，性能才显著下降，而复杂问题时，噪声比=0.4时，性能猛降。</p><p><img data-src="..\..\..\..\..\zoeChen119.github.io\assets\img023-12-04-RAG检索知识增强ccbadec7b7dfc8af05d8871f80c3983_4_Table_1_1366057918.png" alt="img" data-proofer-ignore></p><p>这表明复杂的问题更容易受到噪声的干扰。我们推测，这是因为解决复杂问题需要整合来自多个文档的信息，而这些信息可以被视为彼此的噪声，使模型更难从文档中提取相关信息。</p></ol><p><strong>4个原因</strong>：（这里分析的是噪声率-0的ChatGLM2-6B的错误数据）</p><p><img data-src="..\..\..\..\..\zoeChen119.github.io\assets\img023-12-04-RAG检索知识增强ccbadec7b7dfc8af05d8871f80c3983_6_Table_6_-1649557801.png" alt="img" data-proofer-ignore></p><ol><li><p>38%的错误实例属于 噪声稳健性 实验中发现的3个错误原因</p><li><p>合并错误（占比28%）：</p><p>模型试图合并两个子问题的答案，错误地使用一个问题的答案直接回答两个子问题，然后忽略另一个子问题相关的所有文档。</p><p>例如：模型回答D 组是法国队和德国队的世界杯小组，而实际上德国队被分到了 E 组。</p><li><p>忽视错误（28%）</p><p>模型只回答一个问题，直接忽略另一个问题。原因是模型对问题缺乏全面的理解，没有认识到问题由多个子问题组成。</p><p>例如：模型只提供了 2022 年超级碗 MVP 的答案，而没有考虑 2023 年。</p><li><p>错位误差（6%）</p><p>模型把子问题1的文档误识别为子问题2的问答，导致答案错位。</p><p>例如：模型仅提到了 2023（95）学院奖的最佳图片，完全忽略了 2022 年奖项。此外，它错误地指出，“CODA”是 2023 的最佳图片，当它实际上被授予 2022 年的最佳图片时。</p></ol><p>上述错误主要是由于对复杂问题的理解有限，这阻碍了有效利用来自不同子问题的信息的能力。<strong>关键是提高模型的推理能力。</strong>一种可能的解决方案是使用<strong>思维链方法来分解复杂问题</strong>（Zhou et al. 2023a; Xu et al. 2023b; Drozdov et al. 2023）。然而，这些方法<strong>会减慢推理速度</strong>，无法提供及时的响应。</p><h5 id="results-on-counterfactual-robustness-testbed"><span class="mr-2">Results on Counterfactual Robustness testbed</span><a href="#results-on-counterfactual-robustness-testbed" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>针对 反事实鲁棒性 实验：</p><p>（考察模型对于已知事实知识的问题，包含事实错误的噪声文档能否干扰他的思考。因此只考虑准确率超过 70% 的 LLM。）</p><p>4个指标：①不包含任何文档的准确率、②包含反事实文档的准确率、③错误识别率、④错误纠正率</p><p><img data-src="..\..\..\..\..\zoeChen119.github.io\assets\img023-12-04-RAG检索知识增强ccbadec7b7dfc8af05d8871f80c3983_6_Table_7_2078354437.png" alt="img" data-proofer-ignore></p><p>采用<strong>精确匹配</strong>评估错误识别率（ED），利用<strong>ChatGPT</strong>来识别LLM的response是否包含反事实错误信息（ED*）。</p><ol><li>ChatGPT-en在错误纠正率（CR）方面表现最好，达到了57.14%，远高于其他两种模型。<li>所有的模型在有反事实文档时的准确率（ACCdoc）都比没有外部文档时的准确率（Acc）下降了很多，说明反事实文档对LLMs的生成能力有很大的干扰。<li>Qwen-7B-Chat-zh在错误检测率（ED）方面表现优于ChatGPT-zh，但在错误纠正率（CR）方面不如ChatGPT-zh，说明它能够发现错误，但不能有效地修正错误。</ol><p>原因分析：</p><ol><li>LLMs过于依赖检索到的文档，而不是利用自身的知识。这导致LLMs容易被含有错误事实的文档所误导，从而产生不准确的回答。</ol><blockquote><p>检索增强生成不是为自动解决给定上下文中的事实错误而设计的，因为这与模型缺乏知识并且依赖于检索到的文档以获取附加信息的基本假设相矛盾。</p><ul><li>检索增强生成（RAG）是一种让大型语言模型（LLMs）在生成内容时，可以从外部知识源获取额外信息的技术。<li>RAG的一个基本假设是，LLMs缺乏知识，需要依靠检索到的文档来补充信息。也就是说，RAG认为检索到的文档是可靠的，可以帮助LLMs生成更准确的答案。<li>但是，如果检索到的文档中包含了错误的事实，也就是与真实情况相悖的信息，那么RAG就会出现问题。因为RAG没有设计自动纠正文档中的错误事实的能力，它会盲目地信任检索到的文档，从而生成错误的答案。<li>这就是为什么RAG并不适合解决反事实鲁棒性的问题。反事实鲁棒性是指LLMs在处理与事实相悖的外部文档时，能否正确地检测和纠正错误的能力。这是一种重要的信息整合能力，对于实际应用中的可靠性和安全性至关重要。</ul><p>然而，由于互联网上假新闻泛滥，这个问题在实际应用中至关重要。现有的 LLM 不具备处理因错误信息造成的不准确回复的保障措施。事实上，它们在很大程度上依赖于检索到的信息。<strong>即使 LLM 包含有关问题的内部知识，它们也经常相信检索到的虚假信息</strong>。这对未来在 LLMs 中发展 RAG 提出了重大挑战。</p></blockquote><p>&lt;font color=#008B8B&gt;</p><h4 id="个人总结"><span class="mr-2">🤹个人总结</span><a href="#个人总结" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p><img data-src="..\..\..\..\..\zoeChen119.github.io\assets\img023-12-04-RAG检索知识增强\image-20231206113404826.png" alt="image-20231206113404826" data-proofer-ignore></p><p>&lt;/font&gt;</p><h3 id="2-active-retrieval-augmented-generationactive-retrieval-augmented-generation"><span class="mr-2">2. Active Retrieval Augmented Generation[^Active Retrieval Augmented Generation]</span><a href="#2-active-retrieval-augmented-generationactive-retrieval-augmented-generation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p align="right">---EMNLP 2023，卡耐基梅隆大学，23年8月</p><h4 id="摘要-1"><span class="mr-2">摘要：</span><a href="#摘要-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><blockquote><p>Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution.</p></blockquote><p>尽管大型语言模型 (LM) 具有理解和生成语言的显着能力，但它们倾向于产生幻觉并创建事实不准确的输出。通过从外部知识资源中检索信息来增强 LM 是一种很有前途的解决方案。</p><blockquote><p>Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential.</p></blockquote><p><strong>大多数现有的检索增强LMs采用检索-生成设置，仅根据输入检索一次信息。然而，在涉及生成长文本的更一般的场景中，这是有局限的，在生成过程中不断收集信息是必不可少的。</strong></p><blockquote><p>In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We proposeForward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.</p></blockquote><p>在这项工作中，我们提供了主动检索增强生成的广义视图，<strong>这些方法在生成过程中主动决定何时检索和检索什么</strong>。我们提出了前瞻性主动检索增强生成(FLARE)，这是一种通用方法，它迭代地使用对即将到来的句子的预测来预测未来的内容，然后将其用作检索相关文档的查询，以便在包含低置信度令牌的情况下重新生成句子。实验是在4个长篇知识密集型生成任务上验证的效果。</p><h4 id="introduction-1"><span class="mr-2">Introduction：</span><a href="#introduction-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20231211143315512.png" alt="image-20231211143315512" data-proofer-ignore></p><p><strong>LLM→幻觉</strong></p><blockquote><p>Increasingly powerful large LMs have also demonstrated abilities in more complex tasks that involve generating long-form output</p></blockquote><p>LLM在更复杂的任务中保有优秀的产生长序输出的能力。这些复杂任务（<strong>long-form输出任务</strong>）有：long-form QA，open-domain summarization，chain-of-thought（CoT）reasoning。</p><p><strong>RAG+LM/LLM：检索+生成 → 文本长度受限/单次检索/被动注入</strong></p><blockquote><p>These single-time retrieval augmented LMs outperform purely parametric LMs, particularly for short-form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the information needs are clear in the user’s input, and it is sufficient to retrieve relevant knowledge once solely based on the input.</p></blockquote><p>对于short-form的知识密集型生成任务，比如事实性问答（factoid question answering）任务，这类任务是<strong>单次</strong>的，这种情况下，信息需求在用户的输入时就明确了，所以仅基于输入执行单次检索（相关知识）就可以支撑。</p><p><strong>short-form &amp; long-form</strong></p><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20231211161906646.png" alt="image-20231211161906646" data-proofer-ignore></p><blockquote><p>In contrast to short-form generation, long-form generation presents complex information needs that are not always evident from the input alone. Similar to how humans gradually gather information as we create content such as papers, essays, or books, long-form generation with LMs would require gathering multiple pieces of knowledge throughout the generation process.</p></blockquote><p>与short-form生成相比，long-form生成需要复杂的信息，<strong>而这些信息并不总是仅从输入中就能看出来</strong>。<em>与人类在创作论文、散文或书籍等内容时逐步收集信息的方式类似，使用 LM 的long-form生成需要在整个生成过程中收集多种知识</em>。</p><p><strong>FLARE的idea</strong></p><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20231211154021414.png" alt="image-20231211154021414" data-proofer-ignore></p><p>例如：要生成某个特定主题的摘要，基于主题名称的初始检索可能无法涵盖所有方面和细节。在生成过程中，如生成某个方面（如乔·拜登的教育史）或某个特定细节（例如乔·拜登的总统竞选公告日期）时，根据需要检索额外信息至关重要。</p><p>具体来说，从用户输入 x 和初始检索结果 Dx 开始，FLARE 会反复生成一个临时的下一个句子（以灰色斜体显示），并检查其中是否包含低概率标记（以下划线表示）。如果是（步骤 2 和 3），系统将检索相关文档并重新生成句子。</p><p><strong>除了FLARE，还有哪些尝试？</strong></p><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20231211160927015.png" alt="image-20231211160927015" data-proofer-ignore></p><p><strong>FLARE的关键思路</strong></p><p>对于这些尝试，作者提出以下这个问题：</p><ul><li>能否创建一个简单通用的检索增强LM，在整个生成过程中主动决定<strong>何时检索</strong>以及<strong>检索什么</strong>，并适用于各种长形式的生成任务？</ul><p><strong>问题1：何时检索</strong></p><p><strong>假设</strong>-只有在缺乏所需知识的情况下才应该检索信息，以避免被动检索增强LMs中发生的不必要或不适当的检索</p><blockquote><p>本文鉴于LLM通常具有良好的校准性，因此只有当出现低概率（低置信度）的时候表明LM缺乏知识，因此采用一种主动检索策略，该策略仅在LM生成低概率token时检索。</p></blockquote><p><strong>问题2：检索什么</strong></p><p><strong>假设</strong>-在考虑检索什么的时候，重要的是考虑LMs未来打算生成什么，因为主动检索的目标是优化未来的生成。</p><blockquote><p>本文提出通过生成临时下一句来预测未来，将其用作检索相关文档的查询，然后根据检索到的文档重新生成下一句。</p></blockquote><p><strong>FLARE的性能：</strong></p><p>&lt;font color=#008B8B&gt; FLARE无需训练：FLARE is applicable to any existing LMs at inference time without additional training.&lt;/font&gt;</p><blockquote><p>考虑到GPT-3.5（Ouyang et al.，2022）在各种任务上取得的令人印象深刻的性能，我们在text-davinci-003上检验了我们的方法的有效性。</p></blockquote><p>模型：text-davinci-003</p><p>方法：FLARE</p><p>任务：</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre>* 多跳QA（2WikiMultihopQA）
* 常识推理（StrategyQA）
* long form QA（ASQA）
* 开放域摘要（WikiAsp）
</pre></table></code></div></div><p>结果显示：</p><p>单次检索 ❌ → 多次检索 ❌ → FLARE ✅</p><h4 id="rag的数学定义"><span class="mr-2">RAG的数学定义</span><a href="#rag的数学定义" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><ol><li><p>正式定义单次检索增强生成</p><li><p>主动检索增强生成的定义</p></ol><h4 id="详解flare"><span class="mr-2">详解FLARE</span><a href="#详解flare" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>本研究的动机：</p><ol><li>LM应该<strong>只在必要的时候检索信息</strong>，避免不必要的或不适当的检索。<li>检索query应该能够反映后续生成的意图，即预测下一句话的内容，根据其检索相关文档。</ol><p>本文提出了2种前瞻性的主动检索增强生成（FLARE）方法：</p><ol><li>FLAREinstruct：在生成答案的时候，使用一些鼓励检索的指令，提示LM在必要时生成检索查询。<li>FLAREdirect：直接使用LM的生成作为检索查询，迭代地生成下一句话，如果出现不确定的词，就检索相关文档，重新生成下一句话。</ol><h5 id="方法1flareinstructflare-with-retrieval-instructions"><span class="mr-2">方法1：（FLAREinstruct）FLARE with Retrieval Instructions</span><a href="#方法1flareinstructflare-with-retrieval-instructions" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>我们受Toolformer的启发，FLAREinstruct是一种直接表达检索信息需求的方法。</p><blockquote><p>Toolformer：一种让语言模型（LMs）在生成文本的过程中，根据需要从外部知识资源中检索相关信息的方法。</p></blockquote><p>具体来说，在需要额外信息时，向query中添加”[Search(query)]”</p><p>e.g.</p><p>“The colors on the flag of Ghana have the following meanings. Red is for <strong>[Search(Ghana flag red meaning)]</strong> the blood of martyrs, …”</p><p>当使用只提供API接口的模型（比如GPT3.5和GPT4）时，通过few-shot prompting结合这种toolformer的方法诱导模型检索必要的信息以生成回答。</p><p>而FLAREinstruct：</p><p>具体来说，对于下游任务，我们将与检索相关的指令和示例作为Skill 1放在开头，然后将下游任务的指令和示例作为Skill 2。当给出一个sample时，要求LM在执行任务时结合技能1和2生成检索query。提示的结构如下图：</p><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20231222113000287.png" alt="image-20231222113000287" data-proofer-ignore></p><p>FLAREinstruct的流程如下图：</p><p><img data-src="https://pdf.cdn.readpaper.com/parsed/fetch_target/e56e7ef7642576be3129961115b04a63_3_Figure_2_-190988821.png" alt="img" data-proofer-ignore></p><p>如图2所示，当LM生成“[Search(query)]”(以灰色斜体显示)时，我们停止生成并使用查询条件来检索相关文档，这些文档在用户输入之前添加，以帮助将来生成，直到生成下一个搜索查询或到达结束。附加的实现细节包含在附录A中。</p><h5 id="方法2flaredirectdirect-flare"><span class="mr-2">方法2：（FLAREdirect）Direct FLARE</span><a href="#方法2flaredirectdirect-flare" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><blockquote><p>Since we cannot fine-tune black-box LMs, we found queries generated by FLAREinstruct through retrieval instructions might not be reliable.</p></blockquote><p>由于不能对黑盒LM进行微调，所以用FLAREinstruct方法生成的检索查询可能不太可靠，也就是说，可能不能很好地反映用户的意图，或者不能找到最合适的文档。</p><p>这就引出了FLAREdirect，作者概括这个方法是一种更直接的前瞻性主动检索方法，该方法可以使用the next sentence来决定何时以及如何检索。</p><p><strong>3.2.1 基于置信度的主动检索</strong></p><ol><li><p>每个step中（假设step t），首先生成1个临时的next sentence ，这个句子不考虑外部文档。</p><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20231225160746595.png" alt="image-20231225160746595" data-proofer-ignore></p><li><p>根据这个$\hat{s}_t$决定是否检索、生成query $q_t$</p><li><p>如果LM对于$\hat{s}_t$很自信，那么不在额外检索。</p><p>否则，用$q_t$检索出相关文档，再重新生成the next sentence $s_t$</p></ol><blockquote><p>也可以用段落作为迭代的basis，作者选句子，是因为它不长不短蛮合适。</p></blockquote><blockquote><p>Since LMs tend to be well-calibrated that low probability/confidence often indicates a lack of knowledge (Jiang et al., 2021; Kadavath et al., 2022; Varshney et al., 2022)</p></blockquote><p>多篇论文都发现，经过<strong>无监督预训练</strong>的大型LM，比如GPT-3，通常是比较好的校准，也就是说，它们的置信度和概率是比较一致的。</p><hr /><p><strong>* 什么是LM的置信度校准？</strong></p><p>指LM的置信度（confidence）能够反映出它的预测正确的概率（probability）。比如，如果LM对一个答案的置信度是80%，那么它的预测正确的概率也应该接近80%。</p><p>如果LM的置信度和概率不一致，那么就说明LM是不校准的（miscalibrated），可能会导致一些问题，比如过于自信（overconfident）或过于谨慎（underconfident）。</p><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20231225164547118.png" alt="image-20231225164547118" data-proofer-ignore></p><p><strong>预训练的LM，通常 置信度≈准确率（概率）；</strong></p><p><strong>微调后的LM / RLHF之后的LM，通常 置信度不约等于准确率（即置信度校准变差），往往是会变得“过于自信”，置信度&gt;准确率。</strong></p><hr /> <font color="DarkBlue" size="4">那么，if LM的置信度低，就很可能代表它没有足够的数据或信息来支持它的responce/predict（缺乏知识）。**此时，就是LM需要外部知识的时候。**</font><p>本文的设定是，</p><p>if 任何1个token的概率（置信度）低于阈值$\theta \in [0,1]$ → 检索</p><p>else →不检索</p><p>如下：</p><p>​ \(\theta = 0:不检索\)</p><p>​ \(\theta&lt;1:当句子中存在概率低于\theta的token时，触发检索\)</p><p>​ \(\theta = 1:每句话都检索\)</p><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20231225171940233.png" alt="image-20231225171940233" data-proofer-ignore></p><p>其中，$q_t$是基于$\hat{s}_t$生成的。</p><p><strong>3.2.2 基于置信度的query公式</strong></p><blockquote><p>先检索后生成or先生成后纠正？</p></blockquote><p>方式1：使用$\hat{s}_t$直接作为$q_t$，这种方式放在long-form生成中，就必须有主动信息访问（active information access）。</p><blockquote><p>参考：使用LMs生成的假设标题或段落作为query/evidence的方法。(Gao et al., 2022; Sun et al., 2022; Yu et al., 2022; Mao et al., 2021)</p></blockquote><p>previous context指的是生成过程中已经生成的句子，也就是当前句子之前的内容。这篇论文的作者发现，如果只根据previous context来检索文档，那么检索到的文档可能与当前句子的主题不相关，或者与下一句的内容有冲突。因此，他们提出了使用next sentence来检索文档，也就是根据当前句子的生成概率分布，选择一个最可能的下一句作为查询。这样做的好处是可以提前预测未来的内容，从而检索到更相关的文档，同时也可以避免生成重复或无意义的句子。</p><p>我们发现，与前一个上下文相比，使用下一个句子进行检索可以获得更好的结果，如后面的第6.2小节所示。弊端是，这种方式有很高的风险出现“错误传播”的问题。比如：如果LM生成句子”Joe Biden attended the University of Pennsylvania”，事实是他就读于the University of Delaware，使用这个包含事实错误的句子可能会导致后续的检索都被误导，本文提出了<strong>2个简单的方法</strong>来客服这个问题，如下图3：</p><p><img data-src="https://pdf.cdn.readpaper.com/parsed/fetch_target/e56e7ef7642576be3129961115b04a63_4_Figure_3_528787159.png" alt="img" data-proofer-ignore></p><p>图中下划线是概率（置信度）较低的token。</p><p><strong>方法A：隐式query公式（Masked sentences as implicit queries.）</strong></p><p>低于阈值$\beta$的token mask掉。这消除了句子中潜在的干扰，从而提高了检索的准确性。</p>\[mask(\hat{s}_t)\]<p><strong>方法B：显式query公式（Generated questions as explicit queries.）</strong></p><p>针对$\hat{s}_t$中的低置信度的token生成明确的问题，例如，如果LM不确定“宾夕法尼亚大学”，那么像“乔·拜登上过哪所大学？”这样的问题可以帮助检索相关信息。这种方法参考Self-Ask。</p><blockquote><p>Self-ask（Press et al.，2022）通过手动将后续问题插入下游任务示例来实现这一点，如稍后提示D.2所示，这需要特定任务的注释工作。</p></blockquote><p>本文的方法在其基础上升级，在没有额外注释下生成低置信区间的问题。</p><p>Step 1：首先，提取概率低于$\beta$的所有token组成的区间$z$，用prompt3.2这样的模板来使LM生成$q_{t,z}$，这个问题的答案刚好能落在区间$z$中:</p><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20231225180021047.png" alt="image-20231225180021047" data-proofer-ignore></p><p>Step 2：使用每个$q_{t,z}$进行检索，对于返回的文档<strong>交错</strong>排列到1个带index的list中。</p>\[qgen(\hat{s}_t)\]<p>总结方法A和方法B：</p><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20231226092156870.png" alt="image-20231226092156870" data-proofer-ignore></p><p><strong>实现细节：</strong></p><p>基础模型：GPT-3.5 LMs text-davinci-003 访问API</p><p>文档语料集&amp;检索器：<strong>由于本文关注的是检索生成一体化的机制</strong>，所以使用现成的检索器，将query作为input并返回相关文档的list。</p><ul><li><p>依赖wikipedia的数据集：使用<strong>Wikipedia dump</strong>转储外部知识文档。检索器采用<strong>BM25</strong>.</p><li><p>依赖开放网络的数据集：检索器采用<strong>Bing 搜索引擎API</strong>[^ 3]。</p><p>[^ 3](https://www.microsoft.com/en-us/bing/apis/ bing-web-search-api)</p></ul><h4 id="多次检索的baselines"><span class="mr-2">多次检索的Baselines</span><a href="#多次检索的baselines" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>目前常见的被动多次检索增强语言模型也可以采用FLAREdirect框架。</p><p>针对When&amp;What去检索，本文梳理了3个baseline大类，这些baselines并不是相应论文的精确复制品，因为许多设计选择不同，因此无法进行直接比较。本文使用相同的设置来实现它们，仅控制When&amp;What to retrieve变化。</p><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20231226110114103.png" alt="image-20231226110114103" data-proofer-ignore></p><p><strong>Previous-window</strong>这种方法生成一些tokens就使用当前句子之前的 $l$ 个token作为query触发检索，其中 $l$ = 窗口大小。前一个窗口生成的token用于query：</p><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20231226101313819.png" alt="image-20231226101313819" data-proofer-ignore></p><p>RETRO、IC-RALM、KNNLM都是属于这一类，前两个是每隔一些tokens触发检索，最后一个是每个token都触发检索。本文follow IC-RALM设置 $l=16$ 。</p><blockquote><p>这个公式是从<a href="https://zhuanlan.zhihu.com/p/342791154">这篇论文</a><a href="https://zhuanlan.zhihu.com/p/342791154">的第三页的第一段引用的</a><a href="https://zhuanlan.zhihu.com/p/342791154">1</a>。它是在介绍一种名为RETRO的方法，它是一种用于文本生成的方法，它可以根据输入和生成过程中的信息来动态地检索外部知识资源，从而提高生成质量。这个公式的意思是，将生成的文本分成长度为l的窗口，每个窗口包含l个词。yt表示第t个窗口，它是一个向量，包含从第(t−1)l+1个词到第tl个词的所有词。例如，如果l=3，那么y1=[w1,w2,w3]，y2=[w4,w5,w6]，以此类推。这样做的目的是为了方便地使用当前句子之前的l个词作为查询，来检索与未来内容相关的文档。</p><p>假设设定窗口=3，那么是不是就一定是在3的倍数的时候触发检索？</p><p>不一定是这样的。RETRO的检索方法是每生成几个词，就使用当前句子之前的l个词作为查询，来检索与未来内容相关的文档。这里的“几个词”是一个可调节的参数，它可以根据不同的任务和数据集来设置。如果这个参数等于l，那么就相当于每生成一个窗口，就触发一次检索。如果这个参数小于l，那么就相当于每生成几个词，就触发一次检索。如果这个参数大于l，那么就相当于每生成几个窗口，就触发一次检索。这篇论文中的实验结果表明，这个参数的选择会影响生成的质量和效率，因此需要根据具体的情况来确定。</p></blockquote><p><strong>Previous-sentence</strong>这种方法每个句子都触发检索，并且使用前一个句子作为query。</p><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20231226105059989.png" alt="image-20231226105059989" data-proofer-ignore></p><p>IRCoT属于这类。</p><p><strong>Question decomposition</strong>这种方法针对于特定任务的样本，人工标注以引导语言模型生成分解的子问题、同时生成输出。比如，Self-ask属于这类，它在样本中人工插入子问题，使用的Prompt如下图。对于测试样本，每当模型生成子问题时，都会动态触发检索。</p><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20231226105710483.png" alt="image-20231226105710483" data-proofer-ignore></p><p>这三类方法都是<strong>在生成过程中检索外部知识</strong>。他们的缺点有：</p><ol><li>使用<strong>前面</strong>生成的结果中的token作为query可能无法反映LMs<strong>将来</strong>打算生成什么<li>以固定间隔的token作为query检索信息的效率可能很低，因为它很可能选择的不是合适的token<li>问题分解方法要求<strong>特定任务的prompt工程</strong>，限制了在新任务中的泛化性</ol><h4 id="实验设置flareflaredirect"><span class="mr-2">实验设置（FLARE=FLAREdirect）</span><a href="#实验设置flareflaredirect" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>策略：few-shot incontext learning</p><p>评估任务：4个知识密集型任务，每个数据集抽样500个样本。如表7</p><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20231226114021885.png" alt="image-20231226114021885" data-proofer-ignore></p><p>follow的工作：Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.</p><p>超参数：根据dev集选择，如表9</p><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20231226114116523.png" alt="image-20231226114116523" data-proofer-ignore></p><p><strong>Multihop QA</strong>：通过信息检索和推理来回答复杂问题。</p><p>2WikiMultihopQA数据集：来自维基百科文章的2-hop复杂问题，回答这些问题需要模型具备“组合”、“比较”、“推理”的能力。</p><p>本文中，采用Self consistency improves chain of thought reasoning in language models.中提出的方法生成思维链和最终答案。具体设置参考表7。</p><blockquote><p>“Why did the founder of Versus die?”(“范思哲创始人为何去世？”)</p><p>目标输出：（”The founder of Versus was Gianni Versace. Gianni Versace was shot and killed on the steps of his Miami Beach mansion on July 15, 1997. So the answer is shot.”）”范思哲的创始人是詹尼-范思哲。詹尼-范思哲于 1997 年 7 月 15 日在迈阿密海滩豪宅的台阶上被枪杀。所以答案是枪杀”</p><p>使用检索器为BM25，检索预料是维基百科文章。prompt为：其中这个8个示例是examples</p><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20231226144849137.png" alt="image-20231226144849137" data-proofer-ignore></p><p>与Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.这篇文章类似，本文也发现了将<strong>examples的检索结果纳入其中可以提高性能</strong>，使用每个example的输入x作为query来检索一些文档，然后用下面这个prompt的格式添加这些文档。</p><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20231226151113579.png" alt="image-20231226151113579" data-proofer-ignore></p><p>我们发现，检索文档数量越多能提高性能，所以本文使用了text-davinci-003 输入长度限制内可容纳的最大文档数，即2WikiMultihopQA 的文档数为 2</p></blockquote><p><strong>Commonsense reasoning</strong>：需要世界知识和常识知识来回答的问题。</p><p>StrategyQA：这是一个众包的是/否问题的集合</p><p>本文中，采用Chain of thought prompting elicits reasoning in large language models.中提出的方法生成思维链和最终答案。具体设置参考表7。</p><p>我们提取最终答案，并使用<strong>精确匹配</strong>将其与黄金答案进行匹配。</p><blockquote><p>prompt是6个example和3个文档：</p><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20231226172453159.png" alt="image-20231226172453159" data-proofer-ignore></p></blockquote><p><strong>Long-form QA</strong>：必须回答出复杂问题的全面答案。</p><p>ASQA作为测试平台，其中输入是具有多种解释的模糊问题，输出应该涵盖所有这些问题。</p><blockquote><p>例如，“费城老鹰队在哪里进行主场比赛？”可以询问城市、体育综合体或体育场。我们发现，在许多情况下，甚至对人类来说，识别问题的哪个方面是模糊的也是一项挑战。</p></blockquote><p>本文创建了另一个设置（ASQA-hint），在这里我们提供了一个简短的hint，以指导LMs在生成答案时保持正轨。上述例子的hint是“这个问题在具体地点或地点方面不明确。”</p><p>指标：follow ASQA: Factoid Questions Meet Long-Form Answers这篇文章的指标，包括EM、基于RoBERTa的QA评分（DisambigF1）、ROUGE（Lin，2004），以及Disambig-F1和ROUGE的综合评分（DR）。</p><p>prompt：手动注释了8个example，和3个文档。</p><p><strong>Open-domain summarization</strong>：通过从开放网络收集信息来生成关于主题的全面摘要。</p><p>WikiAsp：从维基百科的20个领域中生成关于实体的aspect-based 的摘要。例如，”生成关于回声学校（俄勒冈州）的摘要，包括以下方面：学术、历史”。</p><p>指标：包括ROUGE，基于命名实体的F1，以及衡量事实一致性的UniEval（Zhong et al.，2022）。</p><h4 id="实验结果"><span class="mr-2">实验结果</span><a href="#实验结果" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><ol><li><p>整体结果</p><p><img data-src="https://pdf.cdn.readpaper.com/parsed/fetch_target/e56e7ef7642576be3129961115b04a63_6_Figure_4_1366057918.png" alt="img" data-proofer-ignore></p><ul><li><p>在所有任务/数据集上，FLARE 的性能都优于所有基线，这表明 FLARE 是一种通用方法，可以在整个生成过程中有效地检索更多信息。</p><li>在各种任务中，多跳 QA 的改进最为显著。这主要是由于该任务定义明确，目标明确，即通过 2 跳推理过程生成最终答案，这使得 LM 更容易生成主题输出。<li>相比之下，ASQA 和 WikiAsp 更具开放性，增加了生成和评估的难度。<li>ASQA-hint 的改进幅度大于 ASQA，这是因为在很多情况下，识别模棱两可的方面即使对人类来说也是一项挑战，而提供通用提示则有助于 LM 紧扣主题。</ul></ol><p><img data-src="https://pdf.cdn.readpaper.com/parsed/fetch_target/e56e7ef7642576be3129961115b04a63_6_Table_1_755530990.png" alt="img" data-proofer-ignore></p><p>FLARE和baselines在2WikiMultihopQA上的整体结果。</p><ul><li>大多数情况下，多次检索增强都优于单次检索，但各有优势。<li>the previous sentence进行检索的改进相对较小，我们认为这主要是因为在 2WikiMultihopQA 中，上一句话描述的实体或关系往往与下一句话中的实体或关系不同。<li>the previous-window可能会利用句子的前半部分来检索可能有助于生成后半句的信息。<li>在所有基线中，问题分解方法（Press 等人，2022 年）的性能最好，这并不奇怪，因为用分解的子问题（提示 D.2）手动注释的非上下文示例可以引导 LM 生成与后代主题/意图一致的子问题。<li>FLARE 的表现优于这一基线，这表明手动注释示例对于有效的未来感知检索并非必要。<li>FLAREinstruct 与问题分解之间的差距很大，这表明使用一个固定的检索指令和示例来生成搜索查询具有挑战性。比如语言模型可能无法理解检索指令的含义，或者无法根据示例生成合适的查询，或者生成的查询过于简单或重复，导致检索出的文档质量不高</ul><p><img data-src="https://pdf.cdn.readpaper.com/parsed/fetch_target/e56e7ef7642576be3129961115b04a63_7_Table_2_-692736178.png" alt="img" data-proofer-ignore></p><ul><li>在 ASQA 数据集上，使用the previous window 进行检索的性能低于单次检索，我们假设这是因为前一个窗口不能准确反映未来的意图。</ul><p><strong>由于我们专注于评估事实性，强调事实内容的指标（例如 EM、Disambig-F1、UniEval）比在所有标记上计算的指标更可靠（ROUGE-L）。</strong></p><ol><li>消融实验</ol><p><strong>前瞻性检索和基于过去上下文的检索哪个更有效？</strong></p><p>在 2WikiMultihopQA 和 ASQA-hint 上进行了消融实验，比较了使用上一句和下一句进行检索的效果。</p><p>具体来说，这两种方法都检索每个句子，并直接使用完整的上一句/下一句作为查询。如表 3 所示，使用下一句进行检索的效果明显优于使用上一句，这证实了我们的假设。</p><p><img data-src="https://pdf.cdn.readpaper.com/parsed/fetch_target/e56e7ef7642576be3129961115b04a63_7_Table_3_1744204211.png" alt="img" data-proofer-ignore></p><p>我们还使用不同数量的过去标记作为查询，运行了前一窗口方法。如表 4 所示，使用过多的过去标记（大于 32 个）会降低性能，这进一步证实了我们的假设，即过去的上下文可能与后代的意图无关。</p><p><strong>主动检索的重要性</strong></p><p>策略是调整主动检索的阈值，从不检索到逐句检索，$\theta$从0-1。然后计算激活检索的steps/sentences的比例，基于此呈现性能。如图5所示：</p><p><img data-src="https://pdf.cdn.readpaper.com/parsed/fetch_target/e56e7ef7642576be3129961115b04a63_7_Figure_5_-2135125727.png" alt="img" data-proofer-ignore></p><p>在2WikiMultihopQA上，当检索百分比超过60%时，性能趋于平稳，这表明在lm是自信的情况下检索是不必要的。</p><p>在StrategyQA上，当检索百分比超过50%时，性能下降，表明不必要的检索会引入噪声，阻碍原始生成过程。我们发现对40%-80%的句子触发检索通常会导致跨任务/数据集的良好性能。</p><p><strong>不同query公式的影响</strong></p><p><img data-src="https://pdf.cdn.readpaper.com/parsed/fetch_target/e56e7ef7642576be3129961115b04a63_8_Table_5_-2070921320.png" alt="img" data-proofer-ignore></p><p>在表 5 中，我们比较了 FLARE 在不同屏蔽阈值 β 下的性能。</p><ul><li><p>直接检索完整句子（β = 0）比屏蔽低概率标记更差，这证实了我们的假设，即低置信度的错误标记会分散检索者的注意力。</p><p><img data-src="https://pdf.cdn.readpaper.com/parsed/fetch_target/e56e7ef7642576be3129961115b04a63_8_Table_6_1366057918.png" alt="img" data-proofer-ignore></p></ul><p>我们在表 6 中比较了隐式和显式查询表述方法。</p><ul><li>两种方法的性能相似，表明这两种方法都能有效反映信息需求。</ul><h4 id="局限性"><span class="mr-2">局限性</span><a href="#局限性" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>FLARE在Wizard of Wikipedia (Dinan et al.， 2019)和ELI5上没有提高性能。</p><ol><li><p>Wizard of Wikipedia是一个知识密集型对话生成数据集，其输出相对较短(平均约20个令牌)，因此可能不需要检索多个不同的信息片段。</p><li><p><a href="https://github.com/jzbjyb/FLARE">在ELI5数据集上，生成的答案内容比较长，所以需要检索多个相关的信息源，但是这也带来了一些困难，比如如何将检索到的信息与生成的文本进行有效的对齐，以及如何评估生成文本的质量和事实性</a><a href="https://github.com/jzbjyb/FLARE">4</a>。这些困难导致了单次检索和FLARE的方法都没有比不使用检索的方法有显著的提升。</p><li><p>从工程角度看，交错生成和检索的天真实施会增加生成的开销和成本。LMs 需要激活多次（每次检索一次），而且无缓存实现还需要在每次检索后重新计算之前的激活。如果采用特殊的架构设计，对检索文档 Dqt 和输入/生成 (x/y&lt;t) 进行独立编码，就有可能缓解这一问题。</p></ol><h3 id="3-making-retrieval-augmented-language-models-robust-to-irrelevant-context"><span class="mr-2">3. MAKING RETRIEVAL-AUGMENTED LANGUAGE MODELS ROBUST TO IRRELEVANT CONTEXT</span><a href="#3-making-retrieval-augmented-language-models-robust-to-irrelevant-context" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>https://github.com/oriyor/ret-robust</p><h4 id="针对的问题"><span class="mr-2">针对的问题：</span><a href="#针对的问题" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>RALM 的一个重要要求是检索到的信息在相关时有助于模型性能，并且在不会损害性能。这在多跳推理场景中尤其重要，因为滥用不相关的证据会导致级联错误。</p><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20240111150739380.png" alt="image-20240111150739380" data-proofer-ignore></p><h4 id="本文的工作"><span class="mr-2">本文的工作：</span><a href="#本文的工作" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><ol><li><p>对<strong>5个</strong>开放域问答基准进行了彻底的分析，描述了检索何时降低准确性的情况。</p><li><p>提出了2种缓解方法：</p><p>方法1：（Baseline）根据小型自然语言推理 (NLI) 模型过滤掉不包含问答对的检索到的段落。这可以有效地防止性能下降，但代价是也丢弃相关段落。</p><p>方法2：一种自动生成数据的方法来微调语言模型（训练llm何时使用检索），具体，在训练时使用相关和不相关上下文的混合。</p><p>实验表明，即使是 1,000 个示例也足以对模型进行训练，使其对无关上下文具有鲁棒性，同时在具有相关上下文的示例中保持较高的性能。</p></ol><h4 id="工作1对5个开放域问答基准进行了彻底的分析描述了检索何时降低准确性的情况"><span class="mr-2">工作1：对<strong>5个</strong>开放域问答基准进行了彻底的分析，描述了检索何时降低准确性的情况。</span><a href="#工作1对5个开放域问答基准进行了彻底的分析描述了检索何时降低准确性的情况" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><h5 id="工作11-对5个开放域问答基准进行了彻底的分析"><span class="mr-2">工作1.1 对<strong>5个</strong>开放域问答基准进行了彻底的分析</span><a href="#工作11-对5个开放域问答基准进行了彻底的分析" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20240111151355500.png" alt="image-20240111151355500" data-proofer-ignore></p><p>lama-2- 13b在五个QA任务中提示的准确性，三种设置下:(a)没有检索，(b)从强大的搜索引擎检索top-1，以及(c)随机检索的通道。</p><p>发现：检索增强可以提高性能，但即使是强大的检索也会损害StrategyQA和Fermi上的性能，并且随机上下文会显著降低性能。</p><h5 id="工作12-将evidence文档纳入ralm的常用方法概述"><span class="mr-2">工作1.2 将evidence/文档纳入RALM的常用方法概述</span><a href="#工作12-将evidence文档纳入ralm的常用方法概述" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>RALMs和LMs的定义</p>\[p_{LM}=\prod_{i=1}^{n}p_\theta(x_i|x_{&lt;i})\] \[p_{RALM}=\prod_{i=1}^{n}p_\theta(x_i|R_C(x_{&lt;i});x_{&lt;i})\]<p>其中$R_C$是检索操作。</p><p>本文follow Self-Ask和IR-CoT，关注ODQA（开放域问答）的多跳问答交错检索。</p><blockquote><p>多跳问答交错检索：</p><p>对每个中间问题进行检索，并为每个问题准备上下文。</p><p>单跳VS多跳</p><p>在单跳设置中，模型必须在“给定问题”和“检索到的上下文”的情况下生成答案。</p><p>在多跳设置中，模型必须生成中间问题和答案，直到得到最终答案，并且在每个中间问题之后调用原始问题的检索器。</p></blockquote><h4 id="工作22种缓解方法"><span class="mr-2">工作2：2种缓解方法</span><a href="#工作22种缓解方法" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><h5 id="工作21探讨使用nli模型识别不相关上下文的潜力"><span class="mr-2">工作2.1：探讨使用NLI模型识别不相关上下文的潜力</span><a href="#工作21探讨使用nli模型识别不相关上下文的潜力" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>NLI的一个用法是<a href="https://www.sbert.net/docs/pretrained-models/nli-models.html">判断一个假设（hypothesis）是否由一个前提（premise）推出，或者是否与前提相矛盾，或者是否与前提无关</a><a href="https://www.sbert.net/docs/pretrained-models/nli-models.html">1</a>。例如：</p><p>前提：有一只猫在沙发上睡觉。 假设：有一只动物在沙发上睡觉。 结果：蕴含（entailment），因为前提可以推出假设。</p><p>前提：有一只猫在沙发上睡觉。 假设：有一只狗在沙发上睡觉。 结果：矛盾（contradiction），因为前提与假设不一致。</p><p>前提：有一只猫在沙发上睡觉。 假设：今天是星期三。 结果：中立（neutral），因为前提与假设无关。</p><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20240111154010876.png" alt="image-20240111154010876" data-proofer-ignore></p><p>本文提出首先利用一个检索模型（retrieval model）从外部文档（external document）中找到与问题相关的上下文（context），然后利用一个生成模型（generation model）根据上下文生成答案。</p><p>生成模型有两种，一种是LM，另一种是RALM。</p><p>论文中提出了一种简单的回退策略（back-off strategy），就是先用LM生成一次答案，然后用RALM生成一次答案，最后<strong>用一个NLI模型判断哪个答案更符合上下文的逻辑关系</strong>，如果RALM生成的答案被NLI模型判断为蕴含，就选择RALM的答案，否则就选择LM的答案。</p><h5 id="工作22对模型进行微调以使其对无关上下文具有鲁棒性的过程"><span class="mr-2">工作2.2：对模型进行微调以使其对无关上下文具有鲁棒性的过程</span><a href="#工作22对模型进行微调以使其对无关上下文具有鲁棒性的过程" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>由于仅引入了检索增强的LM（RALM）在他原本的训练时没有引入外部文档检索，所以比posthoc filtering更有效的解决方案可能是训练RALM忽略无关的上下文。</p><p><strong>本文关注在相对较小的数据集（几百个）上训练是否足够</strong></p><p>单跳：</p><p>$R$ 检索到的文档（context）扩充问题即可。</p><p>relevant context：创建训练样本（使用的是$R_C$返回的top1 context）</p><p>irrelevant context：要么是用$R_C$返回的低排序的结果，要么是另一个问题的随机context。</p><p>多跳（本文研究目标）：</p><p>主要挑战是生成训练示例，具体目标是“自动生成检索增强分解步骤”。</p><p>基模型：Llama-2-13B</p><p>训练数据：3 ODQA benchmarks=NQ的1000个训练样本（单跳）+2WIKIQA的500个训练问题1539个样本（显示）+StrategyQA的414个问题和1584个样本（隐式）</p><ul><li>首先，它用 GPT-3作为一个大型语言模型，用 SA-NR提示来生成复杂任务的分解方案。这些分解方案是一系列的简单子任务，每个子任务都有一个输入和一个输出。<li>然后，它用不同的方法来验证生成的分解方案的质量。对于有中间答案的数据集，它用中间答案来过滤掉不正确的分解方案。对于没有中间答案的数据集，它用自洽性来检查分解方案是否能够得到正确的最终答案。<li>接着，它用 Llama-2-13B作为一个微调过的 LLM，用 SA-RetRobust提示来根据分解方案和检索到的证据来解决复杂任务。它在训练过程中用相同的概率来选择最相关的、低排名的或随机的证据，以提高模型的鲁棒性。<ul><li><a href="https://help.sap.com/docs/SAP_BUSINESSOBJECTS_BUSINESS_INTELLIGENCE_PLATFORM/3d4f417fd0764f909c0ef7931e19fe1a/466795506e041014910aba7db0e91070.html">首先，它需要用一个已经生成了分解方案的 LLM，比如 GPT-3</a><a href="https://help.sap.com/docs/SAP_BUSINESSOBJECTS_BUSINESS_INTELLIGENCE_PLATFORM/3d4f417fd0764f909c0ef7931e19fe1a/466795506e041014910aba7db0e91070.html">1</a>，来提供每个子任务的输入和输出。这些分解方案是一系列的简单问题和答案，比如“谁是美国总统？”和“乔·拜登”。<li><a href="https://github.com/f/awesome-chatgpt-prompts">然后，它需要用一个检索模型，比如 DPR</a><a href="https://github.com/f/awesome-chatgpt-prompts">2</a>，来根据每个子任务的输入，从一个大规模的文档集合中检索出最相关的证据。它只用最高排名的证据，也就是 SA-R@1 提示的意思。这些证据是一些包含相关信息的文本片段，比如“乔·拜登于 2021 年 1 月 20 日宣誓就职，成为美国第 46 任总统”。<li><a href="https://github.com/facebookresearch/segment-anything">接着，它需要用一个微调过的 LLM，比如 Llama-2-13B</a><a href="https://github.com/facebookresearch/segment-anything">3</a>，来根据分解方案和检索到的证据来解决复杂任务。它用 SA-R@1 提示来指导 LLM 的输入和输出的格式，比如“输入：谁是美国总统？证据：乔·拜登于 2021 年 1 月 20 日宣誓就职，成为美国第 46 任总统。输出：乔·拜登”。<li><a href="https://huggingface.co/meta-llama/Llama-2-13b-hf">最后，它需要用一个优化方法，比如最大似然估计（MLE）</a><a href="https://huggingface.co/meta-llama/Llama-2-13b-hf">4</a>，来调整 LLM 的参数，使其能够生成正确的子任务和最终任务的输出。它用一些已知答案的训练数据来计算 LLM 的输出的概率，然后找到使这个概率最大的参数值。</ul><li>最后，它在三个开放域问答（ODQA）的基准数据集上评估了它的方法的效果，分别是单跳的 NQ，显式的 2WIKIMQA 和隐式的 STRATEGYQA。它还与其他的对比方法进行了消融实验，分析了不同的分解方案和证据的影响。</ul><p>实验结果：</p><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20240118143039795.png" alt="image-20240118143039795" data-proofer-ignore></p><ul><li>即使不检索外部文档，那用分解问题数据集训练之后在 5 个数据集上表现都提升了，这说明分解问题数据集的训练的确提高了模型的解决复杂问题的能力。<li>在 prompt 中增加外部文档的 RALM 能够提升 LM 在单跳问题和显式问题上的表现，但隐式数据上的表现反而下降了。这可能是因为隐式问题需要更多的推理和创造力，而不是简单的检索和匹配。<li>当使用 NLI 模型来辅助区分外部文档是否相关后，检索就不会再伤害 LM 的性能了。这是因为 NLI 模型可以帮助过滤掉那些与问题无关或有误导性的证据，让 LM 只关注有用的信息。但是，这样做也有一个代价，就是当检索能够提高模型的性能时，这种提升会减少。这是因为 NLI 模型可能也会过滤掉一些有助于解决问题的证据，导致模型缺少一些重要的信息。<li>本文的模型性能最优。这是因为它综合了分解方案、检索证据和 NLI 模型的优势，使得 LM 能够更好地解决复杂任务，尤其是那些需要多跳推理和检索的任务。</ul><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20240118145047784.png" alt="image-20240118145047784" data-proofer-ignore></p><ul><li>当引入的外部文档是low-rank排名低的内容的时候，本文的模型能够提高NQ和2WIKIMQA的性能，保持STRATEGYQA的性能。<li>当引入的外部文档是随机内容的时候，只有本文的模型能够保持住性能，也就是说能够避免收到随机内容的影响。</ul><h3 id="4-self-rag-learning-to-retrieve-generate-andcritique-through-self-reflection"><span class="mr-2">4. SELF-RAG: LEARNING TO RETRIEVE, GENERATE, ANDCRITIQUE THROUGH SELF-REFLECTION</span><a href="#4-self-rag-learning-to-retrieve-generate-andcritique-through-self-reflection" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p align="right">---Arxiv，华盛顿大学 IBM研究院，23年8月</p><p>尽管具有非凡的能力，大型语言模型(llm)经常产生包含事实不准确的响应，因为它们只依赖于它们封装的参数化知识。RAG改善了这个问题，然而，不加选择地检索和合并固定数量的检索段落，无论检索是否必要，或者段落是否相关，都会降低LM的通用性，或者可能导致无益的响应生成。</p><p>我们引入了一个名为 “<strong>自我反思检索</strong>-增强生成（SELF-RAG）”的新框架，通过检索和自我反思来提高 LM 的质量和事实性。Self-RAG的做法：训练一个任意的LM，它可以自适应地按需检索段落，并使用特殊的token(称为[反射token])生成和反映检索到的段落及其自己的生成结果。生成[反射token]使LM在推理阶段可以控制，使其能够根据不同的任务需求调整其行为。</p><p>实验表明，Self-RAG (7B和13B参数)在不同的任务集上显著优于最先进的llm和检索增强模型。具体来说，Self-RAG在开放域QA、推理和事实验证任务上优于ChatGPT和检索增强的Llama2-chat，并且相对于这些模型，它在提高长格式代的事实性和引用准确性方面显示出显著的进步。</p><h4 id="introduction-2"><span class="mr-2">Introduction</span><a href="#introduction-2" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p><img data-src="https://pdf.cdn.readpaper.com/parsed/fetch_target/2e097a024d332e65bea4104f007b7871_1_Figure_1_-678878159.png" alt="img" data-proofer-ignore></p><p>如果需要，我们的端到端训练可以让LM $M$根据检索到的段落生成文本，并通过学习生成特殊标记来批评输出。这些反射token（表1）表示需要检索或确认输出的相关性、支持性或完整性。</p><p>具体来说：</p><p>给定input（或还有前几轮生成）</p><p>Self-RAG首先确定 是否有必要触发检索</p><p>​ if yes：</p><p>​ 1. 输出检索token <code class="language-plaintext highlighter-rouge">retrieve</code> ，然后<strong>（如上图中step1：Retrieve on demand）</strong>调用检索器。</p><p>​ 2. Self-RAG同时评估检索到的这些外部文档的相关性</p><p>​ 3. 给每个文档构造1个prompt并传给LM生成response（<strong>如上图中step2：Generate segment in parallel）</strong></p><p>​ 4. 生成批评token<code class="language-plaintext highlighter-rouge">Critique</code>评价responses，然后根据真实性和整体质量选择最佳输出（<strong>如上图中step3：Critique outputs and select best segment</strong></p><h4 id="self-rag"><span class="mr-2">SELF-RAG</span><a href="#self-rag" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><h5 id="-self-rag-inference"><span class="mr-2">① SELF-RAG Inference</span><a href="#-self-rag-inference" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20231227102023945.png" alt="image-20231227102023945" data-proofer-ignore></p><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20231227104301581.png" alt="image-20231227104301581" data-proofer-ignore></p><p><img data-src="https://pdf.cdn.readpaper.com/parsed/fetch_target/2e097a024d332e65bea4104f007b7871_4_Figure_2_562248199.png" alt="img" data-proofer-ignore></p><h5 id="-self-rag-training"><span class="mr-2">② Self-RAG Training</span><a href="#-self-rag-training" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>$R$-检索器模型：得到相关段落，交错排序。</p><p>$C$-评论家模型：需要训练$C$能生成<code class="language-plaintext highlighter-rouge">IsRel</code> <code class="language-plaintext highlighter-rouge">IsSup</code> <code class="language-plaintext highlighter-rouge">IsUse</code></p><p>$M$-生成器LM：在包含了相关文档和反射tokens的精选语料库中训练$M$，沿用传统的LM目标训练。</p><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20231227111059735.png" alt="image-20231227111059735" data-proofer-ignore></p><p><strong>3.2.1 Training The Critic Model</strong></p><ul><li><p>for评论家模型的数据收集</p><p>人工标注反射token昂贵，pass。用GPT-4来生成这样的反馈（feedback），但是依赖这种专有的LMs会增加API成本并降低再现性。本文通过prompt GPT4生成反射token创建可复用的监督数据，训练一个评论家模型$C$来学习这些监督数据的知识。</p><p>由于不同的反射令牌组有自己的定义和输入如表1，对于每组反射token，都分别从原始训练数据中给他们采样一批样本，并且分别设计不同的prompt指令。</p><div class="table-wrapper"><table><tbody><tr><td>比如<code class="language-plaintext highlighter-rouge">retrieve</code>token，我们用”Given an instruction, make a judgment on whether finding some external documents from the web helps to generate a better response.”这个few-shot prompt来引导GPT-4生成反射token $r$。这个过程形式化表示为$p(r<td>I,x,y)$，$I=$few shot examples。</table></div><p>人工评估了GPT-4生成的结果和人类标注的结果高度一致。</p><p>最终给每组token形成了4k-20k的监督数据，详见Section D和附录A.1。</p><p>下面展示了使用GPT4生成数据阶段的所有instruction和demonstrations。</p><ol><li>表8：生成初始检索token<code class="language-plaintext highlighter-rouge">[Retrieval]</code>，7个example。</ol><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20240103100334552.png" alt="image-20240103100334552" data-proofer-ignore></p></ul><ol><li><div class="table-wrapper"><table><tbody><tr><td>表9：生成[Continue to Use Evidence]<td>[No Retrieval]<td>[Retrieval]。</table></div><div class="table-wrapper"><table><tbody><tr><td>(如果输出句子只能用证据进行验证<td>如果句子不需要任何事实验证（例如，关于常识的主观句子或句子）<td>如果需要额外的信息来验证输出句子并提供解释)</table></div></ol><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20240103100754798.png" alt="image-20240103100754798" data-proofer-ignore></p><ol><li><div class="table-wrapper"><table><tbody><tr><td>表10：生成[Relevant]<td>[Irrelevant]</table></div></ol><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20240103101903107.png" alt="image-20240103101903107" data-proofer-ignore></p><ol><li><div class="table-wrapper"><table><tbody><tr><td>表11：生成[Fully supported]<td>[Partially supported]<td>[No support / Contradictory]</table></div></ol><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20240103102304935.png" alt="image-20240103102304935" data-proofer-ignore></p><ol><li><p>表12：生成[IsUse]token，这个token为数值形式，是有用程度。</p><ol><li>output几乎没有主题或完全不相关<li>output解决了主要请求，但与instruction/query不完整或不相关<li>output是可以接受的，但需要一些关键信息的添加和改进<li>output大部分内容满足query的需求，需要细微的改进，比如更详细的信息、更好的响应结构、语言流畅性<li>output提供了对查询的完整、高度详细和信息丰富的响应，完全满足信息需求</ol><p><img data-src="../../../assets/img/2023-12-04-RAG检索知识增强/image-20240103102455240.png" alt="image-20240103102455240" data-proofer-ignore></p></ol><ul><li><p>评论家模型训练过程</p><p>采集了训练数据$D_{critic}$之后，本文用Llama 2-7B来初始化$C$(跟生成器LM $M$ 保持一致)，</p><li></ul><p>1月底之前复现一个论文，follow他的包括数据集、实验设置等。</p><h3 id="5--novelty-controlled-paraphrase-generation-with-retrieval-augmented-conditional-prompt-tuning"><span class="mr-2">5. Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning</span><a href="#5--novelty-controlled-paraphrase-generation-with-retrieval-augmented-conditional-prompt-tuning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>吗qx7dz2qo</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/nlp/'>NLP</a>, <a href='/categories/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/'>基础知识</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/nlp/" class="post-tag no-text-decoration" >nlp</a> <a href="/tags/rag/" class="post-tag no-text-decoration" >RAG</a> <a href="/tags/%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/" class="post-tag no-text-decoration" >技术综述</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> 本文由作者按照 <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> 进行授权</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">分享</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=RAG+-+zoe+Chen&url=https%3A%2F%2Fzoechen119.github.io%2Fposts%2FRAG%25E6%25A3%2580%25E7%25B4%25A2%25E7%259F%25A5%25E8%25AF%2586%25E5%25A2%259E%25E5%25BC%25BA-%282%29%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=RAG+-+zoe+Chen&u=https%3A%2F%2Fzoechen119.github.io%2Fposts%2FRAG%25E6%25A3%2580%25E7%25B4%25A2%25E7%259F%25A5%25E8%25AF%2586%25E5%25A2%259E%25E5%25BC%25BA-%282%29%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fzoechen119.github.io%2Fposts%2FRAG%25E6%25A3%2580%25E7%25B4%25A2%25E7%259F%25A5%25E8%25AF%2586%25E5%25A2%259E%25E5%25BC%25BA-%282%29%2F&text=RAG+-+zoe+Chen" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="分享链接" data-title-succeed="链接已复制！"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">最近更新</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/C25_11%E5%85%A5%E9%97%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%860/">【NLP入门趣味题】肉眼找朋友</a><li><a href="/posts/C25_11%E5%85%A5%E9%97%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%861/">【NLP入门趣味题】探索语言模型与词向量</a><li><a href="/posts/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a><li><a href="/posts/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%AE%9E%E9%AA%8C/">多模态实验</a><li><a href="/posts/%E5%A4%9A%E6%A8%A1%E6%80%81RAG/">多模态rag</a></ul></div><div id="access-tags"><div class="panel-heading">热门标签</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/">技术综述</a> <a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> <a class="post-tag" href="/tags/rag/">RAG</a> <a class="post-tag" href="/tags/%E8%AF%AD%E8%A8%80%E5%AD%A6/">语言学</a> <a class="post-tag" href="/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/">预训练模型</a> <a class="post-tag" href="/tags/bert/">BERT</a> <a class="post-tag" href="/tags/courses/">Courses</a> <a class="post-tag" href="/tags/chatgpt/">ChatGPT</a> <a class="post-tag" href="/tags/mamba/">Mamba</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">文章内容</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>相关文章</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/RAG%E6%A3%80%E7%B4%A2%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA/"><div class="card-body"> <em class="small" data-ts="1708444800" data-df="YYYY-MM-DD" > 2024-02-21 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RAG</h3><div class="text-muted small"><p> RAG检索知识增强 不完全手册 本文从k个角度对RAG技术进行综述，第一个角度是“范式演变”，即原始的“朴素RAG”-“进阶RAG”-“模块化RAG”，这个角度个人认为是工程的角度、宏观的角度；第二个角度是“关键问题和相关研究”，大类上可分两类“以优化生成模型的表征空间为目的”和“以优化知识密集型问答任务为目的”。 通常“以优化生成模型的表征空间为目的”的这些模型有几个特点：（1）检...</p></div></div></a></div><div class="card"> <a href="/posts/RAG/"><div class="card-body"> <em class="small" data-ts="1709136000" data-df="YYYY-MM-DD" > 2024-02-29 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RAG</h3><div class="text-muted small"><p> RAG调研 一、一些共识 External knowledge is the key to resolving the problems of LLMs such as hallucination and outdated knowledge, which can make LLMs generate more accurate and reliable responses thr...</p></div></div></a></div><div class="card"> <a href="/posts/RAG%E6%A3%80%E7%B4%A2%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA/"><div class="card-body"> <em class="small" data-ts="1701619200" data-df="YYYY-MM-DD" > 2023-12-04 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RAG</h3><div class="text-muted small"><p> RAG调研 一、一些共识 External knowledge is the key to resolving the problems of LLMs such as hallucination and outdated knowledge, which can make LLMs generate more accurate and reliable responses thr...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/%E5%AF%B9%E8%AF%9D%E6%84%8F%E5%9B%BE%E8%AF%86%E5%88%AB/" class="btn btn-outline-primary" prompt="上一篇"><p>对话意图识别</p></a> <a href="/posts/RAG%E6%A3%80%E7%B4%A2%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA/" class="btn btn-outline-primary" prompt="下一篇"><p>RAG</p></a></div><script src="https://utteranc.es/client.js" repo="zoeChen119/zoeChen119.github.io" issue-term="title" crossorigin="anonymous" async> </script> <script type="text/javascript"> $(function() { const origin = "https://utteranc.es"; const iframe = "iframe.utterances-frame"; const lightTheme = "github-light"; const darkTheme = "github-dark"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } addEventListener("message", (event) => { let theme; /* credit to <https://github.com/utterance/utterances/issues/170#issuecomment-594036347> */ if (event.origin === origin) { /* page initial */ theme = initTheme; } else if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); } else { return; } const message = { type: "set-theme", theme: theme }; const utterances = document.querySelector(iframe).contentWindow; utterances.postMessage(message, origin); }); }); </script></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://github.com/zoeChen119">陈政伊</a>. <span data-toggle="tooltip" data-placement="top" title="除非另有说明，本网站上的博客文章均由作者按照知识共享署名 4.0 国际 (CC BY 4.0) 许可协议进行授权。">保留部分权利。</span></p></div><div class="footer-right"><p class="mb-0"> 本站由 <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> 生成，采用 <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> 主题。</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">热门标签</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/">技术综述</a> <a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> <a class="post-tag" href="/tags/rag/">RAG</a> <a class="post-tag" href="/tags/%E8%AF%AD%E8%A8%80%E5%AD%A6/">语言学</a> <a class="post-tag" href="/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/">预训练模型</a> <a class="post-tag" href="/tags/bert/">BERT</a> <a class="post-tag" href="/tags/courses/">Courses</a> <a class="post-tag" href="/tags/chatgpt/">ChatGPT</a> <a class="post-tag" href="/tags/mamba/">Mamba</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">发现新版本的内容。</p><button type="button" class="btn btn-primary" aria-label="Update"> 更新 </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">搜索结果为空</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/zh.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
