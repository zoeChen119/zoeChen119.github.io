<!DOCTYPE html><html lang="zh-CN" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="深入研究BERT位置编码的原理" /><meta property="og:locale" content="zh_CN" /><meta name="description" content="深入研究BERT位置编码的原理" /><meta property="og:description" content="深入研究BERT位置编码的原理" /><link rel="canonical" href="https://zoechen119.github.io/posts/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/" /><meta property="og:url" content="https://zoechen119.github.io/posts/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/" /><meta property="og:site_name" content="zoe Chen" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-09-14T10:10:56+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="深入研究BERT位置编码的原理" /><meta name="twitter:site" content="@twitter_username" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-09-22T14:44:55+08:00","datePublished":"2022-09-14T10:10:56+08:00","description":"深入研究BERT位置编码的原理","headline":"深入研究BERT位置编码的原理","mainEntityOfPage":{"@type":"WebPage","@id":"https://zoechen119.github.io/posts/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/"},"url":"https://zoechen119.github.io/posts/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/"}</script><title>深入研究BERT位置编码的原理 | zoe Chen</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="zoe Chen"><meta name="application-name" content="zoe Chen"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/img/avatar.jfif" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">zoe Chen</a></div><div class="site-subtitle font-italic">nlper, dler, sims4er</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>首页</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>分类</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>标签</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>归档</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>关于</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/zoeChen119" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['zoe9698','163.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> 首页 </a> </span> <span>深入研究BERT位置编码的原理</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> 文章</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="搜索..."> </span> <span id="search-cancel" >取消</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>深入研究BERT位置编码的原理</h1><div class="post-meta text-muted"> <span> 发表于 <em class="" data-ts="1663121456" data-df="YYYY-MM-DD" data-toggle="tooltip" data-placement="bottom"> 2022-09-14 </em> </span> <span> 更新于 <em class="" data-ts="1663829095" data-df="YYYY-MM-DD" data-toggle="tooltip" data-placement="bottom"> 2022-09-22 </em> </span><div class="d-flex justify-content-between"> <span> 作者 <em> <a href="https://github.com/zoeChen119">陈政伊</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="4327 字"> <em>24 分钟</em>阅读</span></div></div></div><div class="post-content"><h1 id="深入研究bert位置编码的原理">深入研究BERT位置编码的原理</h1><p>记录日期：2022/08/29</p><hr /><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre>📃参考文章
    [1] 苏建林博客
    [2] Transformer学习笔记一：Positional Encoding（位置编码）
    [3] 知乎.实践中BERT如何对长度大于500的文本进行处理？
    [4] 位置编码全面研究-好文！💎
    
    [5] how to generate position embedding heatmap
    [6] The Illustrated Transformer
    [7] BERT的三个Embedding详解
</pre></table></code></div></div><h3 id="0什么是位置编码为什么破解bert处理长文本的问题要研究位置编码"><span class="mr-2">0：什么是位置编码？为什么破解BERT处理长文本的问题要研究位置编码？</span><a href="#0什么是位置编码为什么破解bert处理长文本的问题要研究位置编码" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h4 id="研究位置编码的动机"><span class="mr-2">研究位置编码的动机</span><a href="#研究位置编码的动机" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>在transformer的encoder和decoder的输入层中，使用了Positional Encoding，使得最终的输入满足： input=input_embedding+positional_encoding</p><p>这里，input_embedding是通过常规embedding层，将每一个token的向量维度从vocab_size映射到d_model，由于是相加关系，自然而然地，这里的positional_encoding也是一个d_model维度的向量。（在原论文里，d_model = 512）</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>📌通过常规embedding层，将每一个token的向量维度从vocab_size映射到d_model
这句话表述的流程如下：
</pre></table></code></div></div><p><img data-src="/assets/img/2022-09-14-位置编码/2022-09-19-17-29-39.png" alt="" data-proofer-ignore></p><p>那么，我们为什么需要position encoding呢？在transformer的self-attention模块中，序列的输入输出如下（不了解self-attention没关系，这里只要关注它的输入输出就行）： <img data-src="/assets/img/2022-09-14-位置编码/2022-09-19-17-42-28.png" alt="" data-proofer-ignore></p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre>在self-attention模型中，输入是一整排的tokens，对于人来说，我们很容易知道tokens的位置信息，比如：
（1）绝对位置信息。a1是第一个token，a2是第二个token......
（2）相对位置信息。a2在a1的后面一位，a4在a2的后面两位......
（3）不同位置间的距离。a1和a3差两个位置，a1和a4差三个位置....
但是这些对于self-attention来说，是无法分辩的信息，因为self-attention的运算是无向的。因为，我们要想办法，把tokens的位置信息，喂给模型。
                                            *以上引自[2] Transformer学习笔记一：Positional Encoding（位置编码）*
</pre></table></code></div></div><p>为什么？ 在做NLP的相关任务中，最常见的一个问题便是当输入序列过长时应该如何进行处理。例如在谷歌开源的预训练模型中，最大长度只支持512个Token。</p><p>大家都知道，目前的主流的BERT模型最多能处理512个token的文本。导致这一瓶颈的根本原因是BERT使用了从随机初始化训练出来的绝对位置编码，一般的最大位置设为了512，因此顶多只能处理512个token，多出来的部分就没有位置编码可用了。当然，还有一个重要的原因是Attention的O(n2*d)复杂度，导致长序列时显存用量大大增加，一般显卡也finetune不了。[1]</p><p>对于原因一应该怎么进行处理呢？是简单的进行截断处理吗？ 总的来说，对于这类问题可以有两种方式来进行解决：第1种是从模型入手，例如消除最大长度为512的限制；第2种是从输入入手，改变输入序列的长度重新构造任务。[3]</p><p>对于原因二？ 稀疏注意力</p><h4 id="位置编码2"><span class="mr-2">位置编码[2]：</span><a href="#位置编码2" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><h5 id="-绝对位置最自然的想法"><span class="mr-2">• 绝对位置（最自然的想法）</span><a href="#-绝对位置最自然的想法" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>一种自然而然的想法是，给第一个token标记1，给第二个token标记2…，以此类推。 这种方法产生了以下几个主要问题： （1）模型可能遇见比训练时所用的序列更长的序列。不利于模型的泛化。 （2）模型的位置表示是无界的。随着序列长度的增加，位置值会越来越大。</p><h5 id="-用浮点数表示3个字0051"><span class="mr-2">• 用浮点数表示（3个字：[0,0.5,1]）</span><a href="#-用浮点数表示3个字0051" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>为了解决整型值带来的问题，可以考虑将位置值的范围限制在[0, 1]之内，其中，0表示第一个token，1表示最后一个token。比如有3个token，那么位置信息就表示成[0, 0.5, 1]；若有四个token，位置信息就表示成[0, 0.33, 0.69, 1]。 但这样产生的问题是： （1）当序列长度不同时，token间的相对距离是不一样的。 例如在序列长度为3时，token间的相对距离为0.5；在序列长度为4时，token间的相对距离就变为0.33。 因此，我们需要这样一种位置表示方式，满足于： （1）它能用来表示一个token在序列中的绝对位置 （2）在序列长度不同的情况下，不同序列中token的相对位置/距离也要保持一致 （3）可以用来表示模型在训练过程中从来没有看到过的句子长度。</p><h5 id="-one-hot编码01标记"><span class="mr-2">• one-hot编码（[0,1]标记）</span><a href="#-one-hot编码01标记" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>考虑到位置信息作用在input embedding上，因此比起用单一的值，更好的方案是用一个和input embedding维度一样的向量来表示位置。 input embedding和位置编码怎么concat见：下一节 | 先说Transformer 这时我们就很容易想到二进制编码。如下图，假设d_model = 3，那么我们的位置向量可以表示成： <img data-src="/assets/img/2022-09-14-位置编码/2022-09-19-17-44-17.png" alt="" data-proofer-ignore></p><p>这下所有的值都是有界的（位于0，1之间），且transformer中的d_model本来就足够大，基本可以把我们要的每一个位置都编码出来了。</p><p>但是这种编码方式也存在问题： （1）这样编码出来的位置向量，处在一个离散的空间中，不同位置间的变化是不连续的。这样无法体现相对位置关系。</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre><td class="rouge-code"><pre>假设d_model = 2
input sequence=4
sequence vector=[
                [0,0,1,...,0]1 * vocab_size , 
                [0,0,...,1,0]1 * vocab_size , 
                [0,1,0,...,0]1 * vocab_size , 
                [0,0,...,0,1]1 * vocab_size
                ]
sequence vector经过embedding层变成
token embedding vector=[
                [0.xx,0.xx]1 * d_model=1 * 2 ,
                [0.xx,0.xx]1 * d_model=1 * 2 ,
                [0.xx,0.xx]1 * d_model=1 * 2 ,
                [0.xx,0.xx]1 * d_model=1 * 2 ,
                ]
</pre></table></code></div></div><p>我们有4个位置需要编码，这四个位置向量可以表示成[0,0],[0,1],[1,0],[1,1]。 然后我们就可以把token embedding vector和位置向量加起来。 我们把它的位置向量空间做出来：</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>🌀按理说，[0,0]和[1,0]的距离是2，[0,0]和[1,1]的距离是3，[0,1]和[1,0]的距离是1，但显然图上黑线不是这样表示相对距离的，我认为（索引）这种一维序列本身就不适合用高维向量表示。 &lt;u&gt;如果我们能把离散空间（黑色的线）转换到连续空间（蓝色的线）&lt;/u&gt;，那么我们就能解决位置距离不连续的问题。同时，我们不仅能用位置向量表示整型，我们还可以用位置向量来表示浮点型。 ##### • 连续空间的编码（一维表示“索引”） ![](/assets/img/2022-09-14-位置编码/2022-09-19-17-46-01.png)
</pre></table></code></div></div><p>用连续函数编码，那么要保证①每个位置都有独一无二的编码，②编码空间不是无限域。 出于②的原因，正余弦函数是一个很好的选择，值域不是无穷区间，在[-1,1]。 如果我们是用绝对位置也就是0，1，2，3，…这些用正余弦函数sin,cos来编码，自然会出现不同位置有相同的编码结果值。例如下面图中的第0位和第6位就是相同的编码结果。</p><p>###################################################</p><p>正弦图像是具有周期性的，不巧的话就会出现不同的位置是一个值，真是那样的话，位置编码的意义就失去了，如何让不同词之间分开是位置编码的目的，接下来引入i，依照公式，只有i为偶数才为正弦图，引入i之后代表进一步细分，不止是按照一个一个词来分，<u>而是按照词和词嵌入的特征来分</u>，其实可以想想，人其实大多数情况的行为举止都是差不多的，细分才能看出不同，这里用了 p0 和 p6 第1，3，5个特征： <u>按照词和词嵌入的特征来分</u>：也就是说，如果我们用维度当做正选函数的周期，位置越远正余弦函数的震荡周期越大，那么即使是同一个位置在不同词的表征向量中也能得到不同的数值。</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre>                 位置1   位置2     位置3
tensor([[ 0.4924,  0.3564,  0.4850],词1的表征向量
        [ 1.0244, -0.0162, -0.0017],词2的表征向量
        [ 0.6738,  0.7967,  0.6629]])词3的表征向量 ![f30695e06a59a713fa45ceb33544cc4d.jpeg](en-resource://database/517:1)
</pre></table></code></div></div><p>不难发现，不仅不同词之间分开了，同一个词不同的特征之间也能区分出来 当两个参数都变得很大的时候，又会发生什么呢？ transformer默认的词嵌入维度是512，这里举个特例，假如句子很长，以第101个词为例</p><pre><code class="language-math">y = sin( \frac {100} {10000^{(2i/512)}})
</code></pre><p><img data-src="en-resource://database/519:1" alt="5d29a3a43b46ed8963bf24de6029e8ce.png" data-proofer-ignore></p><p>显然，前期有大量特征会出现重合，且会出现较高频率变换，后期几乎趋于一致（奇数情况也如此），我们其实可以猜想出后面的其实几乎不带有位置信息，下图为奇偶混合的情况，可以看出有很多重合，当i较大时，两者都不带任何信息（第三节会有验证） 下图为小demo的位置编码的可视化结果 <img data-src="en-resource://database/521:1" alt="0ed7dcc56e0e08aa8f45a9e85400ab95.png" data-proofer-ignore></p><p>深度是一个词的特征维度，Position代表的是哪一个词</p><blockquote><p>为什么会出现：当i较小时，特征之间重合较大，而且离得近的位置之间特征变化很快，当i大到一定程度，几乎没有任何区别</p></blockquote><p>我的猜想是当词与词之间离得很近时（pos差得小）或者对于一个词不同特征来说，携带信息相同其实是符合逻辑的。在word2vec里面，如果两个词意相同，转换为坐标空间内的向量就会离得近（对应于一个词不同特征），有时候，一个句子之间离得近的词可能有关联效果，如”I love reading books.”，reading和books它们一起出现的概率是较大的（对应于离得近的词）。</p><blockquote><p>位置编码是加上去的，为什么不拼接上去呢？</p></blockquote><p>拼接的话会扩大参数空间，占用内存增加，而且不易拟合，而且其实没有证据表明拼接就比加来的好</p><blockquote><p>那么，既然有了公式，那位置编码是算出来的还是学出来的呢？</p></blockquote><p>其实算出来和学出来的效果差不多，但是考虑到算出来的可以接受更长的序列长度而不必受训练的干扰，所以在这个模型中，位置编码是通过公式算出来的。</p><ul><li>“不足”：</ul><p>先对位置编码进行可视化（见下图，这里深度变成了512，与Transformer默认参数保持一致） 先看第一列，即每个词的第一个维度，可以发现相似度变化很大，有差不多相似的，但也有几乎无关的，相似的原因猜测是序列所需要的（之所以不用决定是因为提前公式决定的和学出来的差不多），每个句子中都有一个词与另一个词词义相近。因而一开始看好像在低维时位置编码似乎不错，可是当维度显著提升后，300维之后全部都一模一样的，这显然不符合位置编码设计的初衷（捕捉句子间的序列关系），不妨猜测可能是真正起作用的是较小维度的值或者是经过dropout之后将一部分完全相同的值给剔除掉来一定程度提升模型，可是经过我的语言翻译模型实验发现其实加不加结果差不多。</p><blockquote><p>总结就是当嵌入维度显著上升后，模型捕捉词与词之间关系的能力大大下降。</p></blockquote><p><img data-src="en-resource://database/523:1" alt="e0fecf04072c55c51b03de2016c3f4f3.jpeg" data-proofer-ignore></p><blockquote><p>既然400维往后都一样，全部不要怎么样？</p></blockquote><p>接下来我将400维及其后面的全部重置为0（见上），进行了seq2seq法译英任务（基于Transformer），结果是几乎无影响，左图为未做改变的训练和测试准确率图，下面翻译的结果也很丝滑。结果放到了colab，对结果存有疑虑可以去复现一下，请确保你对每一个部分都很了解，复制代码到你本地或者云端，按照注释来添加不同的对照组。</p><p><img data-src="en-resource://database/525:1" alt="5f05e01b01adcc399f7583100ca3c660.jpeg" data-proofer-ignore></p><p>`input: je pars en vacances pour quelques jours . target: i m taking a couple of days off . pred: <start> i m leaving on vacation days .</start></p><p>input: je recherche un assistant . target: i am looking for an assistant . pred: <start> i am looking for an assistant .` 因为一开始为了收敛迅速选取了长度小于10的中短句子，为了防止因为句子因素而造成的影响，下面特地选取了长度在 [11,17] 的句子，（注意这次实验并没有短句子）作为对照组（考虑到这个数据集大部分都是中短句子，长句子并不多，因而模型未充分拟合，正确率维持在 74% ，好在不影响实验），左图为原始的： ![31968bb03715b5d009515a62aedbf250.jpeg](en-resource://database/527:1)</start></p><p>考虑到即使位置不对，词对了准确率依然很高，所以还随机地让模型翻译句子，结果表明如果句子长度是中短型的，似乎模型可以学习足够的位置信息而不依赖于位置编码，可是当句子过长时，即使是位置编码的400维以后也发挥了作用。下面选取了两个例子，一个是较短的，另一个较长（上面的图为原始的）： 有趣的是，位置编码的400维以后的信息依然发挥了作用，可以看见注意力图相对来说左侧更加整齐，同时因为两者整体说都不整齐，说明短句子的位置信息没有那么的明确，而400维置为0的也学到了较为重要的位置信息，因而可以翻译的与原始的相当： <img data-src="en-resource://database/529:1" alt="afb54d5924f96b78de1149214099cbb6.png" data-proofer-ignore></p><p>可是，句子较长的时候，模型不能够依赖自学而获取位置信息，而原始的却依然能够沿轴线对齐，保持位置信息： <img data-src="en-resource://database/531:1" alt="425dee9fc441345dc0d4fed9c7a9a2cf.jpeg" data-proofer-ignore></p><blockquote><p>如果直接不要位置编码，会怎么样？</p></blockquote><p>在短句子中两者无论从准确度还是翻译上来看都是相当的。这也说明了位置编码在短句子（长度小于10）和固定开头的句子中并非发挥应有的作用，句子简单时很可能机器能够自己学会位置信息，这也可以推出短句时加完位置编码后根本没必要Dropout，因为信息本来就不重要嘛 <img data-src="en-resource://database/533:1" alt="a703cdf7147488723ddd3a7d3cb4bf3a.jpeg" data-proofer-ignore></p><p>`input: je pars en vacances pour quelques jours . target: i m taking a couple of days off . pred: <start> i m taking a couple of days off .</start></p><p>input: je recherche un assistant . target: i am looking for an assistant . pred: <start> i am looking for an assistant .`</start></p><p>可是当遇到长句子时，位置变多，没有位置编码几乎都是随机排列的，准确率几乎一致，说明词还是都能猜的差不多，猜词和位置编码没关系： <img data-src="en-resource://database/535:1" alt="211466cc3124a214795b0b759cf79893.jpeg" data-proofer-ignore></p><p>虽然是乱序，但是模型在较短的时候还是能够自己独立学会位置信息 <img data-src="en-resource://database/537:1" alt="baaa2f733c7bb0b8bb228227f9630b79.png" data-proofer-ignore></p><p>可是真正遇到长句子时，没有位置编码就完全乱套了 <img data-src="en-resource://database/539:1" alt="c83083e800726586875bfe8e2f73767b.jpeg" data-proofer-ignore></p><center> **以上引自[4]** </center><h4 id="01bert和transformer的位置编码的区别"><span class="mr-2">01：BERT和Transformer的位置编码的区别</span><a href="#01bert和transformer的位置编码的区别" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>Transformer中的位置编码（PE）</p><p>在Transformer中，位置编码是由sin/cossin/cos函数生成的固定值。</p><p>具体做法：<u>用不同频率的正余弦函数对位置信息进行编码</u>，位置编码向量的维度与文本编码向量的维度相同。因此二者（位置编码向量，文本编码向量）可以直接相加作为token最终的编码向量。</p><h4 id="02experiment"><span class="mr-2">02：Experiment</span><a href="#02experiment" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>实验一：sin&amp;cos交叉、纯sin、纯tanh 说明：位置编码的函数三种设置方案，预训练之后position embedding的向量结果可视化对比</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre>🔮 
     基础模型：google_zh_model
     预训练数据：corpora/book_review_bert.txt
     预训练后得到的模型： models/book_review_model.bin
</pre></table></code></div></div><p>512长度的长文本预训练后的位置编码图像 sin&amp;cos交叉 <img data-src="en-resource://database/545:2" alt="4787ba3e5cadcf7aa27c2eaf0e67a010.jpeg" data-proofer-ignore> sin <img data-src="en-resource://database/545:2" alt="4787ba3e5cadcf7aa27c2eaf0e67a010.jpeg" data-proofer-ignore> tanh <img data-src="en-resource://database/547:1" alt="f55190277b067b663eb3a95cdfff3682.jpeg" data-proofer-ignore></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/nlp/'>NLP</a>, <a href='/categories/%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6/'>模型研究</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/nlp/" class="post-tag no-text-decoration" >nlp</a> <a href="/tags/%E8%AF%AD%E8%A8%80%E5%AD%A6/" class="post-tag no-text-decoration" >语言学</a> <a href="/tags/bert/" class="post-tag no-text-decoration" >BERT</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> 本文由作者按照 <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> 进行授权</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">分享</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=%E6%B7%B1%E5%85%A5%E7%A0%94%E7%A9%B6BERT%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%9A%84%E5%8E%9F%E7%90%86+-+zoe+Chen&url=https%3A%2F%2Fzoechen119.github.io%2Fposts%2F%25E4%25BD%258D%25E7%25BD%25AE%25E7%25BC%2596%25E7%25A0%2581%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=%E6%B7%B1%E5%85%A5%E7%A0%94%E7%A9%B6BERT%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%9A%84%E5%8E%9F%E7%90%86+-+zoe+Chen&u=https%3A%2F%2Fzoechen119.github.io%2Fposts%2F%25E4%25BD%258D%25E7%25BD%25AE%25E7%25BC%2596%25E7%25A0%2581%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fzoechen119.github.io%2Fposts%2F%25E4%25BD%258D%25E7%25BD%25AE%25E7%25BC%2596%25E7%25A0%2581%2F&text=%E6%B7%B1%E5%85%A5%E7%A0%94%E7%A9%B6BERT%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%9A%84%E5%8E%9F%E7%90%86+-+zoe+Chen" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="分享链接" data-title-succeed="链接已复制！"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">最近更新</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/C25_11%E5%85%A5%E9%97%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%860/">【NLP入门趣味题】肉眼找朋友</a><li><a href="/posts/C25_11%E5%85%A5%E9%97%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%861/">【NLP入门趣味题】探索语言模型与词向量</a><li><a href="/posts/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a><li><a href="/posts/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%AE%9E%E9%AA%8C/">多模态实验</a><li><a href="/posts/%E5%A4%9A%E6%A8%A1%E6%80%81RAG/">多模态rag</a></ul></div><div id="access-tags"><div class="panel-heading">热门标签</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/">技术综述</a> <a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> <a class="post-tag" href="/tags/rag/">RAG</a> <a class="post-tag" href="/tags/%E8%AF%AD%E8%A8%80%E5%AD%A6/">语言学</a> <a class="post-tag" href="/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/">预训练模型</a> <a class="post-tag" href="/tags/bert/">BERT</a> <a class="post-tag" href="/tags/courses/">Courses</a> <a class="post-tag" href="/tags/chatgpt/">ChatGPT</a> <a class="post-tag" href="/tags/mamba/">Mamba</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">文章内容</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>相关文章</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/BERT%E7%99%BE%E7%A7%91/"><div class="card-body"> <em class="small" data-ts="1649913056" data-df="YYYY-MM-DD" > 2022-04-14 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>BERT百科大全</h3><div class="text-muted small"><p> BERT百科大全 Attention attention结构的神经网络与递归神经网络相比有什么优点 答： attention结构的输入是句子即可，递归神经网络的输入需要包含句子和句子结构 训练Recursive neural net之前，你需要句法树；句法树是一个离散的决策结果，无法连续地影响损失函数，也就无法简单地利用反向传播训练Recursive...</p></div></div></a></div><div class="card"> <a href="/posts/HowNet/"><div class="card-body"> <em class="small" data-ts="1663121456" data-df="YYYY-MM-DD" > 2022-09-14 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>HowNet知网</h3><div class="text-muted small"><p> HowNet知网 机构：清华大学人工智能研究院 定义： HowNet是董振东先生、董强先生父子毕三十年之功标注的大型语言知识库，主要面向中文（也包括英文）的词汇与概念。HowNet秉承还原论思想，认为词汇/词义可以用更小的语义单位来描述。这种语义单位被称为“义原”（Sememe），顾名思义就是原子语义，即最基本的、不宜再分割的最小语义单位。在不断标注的过程中，HowNet逐渐构建出了一套精...</p></div></div></a></div><div class="card"> <a href="/posts/PaperNote_%E8%AF%8D%E6%A0%BC%E6%B3%95/"><div class="card-body"> <em class="small" data-ts="1665209456" data-df="YYYY-MM-DD" > 2022-10-08 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Paper Note:词格法</h3><div class="text-muted small"><p> Paper Note:词格法 《Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models》 作者：Yuxuan Lai, Yijia Liu, Yansong Feng, Songfang Huang, Dongyan Zhao 阿里巴巴达摩院 | 中国...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/HowNet/" class="btn btn-outline-primary" prompt="上一篇"><p>HowNet知网</p></a> <a href="/posts/%E8%AE%B0%E5%BD%95-%E8%81%94%E9%80%9A2022%E7%BD%91%E7%BB%9CAI%E6%8A%80%E8%83%BD%E5%A4%A7%E8%B5%9B/" class="btn btn-outline-primary" prompt="下一篇"><p>记录：联通2022网络AI技能大赛</p></a></div><script src="https://utteranc.es/client.js" repo="zoeChen119/zoeChen119.github.io" issue-term="title" crossorigin="anonymous" async> </script> <script type="text/javascript"> $(function() { const origin = "https://utteranc.es"; const iframe = "iframe.utterances-frame"; const lightTheme = "github-light"; const darkTheme = "github-dark"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } addEventListener("message", (event) => { let theme; /* credit to <https://github.com/utterance/utterances/issues/170#issuecomment-594036347> */ if (event.origin === origin) { /* page initial */ theme = initTheme; } else if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); } else { return; } const message = { type: "set-theme", theme: theme }; const utterances = document.querySelector(iframe).contentWindow; utterances.postMessage(message, origin); }); }); </script></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://github.com/zoeChen119">陈政伊</a>. <span data-toggle="tooltip" data-placement="top" title="除非另有说明，本网站上的博客文章均由作者按照知识共享署名 4.0 国际 (CC BY 4.0) 许可协议进行授权。">保留部分权利。</span></p></div><div class="footer-right"><p class="mb-0"> 本站由 <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> 生成，采用 <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> 主题。</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">热门标签</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/">技术综述</a> <a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> <a class="post-tag" href="/tags/rag/">RAG</a> <a class="post-tag" href="/tags/%E8%AF%AD%E8%A8%80%E5%AD%A6/">语言学</a> <a class="post-tag" href="/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/">预训练模型</a> <a class="post-tag" href="/tags/bert/">BERT</a> <a class="post-tag" href="/tags/courses/">Courses</a> <a class="post-tag" href="/tags/chatgpt/">ChatGPT</a> <a class="post-tag" href="/tags/mamba/">Mamba</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">发现新版本的内容。</p><button type="button" class="btn btn-primary" aria-label="Update"> 更新 </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">搜索结果为空</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/zh.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
