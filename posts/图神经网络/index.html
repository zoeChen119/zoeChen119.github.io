<!DOCTYPE html><html lang="zh-CN" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="图神经网络" /><meta property="og:locale" content="zh_CN" /><meta name="description" content="图神经网络" /><meta property="og:description" content="图神经网络" /><link rel="canonical" href="https://zoechen119.github.io/posts/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" /><meta property="og:url" content="https://zoechen119.github.io/posts/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" /><meta property="og:site_name" content="zoe Chen" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-10-20T13:56:56+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="图神经网络" /><meta name="twitter:site" content="@twitter_username" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-02-06T11:45:57+08:00","datePublished":"2022-10-20T13:56:56+08:00","description":"图神经网络","headline":"图神经网络","mainEntityOfPage":{"@type":"WebPage","@id":"https://zoechen119.github.io/posts/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},"url":"https://zoechen119.github.io/posts/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}</script><title>图神经网络 | zoe Chen</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="zoe Chen"><meta name="application-name" content="zoe Chen"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/img/avatar.jfif" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">zoe Chen</a></div><div class="site-subtitle font-italic">nlper, dler, sims4er</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>首页</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>分类</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>标签</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>归档</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>关于</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/zoeChen119" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['zoe9698','163.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> 首页 </a> </span> <span>图神经网络</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> 文章</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="搜索..."> </span> <span id="search-cancel" >取消</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>图神经网络</h1><div class="post-meta text-muted"> <span> 发表于 <em class="" data-ts="1666245416" data-df="YYYY-MM-DD" data-toggle="tooltip" data-placement="bottom"> 2022-10-20 </em> </span> <span> 更新于 <em class="" data-ts="1707191157" data-df="YYYY-MM-DD" data-toggle="tooltip" data-placement="bottom"> 2024-02-06 </em> </span><div class="d-flex justify-content-between"> <span> 作者 <em> <a href="https://github.com/zoeChen119">陈政伊</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="4751 字"> <em>26 分钟</em>阅读</span></div></div></div><div class="post-content"><h1 id="图神经网络">图神经网络</h1><p>记录日期：2022/10/20</p><hr /><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>📃参考文章
[^GCN&amp;GNN]:https://blog.csdn.net/kc7w91/article/details/121595567
</pre></table></code></div></div><h4 id="欧几里得数据非欧几里得数据"><span class="mr-2">欧几里得数据&amp;非欧几里得数据：</span><a href="#欧几里得数据非欧几里得数据" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><ul><li>欧几里得数据：排列整齐的数据</ul><blockquote><p>这类型的数据排列整齐，不同样本之间可以容易的定义出“距离”这个概念出来。我们且思考，假设现在有两个图片样本，尽管其图片大小可能不一致，但是总是可以通过空间下采样的方式将其统一到同一个尺寸的，然后直接逐个像素点进行相减后取得平方和，求得两个样本之间的欧几里德距离是完全可以进行的。</p></blockquote><ul><li><p>非欧几里得数据:排列不整齐的数据</p><blockquote><p>对于数据中的某个点，难以定义出其邻居节点出来，或者是不同节点的邻居节点的数量是不同的[5]，这个其实是一个特别麻烦的问题，因为这样就意味着难以在这类型的数据上定义出和图像等数据上相同的卷积操作出来，而且因为每个样本的节点排列可能都不同，比如在生物医学中的分子筛选中，显然这个是一个Graph数据的应用，但是我们都明白，不同的分子结构的原子连接数量，方式可能都是不同的，因此难以定义出其欧几里德距离出来，这个是和我们的欧几里德结构数据明显不同的。<strong>因此这类型的数据不能看成是在欧几里德样本空间中的一个样本点了，而是要想办法将其嵌入(embed)到合适的欧几里德空间后再进行度量。而我们现在流行的Graph Neural Network便可以进行这类型的操作。</strong></p></blockquote></ul><p>如何处理 非欧式空间 的结构（典型：社交网络）？——引入了GNN与GCN网络：利用『边的信息』对『节点信息』进行『聚合』从而生成新的『节点表示』。</p><h2 id="图卷积网络gcn"><span class="mr-2">图卷积网络GCN</span><a href="#图卷积网络gcn" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>CNN中的卷积是一种离散卷积（即加权求和），本质上就是利用一个共享参数的过滤器（kernel），通过计算中心像素点以及相邻像素点的加权和来构成feature map、实现空间特征的提取，其中的加权系数就是卷积核的权重系数(W)。</p><h2 id="图神经网络gnn自然语言处理中的图神经网络"><span class="mr-2">图神经网络GNN[^自然语言处理中的图神经网络]</span><a href="#图神经网络gnn自然语言处理中的图神经网络" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>NLP任务中，文本序列是由<strong>token包/词袋</strong>，例如BoW,TF-IDF</p><blockquote><p>bag of tokens：最简单的方式是将自然语言表示为一袋tokens，这种方式完全忽略了文本中tokens出现的具体位置，而只考虑其在文本中出现的次数。采取这种观点的最具代表性的NLP技术是话题模型。[^面向NLP的GNNs论文解读]</p></blockquote><p>-&gt;而随着<strong>词嵌入</strong>技术的成功，句子被表示为一个tokens序列,它通过预测目标单词的上下文单词来学习单词嵌入。 -&gt;利用文本序列中的<strong>句子结构信息</strong>，即依存树和选区分析树等句法分析树，通过以下2种方法</p><ol><li>合并<strong>特定任务的知识</strong>来扩充原始序列数据。<li>用<strong>序列数据中的语义信息</strong>（即语义解析图，如抽象意义表示图和信息提取图）来增强原始序列数据<blockquote><p>常见的文本或知识的图表示包括依赖图、成分图、AMR图、IE图、词汇网络和知识图谱。此外，还可以构建一个包含多个层次元素的文本图，如文档、段落、句子和单词。与上述两种观点相比，这种自然语言的观点能够捕捉到文本元素之间更丰富的关系。许多传统的基于图的方法（如随机游走、标签传播）已经成功地应用于具有挑战性的NLP问题，包括词义消歧、命名消歧、共指解决、情感分析和文本聚类。</p></blockquote></ol><p>然后，这些图结构的数据可以编码实体tokens之间复杂的成对关系，以便学习更多的语义。</p><p><strong>But</strong> 基于序列数据的深度学习网络不能直接适用于图结构数据。</p><font color="ForestGreen">因此产生了以下挑战： * **<font color="ForestGreen">如何自动将原始文本序列数据转换为高度图结构化的数据</font>** * **<font color="ForestGreen">如何正确地确定图表示学习技术</font>**:需要使用专门设计的GNN来学习不同图结构数据的独特特征，如无向、有向、多关系和异质图。 * **<font color="ForestGreen">如何高效地建模复杂数据</font>**:因为许多NLP任务涉及学习基于图的输入和高度结构化的输出数据之间的映射，如序列、树，以及具有多种类型节点和边的图数据。</font><h3 id="research-background"><span class="mr-2">Research Background</span><a href="#research-background" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h4 id="传统基于图的算法"><span class="mr-2">传统基于图的算法</span><a href="#传统基于图的算法" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><ol><li>基于随机游走的图算法<li>图聚类算法<li>图匹配算法<li>标签传播算法</ol><p>这些传统的基于图的方法在NLP领域有其局限性，比如表征能力弱以及没有统一的学习架构。</p><h4 id="图神经网络基础"><span class="mr-2">图神经网络基础</span><a href="#图神经网络基础" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>:cat:输入的是图。</p><h5 id="graph-filtering"><span class="mr-2">Graph filtering</span><a href="#graph-filtering" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p><img data-src="/assets/img/2022-10-20-图神经网络/2022-10-20-17-56-53.png" alt="" data-proofer-ignore></p><center>图谱过滤的分类以及一些代表方法</center><ol><li>spectral-based:基于谱图理论<li>spatial-based:利用其空间近邻节点来计算该节点的嵌入表示<li>attention-based：将不同的注意力权重分配给不同的邻居节点<li>recurrent-based：门控机制，模型参数在不同的GNN层之间共享</ol><h5 id="graph-pooling"><span class="mr-2">Graph pooling</span><a href="#graph-pooling" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>图谱池化层是为了以图为中心的下游任务来生成图级的嵌入表示。 例如：图的分类-依据从图谱过滤中学习到的节点嵌入表示来进行预测的。 （这是因为对于以节点为中心的任务来说，学习到的节点嵌入表示已经足够了。然而，对于以图为中心的任务，需要整个图的嵌入表示。） 池化操作就是为了总结节点嵌入信息和图结构信息。 图池化层可以分为两类：平面图池化（flat pooling）和分层图图池化（hierarchical pooling）。</p><ol><li>flat pooling 平面图池化直接从每步中的节点单独嵌入。<li>hierarchical pooling 分层图池化包含几个图池化层，每个池化层之后又有好多图过滤器。</ol><h3 id="面向nlp的gnns分类面向nlp的gnns分类"><span class="mr-2">面向NLP的GNNs分类[^面向NLP的GNNs分类]</span><a href="#面向nlp的gnns分类面向nlp的gnns分类" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="/assets/img/2022-10-20-图神经网络/2022-10-20-17-49-35.png" alt="" data-proofer-ignore></p><center>图1. NLP中GNNs三个方面的研究以及GNN的应用</center><h4 id="图的构造"><span class="mr-2">图的构造</span><a href="#图的构造" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>:wind_chime:输入是文本序列</p><p>:interrobang:How？ 文本序列——-》图输入</p><p>构建方法合集： <img data-src="/assets/img/2022-10-20-图神经网络/2022-10-20-19-32-38.png" alt="" data-proofer-ignore></p><h5 id="1静态图构建"><span class="mr-2">1.静态图构建</span><a href="#1静态图构建" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>利用现有的<strong>关系解析工具</strong>（例如：依存解析）或<strong>手动定义的规则</strong>在预处理过程中构造图结构。 静态图可以为原始文本序列引入隐藏的领域或外部知识，用丰富的结构化信息来扩充原始文本。</p><ul><li>dependency graph:</ul><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre>依赖图通常用于捕捉给定句子中不同对象之间的依赖关系。
具体流程为：给定一个段落---》使用各种NLP解析工具（例如Stanford CoreNLP）获得依存分析树（例如句法依存树或语义依存关系树）---》从依存分析树中提取依赖关系，并将依赖关系转换为依赖性图。

PS.文本有“顺序信息”，而图节点是无序的，可以引入顺序的link，在图结构中保留“顺序信息”。
</pre></table></code></div></div><p>概括一下流程为： 1）构建依赖关系，2）构建顺序关系，3）最终的图转换。</p><ul><li>constituency graph</ul><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>成分图通常用于捕捉一个或多个句子中基于短语的句法关系。

PS.与依存分析不同的是，依存分析只关注单个词之间一对一的对应关系（即词级），而构成分析则对一个或几个对应词的组合进行建模（即短语级）。
</pre></table></code></div></div><p>成分关系的基本概念：在语言学中，成分关系是指<strong>遵循短语结构语法的关系</strong>，而不是依存关系和依存语法。一般来说，成分关系是由主语（名词短语NP）— 谓语（动词短语VP）关系衍生出来的。</p><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>PS.与依存关系解析树不同的是，所有的节点都有相同的类型，成分分析树区分了终端节点和非终端节点，非终端类别标记分析树的内部节点，叶子节点标记为终端类别。
</pre></table></code></div></div><p>节点集可以表示为：1）非终端节点集2) 终端节点集。 构成关系集合与树的边相关。成分图由非终端节点和终端节点组成，以及成分边和序列边。对于原文中相邻的每个单词节点对，在它们之间添加一条具有特定序列类型的无向边，用来保留顺序信息。</p><p><img data-src="/assets/img/2022-10-20-图神经网络/2022-10-20-19-38-24.png" alt="" data-proofer-ignore></p><center>图2 Dependency Graph and Constituency Graph Construction</center><ul><li>AMR graph AMR图是有根、有标注、有向、无环的图，它被广泛用于表示非结构化的具体自然文本的抽象概念之间的高级语义关系。 :yellow_heart:与句法上的特异性不同，AMR是高层次的语义抽象。:yellow_heart: 更具体地说，在语义上相似的不同句子可能共享相同的AMR解析结果，例如，”保罗描述自己是一个战士 “和 “保罗对自己的描述：一个战士”，如图3所示。与之前介绍的依赖树和成分树类似，AMR图是由AMR解析树衍生出来的。</ul><p><img data-src="/assets/img/2022-10-20-图神经网络/2022-10-20-19-38-36.png" alt="" data-proofer-ignore></p><ul><li>信息抽取图构建（Information Extraction Graph Construction）</ul><p><img data-src="/assets/img/2022-10-20-图神经网络/2022-10-20-20-52-36.png" alt="" data-proofer-ignore></p><center>图4 Information Extraction Graph</center><p>信息抽取图（IE Graph）旨在提取结构信息来表示自然句子之间的高级信息，例如基于文本的文档。 这些提取出来的关系，捕捉到远距离句子之间的关系，在许多NLP任务中很有帮助。在下文中，为给定段落构建IE图的过程分为三个基本步骤。1）指代消解，2）构建IE关系，3）图的构建。</p><ul><li><p>话语图构建（Discourse Graph Construction） 当候选文档太长时，许多NLP任务会受到长距离依赖性的挑战。话语图描述了两个句子之间的逻辑联系，可以有效地解决这种挑战。</p><li><p>知识图谱构建（Knowledge Graph Construction） <img data-src="/assets/img/2022-10-20-图神经网络/2022-10-20-19-38-48.png" alt="" data-proofer-ignore></p></ul><p>捕捉实体和关系的知识图谱（KG）可以大大促进许多NLP应用中的学习和推理。KG可以表示为G(V, E)，它通常由知识库中的元素构建。形式上，定义三元组作为知识库的基本元素，包括是源实体，目标实体和关系类型。然后，在知识库中添加两个节点，即源节点和目标节点，并从节点v1到节点v2添加一条边类型为rel的有向边。</p><p>构建KG的第一件事是获取给定查询中的术语实例。然后，通过一些匹配算法（如最大子串匹配）将术语实例与KG中的概念联系起来。这些概念被看作是提取的子图中的初始节点。下一步是获取初始节点在KG中的1跳邻居。此外，人们可以通过应用一些图节点相关性模型，如个性化的PageRank（PPR）算法，来计算邻居与初始节点的相关性。然后根据结果，进一步修剪出相关性分数低于置信度阈值的边，并删除孤立的邻居。剩余的最终子图随后被用来给任何图表示学习模块提供信息。</p><ul><li>similarity graph <img data-src="/assets/img/2022-10-20-图神经网络/2022-10-20-20-57-03.png" alt="" data-proofer-ignore></ul><p>相似性图旨在量化节点之间的相似性，在许多NLP任务中被广泛使用。 由于相似性图通常是面向应用的，因此我们重点关注构建实体、句子和文档等各种类型元素的相似性图的基本程序，而忽略了具体的应用细节。 <strong>相似性图的构建是在预处理过程中进行的，而不是以端到端的方式与其余学习系统共同训练。</strong> 图6中显示了一个相似性图的例子。</p><ul><li><p>共指图构建（Coreference Graph Construction） 在语言学中，当某个段落中的两个或多个术语指代同一个对象时，就会出现共指。许多工作表明，这种现象有助于更好地理解语料库的复杂结构和逻辑，解决歧义。为了有效地利用共指信息，共指图用来显式的建模隐性共指关系。给定一组短语，共指图可以连接文本语料库中指代同一实体的节点（短语）。</p><li>共现图构建（Co-occurrence Graph Construction） <img data-src="/assets/img/2022-10-20-图神经网络/2022-10-20-20-58-58.png" alt="" data-proofer-ignore> 共现图旨在捕捉文本中词与词之间的共现关系，这在许多NLP任务中被广泛使用，共现关系描述了在固定大小的上下文窗口内共同出现的两个词的频率，是捕捉语料库中词之间语义关系的一个重要特征。共现图的例子见图7<li>topic graph <img data-src="/assets/img/2022-10-20-图神经网络/2022-10-20-20-59-42.png" alt="" data-proofer-ignore></ul><p>话题图是建立在几个文档上的，其目的是<strong>对不同话题之间的高层次语义关系进行建模</strong>。 给定一组文档，首先用一些话题建模算法，如LDA，学习潜在的话题。 然后构建话题图，只有当文档具有该话题时，那么在文档节点和话题节点之间构建一条无向边。</p><ul><li>应用驱动图构建（App-driven Graph Construction） <img data-src="/assets/img/2022-10-20-图神经网络/2022-10-20-21-00-59.png" alt="" data-proofer-ignore></ul><p>应用驱动图指的是为<strong>特定的NLP任务专门设计的图</strong>。在一些NLP任务中，用特定应用的方法通过结构化的形成来表示非结构化的数据是很常见的。例如，SQL语言可以自然地通过SQL解析树来表示。因此，它可以被转换为SQL图。由于这些图在领域知识的基础上过于专业化，所以没有统一的模式来总结如何建立一个应用驱动的图。图9是这种应用驱动图的一个例子，如SQL图。</p><hr /><p>大部分静态图构造方法只考虑节点之间的某些特定关系，构建的图在一定程度上很好地捕捉到了结构信息，但没法直接利用多种不同类型的图关系。</p><p>:interrobang:How？ 将多个图组合起来构建混合图？（这样能丰富图中的语义信息）</p><p>静态图构建的问题/缺陷： 首先，为了构建合理性能的图拓扑结构，需要广泛的人力和专业知识领域。 其次，手动构建的图结构可能容易出错（例如，嘈杂或不完整性）。 第三，由于图构建阶段和图表示学习阶段是不相交互的，因此在图构建阶段引入的错误无法被纠正，可能会累积到以后阶段，进而会导致性能下降。 最后，图的构建通常是由机器学习从业者提供见解信息，这对下游预测任务可能不是最理想的。</p><p>为了解决上述问题，提出了动态图构建方法:point_down:</p><h5 id="2动态图构建"><span class="mr-2">2.动态图构建</span><a href="#2动态图构建" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h5><p>大多数动态图构建方法旨在动态地学习图结构（即加权邻接矩阵），图构建模块可以与后续的图表示学习模块共同优化，从而以端到端的方式完成下游任务。</p><p>下面两个组件是动态图构建的两个部分，（不是两种方法）</p><ul><li><p>图谱相似性度量学习方法similarity metric learning 图相似度度量学习组件用于学习嵌入空间中的成对节点相似性的邻接矩阵</p><li><p>图谱稀疏化方法graph sparsity 稀疏化组件用于从全连接图中提取稀疏图。</p></ul><p><img data-src="/assets/img/2022-10-20-图神经网络/2022-10-20-19-50-50.png" alt="" data-proofer-ignore></p><h4 id="图表示学习"><span class="mr-2">图表示学习</span><a href="#图表示学习" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>构建好了图，接下来利用各种<strong>图表示学习技术</strong>进行学习和特征提取。</p><p>目标：找到一种方法可以通过机器学习模型将<strong>图结构</strong>和<strong>其属性的信息</strong>合并到低维嵌入中。</p><p>原始文本数据构建的图要么是“异构图”要么是“同构图”。 <img data-src="/assets/img/2022-10-20-图神经网络/2022-10-20-20-01-10.png" alt="" data-proofer-ignore></p><ul><li>同构图的各种图表示学习方法<ul><li>原始同构图的场景<li>从异构图转换的场景</ul><li>用于多关系图的基于GNN的方法<li>异构图的基于GNN方法</ul><h4 id="基于图的encoder-decoder模型"><span class="mr-2">基于图的encoder-decoder模型</span><a href="#基于图的encoder-decoder模型" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p><img data-src="/assets/img/2022-10-20-图神经网络/2022-10-20-20-04-40.png" alt="" data-proofer-ignore></p><center>包含了Graph2Seq和Graph2Tree的编码器-解码器GNN模型架构</center><ul><li><p>Graph2Seq 通常采用基于 GNN 的编码器和基于 RNN/Transformer的解码器 与 Seq2Seq模型架构相比，Graph2Seq 更善于捕捉输入文本的丰富结构信息，可应用于任意图结构数据，而且，Graph2Seq 模型在更广泛的 NLP 任务，包括神经机器翻译，文本摘要，问题生成，图谱到文本，SQL到文本，代码摘要以及语义解析。</p><li><p>Graph2Tree 结合输入端和输出端的信息，使编码解码过程中的信息流更加完整。 与考虑输入端结构信息的 Graph2Seq 模型相比，许多NLP 任务还包含复杂结构表示的输出，例如树结构输出（语法解析，语义解析等），它们也是结构信息丰富输出端。</p></ul><h4 id="gnns助力nlp应用场景"><span class="mr-2">GNNs助力NLP应用场景</span><a href="#gnns助力nlp应用场景" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p><img data-src="/assets/img/2022-10-20-图神经网络/2022-10-20-20-09-31.png" alt="" data-proofer-ignore></p><center>GNN在NLP中的应用</center><h4 id="基准数据集评估指标"><span class="mr-2">基准数据集，评估指标</span><a href="#基准数据集评估指标" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><h4 id="gnns-for-nlp的挑战和未来研究方向"><span class="mr-2">GNNs for NLP的挑战和未来研究方向</span><a href="#gnns-for-nlp的挑战和未来研究方向" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><h2 id="reference"><span class="mr-2">Reference</span><a href="#reference" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/nlp/'>NLP</a>, <a href='/categories/%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6/'>模型研究</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" class="post-tag no-text-decoration" >图神经网络</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> 本文由作者按照 <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> 进行授权</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">分享</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C+-+zoe+Chen&url=https%3A%2F%2Fzoechen119.github.io%2Fposts%2F%25E5%259B%25BE%25E7%25A5%259E%25E7%25BB%258F%25E7%25BD%2591%25E7%25BB%259C%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C+-+zoe+Chen&u=https%3A%2F%2Fzoechen119.github.io%2Fposts%2F%25E5%259B%25BE%25E7%25A5%259E%25E7%25BB%258F%25E7%25BD%2591%25E7%25BB%259C%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fzoechen119.github.io%2Fposts%2F%25E5%259B%25BE%25E7%25A5%259E%25E7%25BB%258F%25E7%25BD%2591%25E7%25BB%259C%2F&text=%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C+-+zoe+Chen" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="分享链接" data-title-succeed="链接已复制！"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">最近更新</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/C25_11%E5%85%A5%E9%97%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%860/">【NLP入门趣味题】肉眼找朋友</a><li><a href="/posts/C25_11%E5%85%A5%E9%97%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%861/">【NLP入门趣味题】探索语言模型与词向量</a><li><a href="/posts/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a><li><a href="/posts/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%AE%9E%E9%AA%8C/">多模态实验</a><li><a href="/posts/%E5%A4%9A%E6%A8%A1%E6%80%81RAG/">多模态rag</a></ul></div><div id="access-tags"><div class="panel-heading">热门标签</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/">技术综述</a> <a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> <a class="post-tag" href="/tags/rag/">RAG</a> <a class="post-tag" href="/tags/%E8%AF%AD%E8%A8%80%E5%AD%A6/">语言学</a> <a class="post-tag" href="/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/">预训练模型</a> <a class="post-tag" href="/tags/bert/">BERT</a> <a class="post-tag" href="/tags/courses/">Courses</a> <a class="post-tag" href="/tags/chatgpt/">ChatGPT</a> <a class="post-tag" href="/tags/mamba/">Mamba</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">文章内容</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>相关文章</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/BERT%E7%99%BE%E7%A7%91/"><div class="card-body"> <em class="small" data-ts="1649913056" data-df="YYYY-MM-DD" > 2022-04-14 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>BERT百科大全</h3><div class="text-muted small"><p> BERT百科大全 Attention attention结构的神经网络与递归神经网络相比有什么优点 答： attention结构的输入是句子即可，递归神经网络的输入需要包含句子和句子结构 训练Recursive neural net之前，你需要句法树；句法树是一个离散的决策结果，无法连续地影响损失函数，也就无法简单地利用反向传播训练Recursive...</p></div></div></a></div><div class="card"> <a href="/posts/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/"><div class="card-body"> <em class="small" data-ts="1663121456" data-df="YYYY-MM-DD" > 2022-09-14 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>深入研究BERT位置编码的原理</h3><div class="text-muted small"><p> 深入研究BERT位置编码的原理 记录日期：2022/08/29 📃参考文章 [1] 苏建林博客 [2] Transformer学习笔记一：Positional Encoding（位置编码） [3] 知乎.实践中BERT如何对长度大于500的文本进行处理？ [4] 位置编码全面研究-好文！💎 [5] how to generate po...</p></div></div></a></div><div class="card"> <a href="/posts/Mamba/"><div class="card-body"> <em class="small" data-ts="1701705600" data-df="YYYY-MM-DD" > 2023-12-05 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Mamba</h3><div class="text-muted small"><p> Mamba：Transformer的挑战者 Mamba: Linear-Time Sequence Modeling with Selective State Spaces Mamba：具有选择性状态空间的线性时间序列建模 摘要： 目前深度学习的应用都是基于 Transformer架构（及其注意力模块）。Transformer的一个致命问题是不善于长序列计算。有许多subquadra...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/PaperNote_%E8%AF%8D%E6%A0%BC%E6%B3%95/" class="btn btn-outline-primary" prompt="上一篇"><p>Paper Note:词格法</p></a> <a href="/posts/PaperNote_KEPLER/" class="btn btn-outline-primary" prompt="下一篇"><p>KEPLER：知识嵌入和预训练语言表示的统一模型</p></a></div><script src="https://utteranc.es/client.js" repo="zoeChen119/zoeChen119.github.io" issue-term="title" crossorigin="anonymous" async> </script> <script type="text/javascript"> $(function() { const origin = "https://utteranc.es"; const iframe = "iframe.utterances-frame"; const lightTheme = "github-light"; const darkTheme = "github-dark"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } addEventListener("message", (event) => { let theme; /* credit to <https://github.com/utterance/utterances/issues/170#issuecomment-594036347> */ if (event.origin === origin) { /* page initial */ theme = initTheme; } else if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); } else { return; } const message = { type: "set-theme", theme: theme }; const utterances = document.querySelector(iframe).contentWindow; utterances.postMessage(message, origin); }); }); </script></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://github.com/zoeChen119">陈政伊</a>. <span data-toggle="tooltip" data-placement="top" title="除非另有说明，本网站上的博客文章均由作者按照知识共享署名 4.0 国际 (CC BY 4.0) 许可协议进行授权。">保留部分权利。</span></p></div><div class="footer-right"><p class="mb-0"> 本站由 <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> 生成，采用 <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> 主题。</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">热门标签</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/">技术综述</a> <a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> <a class="post-tag" href="/tags/rag/">RAG</a> <a class="post-tag" href="/tags/%E8%AF%AD%E8%A8%80%E5%AD%A6/">语言学</a> <a class="post-tag" href="/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/">预训练模型</a> <a class="post-tag" href="/tags/bert/">BERT</a> <a class="post-tag" href="/tags/courses/">Courses</a> <a class="post-tag" href="/tags/chatgpt/">ChatGPT</a> <a class="post-tag" href="/tags/mamba/">Mamba</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">发现新版本的内容。</p><button type="button" class="btn btn-primary" aria-label="Update"> 更新 </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">搜索结果为空</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/zh.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
