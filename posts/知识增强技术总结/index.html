<!DOCTYPE html><html lang="zh-CN" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="知识增强技术总结" /><meta property="og:locale" content="zh_CN" /><meta name="description" content="知识增强的价值关键 预训练语言模型目前的瓶颈： 不可解释，黑盒模型 下游任务需要大量标注数据 虽然少样本中人工给出思维链提示的成本很小，但这种注释成本相对于微调还是令人望而却步（也可以用synthetic data generation合成数据生成, or zero-shot generalization零样本泛化来处理这个问题）。 推理能力差" /><meta property="og:description" content="知识增强的价值关键 预训练语言模型目前的瓶颈： 不可解释，黑盒模型 下游任务需要大量标注数据 虽然少样本中人工给出思维链提示的成本很小，但这种注释成本相对于微调还是令人望而却步（也可以用synthetic data generation合成数据生成, or zero-shot generalization零样本泛化来处理这个问题）。 推理能力差" /><link rel="canonical" href="https://zoechen119.github.io/posts/%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/" /><meta property="og:url" content="https://zoechen119.github.io/posts/%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/" /><meta property="og:site_name" content="zoe Chen" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-03-15T00:00:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="知识增强技术总结" /><meta name="twitter:site" content="@twitter_username" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-03-15T00:00:00+08:00","datePublished":"2023-03-15T00:00:00+08:00","description":"知识增强的价值关键 预训练语言模型目前的瓶颈： 不可解释，黑盒模型 下游任务需要大量标注数据 虽然少样本中人工给出思维链提示的成本很小，但这种注释成本相对于微调还是令人望而却步（也可以用synthetic data generation合成数据生成, or zero-shot generalization零样本泛化来处理这个问题）。 推理能力差","headline":"知识增强技术总结","mainEntityOfPage":{"@type":"WebPage","@id":"https://zoechen119.github.io/posts/%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"},"url":"https://zoechen119.github.io/posts/%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/"}</script><title>知识增强技术总结 | zoe Chen</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="zoe Chen"><meta name="application-name" content="zoe Chen"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/img/avatar.jfif" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">zoe Chen</a></div><div class="site-subtitle font-italic">nlper, dler, sims4er</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>首页</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>分类</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>标签</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>归档</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>关于</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/zoeChen119" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['zoe9698','163.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> 首页 </a> </span> <span>知识增强技术总结</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> 文章</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="搜索..."> </span> <span id="search-cancel" >取消</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>知识增强技术总结</h1><div class="post-meta text-muted"> <span> 发表于 <em class="" data-ts="1678809600" data-df="YYYY-MM-DD" data-toggle="tooltip" data-placement="bottom"> 2023-03-15 </em> </span><div class="d-flex justify-content-between"> <span> 作者 <em> <a href="https://github.com/zoeChen119">陈政伊</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1824 字"> <em>10 分钟</em>阅读</span></div></div></div><div class="post-content"><h1 id="知识增强的价值关键">知识增强的价值关键</h1><p>预训练语言模型目前的瓶颈：</p><ol><li>不可解释，黑盒模型<li>下游任务需要大量标注数据 虽然少样本中人工给出思维链提示的成本很小，但这种注释成本相对于微调还是令人望而却步（也可以用synthetic data generation合成数据生成, or zero-shot generalization零样本泛化来处理这个问题）。<li>推理能力差</ol><p>ChatGPT和之前PLM的创新：</p><ol><li>小样本提示学习和指令学习<li>思维链（Chain of Thought，COT）补充了逻辑推理过程（知识）给模型<li>基于人类反馈的强化学习（Reinforcement Learning with Human Feedback，RLHF）<li>训练数据中补充了过程性知识（代码）</ol><p>ChatGPT在专业性强的问题上“一本正经的胡说八道”</p><ol><li>提高结果的可信度【知识】<li>提高推理能力，改善模型的可解释性差的问题【知识】 <img data-src="E:\Zoe\zoeChen119.github.io\assets\img023-03-15-知识增强技术总结.assets023-03-15-11-16-55.png" alt="" data-proofer-ignore></ol><h1 id="一技术研究知识增强">一、技术研究：知识增强</h1><h2 id="what知识是什么"><span class="mr-2">【What】知识是什么？</span><a href="#what知识是什么" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ol><li><p>从业务的角度：事实知识（陈述性知识）、机理知识（过程性知识）、数据知识</p><li><p>从研究的角度：“知识”有两种不同的分类方法[灰色色块为研究中常用的，可实现的数据类型] 2.1. 按照不同来源分类：内部知识、外部知识 <img data-src="E:\Zoe\zoeChen119.github.io\assets\img023-03-15-知识增强技术总结.assets023-03-15-10-39-43.png" alt="" data-proofer-ignore></p></ol><p>2.2. 按照不同性质分类 [根据北大《人工智能原理》] “可变性”：那么知识分为静态/动态； “可理解性”：那么知识分为表层/深层； “内容的性质”：那么知识分为陈述性/过程性； … <img data-src="E:\Zoe\zoeChen119.github.io\assets\img023-03-15-知识增强技术总结.assets023-03-15-10-40-06.png" alt="" data-proofer-ignore></p><h2 id="how如何表征知识"><span class="mr-2">【How】如何表征知识？</span><a href="#how如何表征知识" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="E:\Zoe\zoeChen119.github.io\assets\img023-03-15-知识增强技术总结.assets023-03-15-10-40-14.png" alt="" data-proofer-ignore> 根据上面的分类，内部或外部知识通常用(1)实体（三元组中的实体）词典；(2)知识图谱；(3)纯文本直接作为补充知识；(4)与上下文有关系的图像。</p><h3 id="确定性知识和不确定性知识"><span class="mr-2">确定性知识和不确定性知识</span><a href="#确定性知识和不确定性知识" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h4 id="确定性知识"><span class="mr-2">确定性知识：</span><a href="#确定性知识" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>通常用(1)语义网络；(2)知识图谱；(3)框架语言frame；(4)一阶逻辑；(5)命题逻辑；(6)模态逻辑；(7)描述逻辑；(8)本体…，他们分别由各自的使用情况和局限性。 <img data-src="E:\Zoe\zoeChen119.github.io\assets\img023-03-15-知识增强技术总结.assets023-03-15-10-40-23.png" alt="" data-proofer-ignore></p><h4 id="不确定性知识"><span class="mr-2">不确定性知识：</span><a href="#不确定性知识" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>不确定性知识指的是不精确（imprecise）、不完全（incomplete）、随机性（stochastic）的知识。 表示方法如下 <img data-src="E:\Zoe\zoeChen119.github.io\assets\img023-03-15-知识增强技术总结.assets023-03-15-10-40-52.png" alt="" data-proofer-ignore></p><h3 id="过程性知识和陈述性知识"><span class="mr-2">过程性知识和陈述性知识</span><a href="#过程性知识和陈述性知识" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><blockquote><p>过程性知识描述“怎么做”的知识，描述解决问题的过程，通常也是从已有的知识中整理出来的规则，它具有动态的特征，不同情况不同任务下动态变化。 陈述性知识描述“是什么”的知识，往往是事实性知识，包括事物、事件、过程描述、属性、关系这些知识。</p></blockquote><div class="table-wrapper"><table><tr><td bgcolor="PowderBlue">💡 ChatGPT引入了代码数据作为预训练数据</table></div><h2 id="keplm引入knowledge到plm"><span class="mr-2">—KEPLM：引入Knowledge到PLM—</span><a href="#keplm引入knowledge到plm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h2 id="how知识增强注入的方法"><span class="mr-2">【How】（知识）增强/注入的方法</span><a href="#how知识增强注入的方法" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>PLM的网络结构层有：Input、Embedding层、Encoder层 PLM的训练任务有：Masked Language 掩码任务和NSP下一句预测任务</p><p align="right">💡这些地方都可以作为知识注入的切入口。</p><h3 id="m1修改input"><span class="mr-2">M1：修改Input</span><a href="#m1修改input" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img data-src="E:\Zoe\zoeChen119.github.io\assets\img023-03-15-知识增强技术总结.assets023-03-15-10-46-04.png" alt="" data-proofer-ignore></p><h4 id="思路1"><span class="mr-2">思路1：</span><a href="#思路1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><ol><li>在知识图谱中找$e1$对应的实体的三元组插入在input文本中<li>把$e1$对应的实体描述插入在input文本中 <img data-src="E:\Zoe\zoeChen119.github.io\assets\img023-03-15-知识增强技术总结.assets023-03-15-10-46-20.png" alt="" data-proofer-ignore> 例子：</ol><details> <summary>2个例子</summary> <center> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 60% 200'%3E%3C/svg%3E" style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" width="60%" height="200" data-src="E:\Zoe\zoeChen119.github.io\assets\img023-03-15-知识增强技术总结/2023-03-15-10-47-44.png" alt="" data-proofer-ignore> <br /><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"> ERNIE 3.0 框架图</div></center> <center> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 60% 200'%3E%3C/svg%3E" style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" width="60%" height="200" data-src="E:\Zoe\zoeChen119.github.io\assets\img023-03-15-知识增强技术总结/2023-03-15-10-52-37.png" alt="" data-proofer-ignore> <br /><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"> 一篇来自中文信息处理实验室的发布在AAAI的论文：Benchmarking Knowledge-Enhanced Commonsense Question Answering via Knowledge-to-Text Transformation.</div></center> </details><h4 id="思路2"><span class="mr-2">思路2：</span><a href="#思路2" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>先把原始input文本组织成图结构，再和来自于知识图谱中的子图拼接（可以根据input中的实体词或者其他），构建成补充后的图结构，再重新展平成文本序列的形式作为PLM的输入。 <img data-src="E:\Zoe\zoeChen119.github.io\assets\img023-03-15-知识增强技术总结.assets023-03-15-10-53-41.png" alt="" data-proofer-ignore></p><details> <summary>2个例子</summary> <center> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 60% 200'%3E%3C/svg%3E" style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" width="60%" height="200" data-src="E:\Zoe\zoeChen119.github.io\assets\img023-03-15-知识增强技术总结/2023-03-15-10-53-59.png" alt="" data-proofer-ignore> <br /><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"> [ACL]CoLAKE 知识增强</div></center> <center> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 60% 200'%3E%3C/svg%3E" style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" width="60%" height="200" data-src="E:\Zoe\zoeChen119.github.io\assets\img023-03-15-知识增强技术总结/2023-03-15-10-54-34.png" alt="" data-proofer-ignore> <br /><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"> [AAAI]K-BERT 知识增强</div></center> </details><h3 id="m2在encoder层中增加知识融合模块"><span class="mr-2">M2：在Encoder层中增加知识融合模块</span><a href="#m2在encoder层中增加知识融合模块" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>(1) on top of the entire PLM：我们可以在n个Encoder整体之后增加一个知识融合模块； (2) between the Transformer layers of PLM：我们也可以单层的Encoder增加知识融合模块，这样n个Encoder就重复n次； (3) inside the Transformer layers of PLM：Encoder包含着很多子层比如多头自注意力层，feed forward层等等，我们也可以在这其中插入知识融合模块。</p><p><img data-src="E:\Zoe\zoeChen119.github.io\assets\img023-03-15-知识增强技术总结.assets023-03-15-10-55-30.png" alt="" data-proofer-ignore></p><details> <summary>三个例子</summary> 在n个Encoder整体之后增加知识融合模块的例子： <center> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 60% 200'%3E%3C/svg%3E" style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" width="60%" height="200" data-src="E:\Zoe\zoeChen119.github.io\assets\img023-03-15-知识增强技术总结/2023-03-15-10-55-44.png" alt="" data-proofer-ignore> <br /><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"> 清华ERNIE</div></center> 在单层的Encoder增加知识融合模块的例子： <center> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 60% 200'%3E%3C/svg%3E" style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" width="60%" height="200" data-src="E:\Zoe\zoeChen119.github.io\assets\img023-03-15-知识增强技术总结/2023-03-15-10-55-50.png" alt="" data-proofer-ignore> <br /><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"></div></center> 在Transformer层内部增加知识融合模块的例子 <center> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 60% 200'%3E%3C/svg%3E" style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" width="60%" height="200" data-src="E:\Zoe\zoeChen119.github.io\assets\img023-03-15-知识增强技术总结/2023-03-15-10-56-12.png" alt="" data-proofer-ignore> <br /><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"> [ACL]KALA</div></center> </details><h3 id="m3增加或修改预训练任务"><span class="mr-2">M3：增加或修改预训练任务</span><a href="#m3增加或修改预训练任务" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>(1) 修改掩码任务 <img data-src="E:\Zoe\zoeChen119.github.io\assets\img023-03-15-知识增强技术总结.assets023-03-15-10-58-39.png" alt="" data-proofer-ignore></p><details> <summary>1个例子</summary> <center> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 60% 200'%3E%3C/svg%3E" style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" width="60%" height="200" data-src="E:\Zoe\zoeChen119.github.io\assets\img023-03-15-知识增强技术总结/2023-03-15-10-59-16.png" alt="" data-proofer-ignore> <br /><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"> [中科院]E-BERT</div></center> </details><p>(2) 增加知识相关的预训练任务 <img data-src="E:\Zoe\zoeChen119.github.io\assets\img023-03-15-知识增强技术总结.assets023-03-15-11-00-00.png" alt="" data-proofer-ignore></p><details> <summary>1个例子</summary> <center> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 60% 200'%3E%3C/svg%3E" style="border-radius: 0.3125em; box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" width="60%" height="200" data-src="E:\Zoe\zoeChen119.github.io\assets\img023-03-15-知识增强技术总结/2023-03-15-11-00-09.png" alt="" data-proofer-ignore> <br /><div style="color:orange; border-bottom: 1px solid #d9d9d9; display: inline-block; color: #999; padding: 2px;"> [ACL Trans]KEPLER</div></center> </details><h2 id="评估如何评估keplm的好坏"><span class="mr-2">【评估】如何评估KEPLM的好坏</span><a href="#评估如何评估keplm的好坏" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>过去，评价KEPLM的优劣通常通过它生成的Representation（表征）的<strong>理解能力</strong>好坏来定义。 <img data-src="E:\Zoe\zoeChen119.github.io\assets\img023-03-15-知识增强技术总结.assets023-03-15-11-00-52.png" alt="" data-proofer-ignore> 现在以及以后，KEPLM的<strong>推理能力</strong>将是我们更关注的点，因为它们已经在GLUE/CLUE任务上表现得足够好了。 <img data-src="E:\Zoe\zoeChen119.github.io\assets\img023-03-15-知识增强技术总结.assets023-03-15-11-00-57.png" alt="" data-proofer-ignore></p><div class="table-wrapper"><table><tr><td bgcolor="PowderBlue">💡 ChatGPT引入了思维链CoT增强PLM的推理能力</table></div><h1 id="reference">Reference</h1><div class="language-plaintext highlighter-rouge"><div class="code-header"> <span data-label-text="Plaintext"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>标题	文献来源	发表年份 1	A Survey on Knowledge-Enhanced Pre-trained Language Models	IEEE TRANS	2023.01 2	A Survey of Knowledge Enhanced Pre-trained Models		2022.06 3	A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models		2022.02 4	知识图谱构建技术综述	计算机工程	2022 5	新一代知识图谱关键技术综述	计算机研究与发展	2022 6	A Survey on Knowledge Graphs: Representation, Acquisition and Applications	IEEE transactions	2021 7	CoLAKE: Contextualized Language and Knowledge Embedding	ACL	2020 8	KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation	Transactions of ACL	2021 9	Reasoning About Knowledge		2003.01 10	On Commonsense Cues in BERT for Solving Commonsense Tasks	ACL-IJCNLP 2021	2021.08 11	What does BERT learn about the structure of language?	ACL	2019.07 12	A Structural Probe for Finding Syntax in Word Representations	NAACL-HLT 2019	2019.06 13	A Closer Look at How Fine-tuning Changes BERT	ACL	2022.03 14	DirectProbe: Studying Representations without Classifiers	ACL	2021.04 15	Enhancing Self-Attention with Knowledge-Assisted Attention Maps	NAACL 2022	 16	SKILL: Structured Knowledge Infusion for Large Language Models	NAACL 2022	 17	KroneckerBERT: Significant Compression of Pre-trained Language Models Through Kronecker Decomposition and Knowledge Distillation	NAACL 2022	 18	Modularized Transfer Learning with Multiple Knowledge Graphs for Zero-shot Commonsense Reasoning	NAACL 2022	 19	KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning	AAAI	2021.01 20	Chain of Thought Prompting Elicits Reasoning in Large Language Models	NeurIPS	2023.01 21	Training Verifiers to Solve Math Word Problems		2021.09 22	JAKET: Joint Pre-training of Knowledge Graph and Language Understanding	AAAI	2021.03 23	Memory and Knowledge Augmented Language Models for Inferring Salience in Long-Form Stories	EMNLP	2021.08 24	Benchmarking Knowledge-Enhanced Commonsense Question Answering via Knowledge-to-Text Transformation.	AAAI	2021.03 25	Entities as Experts: Sparse Memory Access with Entity Supervision	EMNLP	2020 26	ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation		2021 27	AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts	EMNLP	2020.08 28	Semantics-aware BERT for Language Understanding	AAAI	2020.05 29	KALA: Knowledge-Augmented Language Model Adaptation		2022.04 30	Knowledge-driven Natural Language Understanding of English Text and its Applications	AAAI	2021 31	Common Sense or World Knowledge? Investigating Adapter-Based Knowledge Injection into Pretrained Transformers		2020 32	KgPLM: Knowledge-guided Language Model Pre-training via Generative and Discriminative Learning.		2020 33	KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning	AAAI	2020.11 34	SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis		 35	DKPLM: Decomposable Knowledge-enhanced Pre-trained Language Model for Natural Language Understanding	AAAI	2022 36	Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing.	ACM	2021.07 37	CoLAKE: Contextualized Language and Knowledge Embedding	ACL	2020.08 38	LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention	EMNLP	2020
</pre></table></code></div></div></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/nlp/'>NLP</a>, <a href='/categories/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/'>基础知识</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/nlp/" class="post-tag no-text-decoration" >nlp</a> <a href="/tags/%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA/" class="post-tag no-text-decoration" >知识增强</a> <a href="/tags/%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/" class="post-tag no-text-decoration" >技术综述</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> 本文由作者按照 <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> 进行授权</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">分享</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93+-+zoe+Chen&url=https%3A%2F%2Fzoechen119.github.io%2Fposts%2F%25E7%259F%25A5%25E8%25AF%2586%25E5%25A2%259E%25E5%25BC%25BA%25E6%258A%2580%25E6%259C%25AF%25E6%2580%25BB%25E7%25BB%2593%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93+-+zoe+Chen&u=https%3A%2F%2Fzoechen119.github.io%2Fposts%2F%25E7%259F%25A5%25E8%25AF%2586%25E5%25A2%259E%25E5%25BC%25BA%25E6%258A%2580%25E6%259C%25AF%25E6%2580%25BB%25E7%25BB%2593%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fzoechen119.github.io%2Fposts%2F%25E7%259F%25A5%25E8%25AF%2586%25E5%25A2%259E%25E5%25BC%25BA%25E6%258A%2580%25E6%259C%25AF%25E6%2580%25BB%25E7%25BB%2593%2F&text=%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93+-+zoe+Chen" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="分享链接" data-title-succeed="链接已复制！"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">最近更新</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/C25_11%E5%85%A5%E9%97%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%860/">【NLP入门趣味题】肉眼找朋友</a><li><a href="/posts/C25_11%E5%85%A5%E9%97%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%861/">【NLP入门趣味题】探索语言模型与词向量</a><li><a href="/posts/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a><li><a href="/posts/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%AE%9E%E9%AA%8C/">多模态实验</a><li><a href="/posts/%E5%A4%9A%E6%A8%A1%E6%80%81RAG/">多模态rag</a></ul></div><div id="access-tags"><div class="panel-heading">热门标签</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/">技术综述</a> <a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> <a class="post-tag" href="/tags/rag/">RAG</a> <a class="post-tag" href="/tags/%E8%AF%AD%E8%A8%80%E5%AD%A6/">语言学</a> <a class="post-tag" href="/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/">预训练模型</a> <a class="post-tag" href="/tags/bert/">BERT</a> <a class="post-tag" href="/tags/courses/">Courses</a> <a class="post-tag" href="/tags/chatgpt/">ChatGPT</a> <a class="post-tag" href="/tags/mamba/">Mamba</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">文章内容</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>相关文章</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/RAG%E6%A3%80%E7%B4%A2%E7%9F%A5%E8%AF%86%E5%A2%9E%E5%BC%BA/"><div class="card-body"> <em class="small" data-ts="1708444800" data-df="YYYY-MM-DD" > 2024-02-21 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RAG</h3><div class="text-muted small"><p> RAG检索知识增强 不完全手册 本文从k个角度对RAG技术进行综述，第一个角度是“范式演变”，即原始的“朴素RAG”-“进阶RAG”-“模块化RAG”，这个角度个人认为是工程的角度、宏观的角度；第二个角度是“关键问题和相关研究”，大类上可分两类“以优化生成模型的表征空间为目的”和“以优化知识密集型问答任务为目的”。 通常“以优化生成模型的表征空间为目的”的这些模型有几个特点：（1）检...</p></div></div></a></div><div class="card"> <a href="/posts/RAG/"><div class="card-body"> <em class="small" data-ts="1709136000" data-df="YYYY-MM-DD" > 2024-02-29 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RAG</h3><div class="text-muted small"><p> RAG调研 一、一些共识 External knowledge is the key to resolving the problems of LLMs such as hallucination and outdated knowledge, which can make LLMs generate more accurate and reliable responses thr...</p></div></div></a></div><div class="card"> <a href="/posts/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"><div class="card-body"> <em class="small" data-ts="1683734400" data-df="YYYY-MM-DD" > 2023-05-11 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>强化学习</h3><div class="text-muted small"><p> # 0 一些共识 基本的NLP任务label是人工或自动标注的，这种既定的答案（又称为监督目标字符串）我们认为是“没有人类偏好的直接信号”，但语言技术的最终目标是与人类互动。因此这样的label其实是不太合理的。 The ultimate aim of language technology is to interact with humans. ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/PaperNote_%E6%80%9D%E7%BB%B4%E9%93%BE/" class="btn btn-outline-primary" prompt="上一篇"><p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</p></a> <a href="/posts/%E8%87%AA%E7%9B%91%E7%9D%A3%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E5%85%B3%E7%B3%BB/" class="btn btn-outline-primary" prompt="下一篇"><p>自监督与注意力机制的关系</p></a></div><script src="https://utteranc.es/client.js" repo="zoeChen119/zoeChen119.github.io" issue-term="title" crossorigin="anonymous" async> </script> <script type="text/javascript"> $(function() { const origin = "https://utteranc.es"; const iframe = "iframe.utterances-frame"; const lightTheme = "github-light"; const darkTheme = "github-dark"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } addEventListener("message", (event) => { let theme; /* credit to <https://github.com/utterance/utterances/issues/170#issuecomment-594036347> */ if (event.origin === origin) { /* page initial */ theme = initTheme; } else if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); } else { return; } const message = { type: "set-theme", theme: theme }; const utterances = document.querySelector(iframe).contentWindow; utterances.postMessage(message, origin); }); }); </script></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://github.com/zoeChen119">陈政伊</a>. <span data-toggle="tooltip" data-placement="top" title="除非另有说明，本网站上的博客文章均由作者按照知识共享署名 4.0 国际 (CC BY 4.0) 许可协议进行授权。">保留部分权利。</span></p></div><div class="footer-right"><p class="mb-0"> 本站由 <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> 生成，采用 <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> 主题。</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">热门标签</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/nlp/">nlp</a> <a class="post-tag" href="/tags/%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0/">技术综述</a> <a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> <a class="post-tag" href="/tags/rag/">RAG</a> <a class="post-tag" href="/tags/%E8%AF%AD%E8%A8%80%E5%AD%A6/">语言学</a> <a class="post-tag" href="/tags/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/">预训练模型</a> <a class="post-tag" href="/tags/bert/">BERT</a> <a class="post-tag" href="/tags/courses/">Courses</a> <a class="post-tag" href="/tags/chatgpt/">ChatGPT</a> <a class="post-tag" href="/tags/mamba/">Mamba</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">发现新版本的内容。</p><button type="button" class="btn btn-primary" aria-label="Update"> 更新 </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">搜索结果为空</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/zh.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
